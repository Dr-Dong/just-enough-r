---
title: 'Anova cookbook'
output:
  bookdown::tufte_html2
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=TRUE)
library(tidyverse)
library(pander)
library(lmerTest)
```




# Anova 'Cookbook'

This section is intended as a shortcut to running Anova for a variety of common types of model. If you want to understand more about what you are doing, read the section on [principles of Anova in R](anova.html).



## Between subjects designs


### Factorial anova, no bigger than 2x2

```{r}

```


### Factorial anova, where one factor has > 2 levels.

We are using a dataset from Howell (REF), chapter 13 which recorded `Recall` among young v.s. older adults (`Age`) for each of 5 conditions.

```{r, include=F}
eysenck <- read.table('howell-data/Tab13-2.dat', header=T) %>% 
  mutate(
    Condition=factor(Condition, 
                     labels=c("Counting", "Rhyming", "Adjective", "Imagery", "Intention")),
    Age = factor(Age, labels=c("Young", "Older")))
saveRDS(eysenck, file="data/eysenck.Rdata")
```

This data would commonly be plotted something like this:

```{r}
eysenck <- readRDS("data/eysenck.Rdata")
eysenck %>% 
  ggplot(aes(Condition, Recall, group=Age, color=Age)) + 
  stat_summary(geom="pointrange", fun.data = mean_cl_boot) +
  ylab("Recall (95% CI)") + xlab("")
```


Or alternatively if we wanted to provde a better summary of the distribution of the raw data we could use a boxplot:

```{r, fig.cap="Combined box and density plot for recall in older and young adults, by condition."}
eysenck %>% 
  ggplot(aes(Age, Recall)) + 
  geom_violin() + geom_boxplot(width=.33) + facet_grid(~Condition) +
  ylab("Recall (95% CI)") + xlab("")
```


We can run a linear model including the effect of `Age` and `Condition` and the interaction of these variables, and calculate the Anova:

```{r}
eysenck.model <- lm(Recall~Age*Condition, data=eysenck)
car::Anova(eysenck.model, type=3)
```


If we want to check assumptions of the model are met, these tables and plots would be a reasonable place to start:

```{r}
car::leveneTest(eysenck.model) %>% 
  pander()
```


```{r, fig.cap="QQ plot to assess normality of model residuals"}
car::qqPlot(eysenck.model)
```


```{r, fig.cap="Residual vs fitted (spread vs. level) plot to check homogeneity of variance."}
# we have to make a dataframe containing the fitted values and residuals first
data_frame(
  fitted = predict(eysenck.model), 
  residual = residuals(eysenck.model)) %>% 
  # and then plot points and a smoothed line
  ggplot(aes(fitted, residual)) + 
    geom_point() + 
    geom_smooth(se=F)
```


If we want to look at post-hoc pairwise tests we can use the the `lsmeans()` function from the `lsmeans::` package:

```{r}
lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition)
```

By default Tukey correction is applied for multiple comparisons which is a reasonable default. If you want to use other methods (e.g. to use false discovery rate adjustment) you can use the `adjust` argument. In the code below we use the `broom::tidy()` function to convert the table into a dataframe, and then show only the first 6 rows as a table in RMarkdown: 

```{r}
# calculate pairwise contrasts
eysenck.fdr <- lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition, adjust="fdr")
# show first 6 rows from this long table
eysenck.fdr$contrasts %>% 
  broom::tidy() %>% 
  head(6) %>% 
  pander(caption="First 6 rows of the pairwise contrasts with FDR-adjusted p values")
```


You should note that the FDR adjusted p values do not represent probabilities in the normal sense. Instead, the p value now indicates the *false discovery rate at which the p value should be considered statistically significant*. So, for example, if the adjusted p value  0.09, then this indicates the contrast *would* be significant if the acceptable false discovery rate is 10% (people often set their acceptable false discover rate to be 5% out of habit, but this is not always appropriate).

```{r}
# Set our acceptable false discovery rate to 10%
FDR <- .1
lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition, adjust="none")$contrast %>% 
  broom::tidy() %>% 
  select(level1, level2, p.value) %>% 
  arrange(p.value) %>% 
  mutate(`q (10% FDR)` = (rank(p.value)/length(p.value))*FDR) %>% 
  mutate(p.fdr.adjust=p.adjust(p.value, method="BH")) %>% 
  mutate(significant = as.numeric(p.value < `q (10% FDR)`)) %>%
  # just show some of the results, at the break between sig and ns contrast
  filter(p.fdr.adjust > .01 & p.fdr.adjust < .4) %>%
  pander(caption="Subset of contrasts, showing the break between significant and ns results, as determined by an FDR of 10%.", split.tables=Inf)
```

Note, that when you use `adjust='fdr'` then the p values returned are 
The [Biostat Handbook](http://www.biostathandbook.com/multiplecomparisons.html) has a good 



## Repeated measures or 'split plot' designs

It might be controversial to say so, but the tools to run traditional repeat measures Anova in R are a pain to use. It's not easy to run repeated measures Anova models using base packages alone and, although there are numerous packages which do simplify this a little, their syntax can be obtuse or confusing, and the output sometimes cryptic. To make matters worse, various textbooks, online guides and the R help files themselves show many ways to achieve the same ends, and it can be difficult to follow the differences between the underlying models that are run.

At this point, given the [many other advantages of linear mixed models over traditional repeated measures Anova](http://jamanetwork.com/journals/jamapsychiatry/article-abstract/481967), and given that many researchers abuse traditional Anova in practice (e.g. using it for unbalanced data, or where some data are missing), the recommendation here is to simply give up and learn how to run linear mixed models. These can (very closely) replicate traditional Anova approaches, but also:

- Handle missing data or unbalanced designs gracefully and efficiently.

- Be expanded to include multiple levels of nesting. For example, allowing pupils to be nested within classes, within schools. Alternatively multiple measurements of individual patients might be clustered by hospital or therapist.

- Allow time to be treated as a continuous variable. For example, time can be modelled as a slope or some kind of curve, rather than a fixed set of observation-points. This can be more parsimonious, and more flexible when dealing with real-world data (e.g. from clinical trials).

It would be best at this point to [jump straight to the main section multilevel or mixed-effects models](multilevel-models.html), but to give one brief example of mixed models in use:



The `sleepstudy` dataset in the `lme4` package provides reaction time data recorded from participants over a period of 10 days, during which time they were deprived of sleep.

```{r}
lme4::sleepstudy %>% head(12) %>% pander
```

We can plot these data to show the increase in RT as sleep deprivation continues:

```{r}
lme4::sleepstudy %>% 
  ggplot(aes(factor(Days), Reaction)) + 
  geom_boxplot() + 
  xlab("Days") + ylab("RT (ms)") + 
  geom_label(aes(y=400, x=2, label="you start to\nfeel bad here"), color="red") + 
  geom_label(aes(y=450, x=9, label="imagine how bad\nyou feel by this point"), color="red") 
```


If we want to test whether there are significant differences in RTs between `Days`, we could fit something very similar to a traditional repeat measures Anova using the `lme4::lmer()` function, and obtain an Anova table for the model using the special `lmerTest::anova()` function:

```{r}
sleep.model <- lmer(Reaction ~ factor(Days) + (1 | Subject), data=lme4::sleepstudy)
lmerTest::anova(sleep.model)
```


If you had really wanted to fit the traditional repeated measures Anova, the closest equivalent would be:

```{r}
afex::aov_car(Reaction ~ Days + Error(Subject/(Days)), data=lme4::sleepstudy)
```

This gives almost-identical results. You may find that in other cases the `lmer` and traditional anova models diverge slightly, but this is likely to be caused by factors including imbalances in the data, partially missing data (only complete cases can be analyses by traditional anova) or other violations of the assumptions of one or both of the models. There is no clear steer in the literature as to which model is 'best' in the general sense, and it is likely that the linear model will be a better fit for a greater range of datasets.

See the [multilevel models section](multilevel-models.html) for details of more interesting models using this dataset which:

- Fit a simple slope for `Days`
- Fit curves or other functions for `Days`
- Allow the effect of sleep deprivation to vary for different participants




