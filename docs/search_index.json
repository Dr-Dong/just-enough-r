[
["index.html", "‘Just enough’ R 1 Introduction A warning", " ‘Just enough’ R 1 Introduction R is software which makes it easy to work with and learn from data. It also happens to be a complete programmming language, but if you’re reading this guide then that might not be of interest to you. That’s OK — the goal here is not to teach you how to program in R1. The goal is to teach you just enough R to be confident to explore your data. We are going to use R the same way we use any other statistics software: To check and visualise data, run statistical analyses, and share our results with others. To do that it’s worth learning the absolute basics of the R language. The next few chapters walk you through those basics: what you need to know to be productive in R and RStudio. By analogy, imagine going on holiday and learning enough French to introduce yourself, count from one to ten, and order a baguette. You won’t be fluent - but you won’t starve, and your trip will be more fun. Part 1: ‘Must read’ Part 1 is meant to be read from beginning to end by those starting from scratch. Getting started Datasets and dataframes Manipulating dataframes (select and filter) Summarising data (split, apply, combine) Visualising data (layering graphics with ggplot2) Part 2: Nice to know Part 2 can be read sequentially, but the chapters are also designed to work as standalone guides for specific techniques. Working with real data Correlations t-tests Linear models Anova in R Anova cookbook Understanding interactions (visualising interactions in raw data) Predictions and marginal effects Mediation Linear mixed models (multilevel models) Meta analysis Confirmatory factor analysis Structural Equation Modelling Power analysis …2 Part 3: Loose ends Part 3 should be used interactively to answer questions that arise along the way. Some of the content here is not specific to R, but may be useful in interpreting the output of some of the techniques taught in sections 1 and 2. Installing RStudio Installing packages Handling missing values in R Using RMarkdown effectively (e.g. using pander and knitr) Confidence intervals vs. prediction intervals A warning This guide is extremely opinionated. There are many ways to get things done with R, but trying to learn them all at once causes unhappiness. In particular, lots of the base R functions are old, quirky, inconstently named, and hard to remember. This guide recommends that you use several new ‘packages’ that replicate and extend some of R’s basic functionality. Using this new set of packages, which are very thoughtfull designed and work together nicely, will help you form a more consistent mental model of how to work in R. You can learn the crufty old bits (which do still have their uses) later on. I also assume you are using the RStudio editor and working in an RMarkdown document (see the next section). This is important because this guide itself is written in RMarkdown, and editing it will be an important part of the learning process. If you don’t have access to RStudio yet, see the installation guide. This is actually a lie, but I’m hoping you won’t notice until it’s too late.↩ It would also be lovely to have chapters on multiple imputation, power analysis, simulation, Bayesian modelling and much else. These aren’t planned imminently, but contributions are welcome.↩ "],
["start_here.html", "2 Getting started with R and RStudio 2.1 Using RMarkdown to record and share work 2.2 Writing and ‘knitting’ RMarkdown 2.3 RStudio 2.4 Your first R commands 2.5 Naming things: variable assignment 3 Vectors and lists 3.1 Vectors 3.2 Lists 3.3 Questions on vectors and lists", " 2 Getting started with R and RStudio 2.1 Using RMarkdown to record and share work This it might seem an odd place to start: we haven’t got anything to share yet! But the RStudio editor (see below) includes important features which help us record and organise our work, and share it with colleagues. For many people this ability to keep a detailed record of your work, and revisit and review it later, turns out to be the major advantages of R over traditional statistics packages. You are currently reading the output of an ‘RMarkdown’ document. ‘R’ is a computer language for working with data. Markdown is a simple text format which allows you to combine writing, images and code (see http://commonmark.org/help/). An RMarkdown document mixes R code with markdown. This means you can combine your analysis with text explaining and interpreting the results. RMarkdown is easily converted to other formats like HTML, Word, or PDF to share with other people. When you click the Knit button (in the Rstudio interface), a document will be generated that combines your text with the results of your R code. 2.2 Writing and ‘knitting’ RMarkdown To include R code within a document we write 3 backticks (```), followed by {r}. We the include our R code, and close the block with 3 more backticks. ```{r} 2 + 2 ``` When a document including this chunk is run or ‘knitted’, the final result will include the the line 2+2 followed by the number 4 on the next line. This means we can use RMarkdown to ‘show our workings’: our analysis can be interleaved with narrative text to explain or interpret the calculations. You can see how this works in practice in the next section. 2.3 RStudio RStudio is a text editor which has been customised to make working with R easy. It can be installed on your own computer, or you can login to a shared RStudio server3 from a web browser. Either way the interface is largely the same. The figure above shows the main RStudio interface, comprising: The main R-script or RMarkdown editor window The R console, into which you can type R commands, and see output from commands run in the script editor. The ‘environment’ panel, which lists all the variables you have defined and currently available to use. The files and help panel. Within this the files tab enables you to open files stored on the server, or in the current project on your disk. You can see a short video demonstrating the RStudio interface here: The video4: Shows you how to type commands into the Console and view the results. Run a plotting function, and see the result. Create RMarkdown file, and ‘Knit’ it to produce a document containing the results of your code and explanatory text. Once you have watched the video: Try creating a new RMarkdown document in RStudio. Edit some of the text, and press the Knit button to see the results. If you feel brave, edit one of the R blocks and see what happens! 2.4 Your first R commands You can type R commands directly into the console and see the result there too, but you should make a habit of working in an RMarkdown file. This keeps a record of everything you try, and makes it easy to edit/amend commands which don’t work as you expect. Now would be a good time to open and RMarkdown document to see how it works. A good place to start would be to open the source to this document. The best way to do this is to download the source code for this project, and then open the file start_here.Rmd. The source is available here: https://github.com/benwhalley/just-enough-r/archive/master.zip5 To run code in the RStudio interface put your cursor on a line within an R Block (or select the code you want to run), and press Ctrl-Enter. The result will appear below the code block. The command in the R block below prints (shows on screen) the first few rows of the mtcars dataset, which is built in to R as an example. Place your cursor somewhere in the line the command is on and run it by typing Ctrl-Enter: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If you are reading this from within RStudio, running head(mtcars) will have included an interactive table in the document, which you can use this to view the mtcars dataset. If you are still reading the compiled html or pdf document you will see a table containing the same data, included within the body of the document. Hopefully at this point it’s obvious that RStudio and RMarkdown give you: A nice place to work with R and explore your data A nice format to share your workings (e.g. with other researchers or your tutor) A mechanism to save reports of your analysis, to share with other people who don’t use RStudio 2.5 Naming things: variable assignment One of the nice things about R is that we can assign labels to parts of our analysis to keep track of them. Using good, descriptive names is good practice and makes your code easier to read (e.g. when you or others need to revisit in future. To assign labels to particular values we use the &lt;- symbol. That is, we have a value and use the &lt;- symbol to point to the variable we want to assign it to. For example: meaning.of.life &lt;- 42 This assigns the value 42 to the variable meaning.of.life. This block wouldn’t display anything because assigning a variable doesn’t create any output. To both assign a variable and display it we would type: meaning.of.life &lt;- 42 meaning.of.life ## [1] 42 Or if we want to be explicit: print(meaning.of.life) ## [1] 42 Helpfully, we can also do simple calculations as we assign variables: one.score &lt;- 20 four.score.years.and.ten &lt;- one.score * 4 + 10 print(four.score.years.and.ten) ## [1] 90 As you will see below, we can give anything a label by assigning it to a variable. It doesn’t have to be simple numbers: we can also assign words, graphics and plots, the results of a statistical model, or lists of any of these things. 3 Vectors and lists When working with data, we often have lists of ‘things’: for example a list of measurements we have made. When all the things are of the same ‘type’, then in R this is called a vector. When the list contains a mix of different things, then R calls it a list In general we should avoid mixing up different types of ‘thing’, and so use vectors wherever possible. 3.1 Vectors We can create a vector of numbers and display it like this: # this creates a vector of heights, in cm heights &lt;- c(203, 148, 156, 158, 167, 162, 172, 164, 172, 187, 134, 182, 175) The c() command is shorthand for combine, so the example above combines the individual elements (numbers) into a new vector. We can create a vector of alphanumeric names just eas easily: names &lt;- c(&quot;Ben&quot;, &quot;Joe&quot;, &quot;Sue&quot;, &quot;Rosa&quot;) And we can check the values stored in these variables by printing them, e.g.: heights ## [1] 203 148 156 158 167 162 172 164 172 187 134 182 175 Try creating your own vector of numbers in a new code block below6 using the c(...) command. Then change the name of the variable you assign it to. 3.1.1 Accessing elements within vectors Once we have created a vector, we often want to access the individual elements again. We do this based on their position. Let’s say we have created a vector: my.vector &lt;- c(10, 20, 30, 40) We can display the whole vector by just typing it’s name, as we saw above. But if we want to show only the first element of this vector, we type: my.vector[1] ## [1] 10 Here, the square brackets specify a subset of the vector we want - in this case, just the first element. 3.1.2 Selecting more than one element in a vector A neat feature of subsetting is that we can grab more than one element at a time. To do this, we need to tell R the positions of the elements we want, and so we provide a vector of the positions of the elements we want. It might seem obvious, but the first element has position 1, the second has position 2, and so on. So, if we wanted to extract the 4th and 5th elements from the vector of heights we saw above we would type: elements.to.grab &lt;- c(4, 5) heights[elements.to.grab] ## [1] 158 167 We can also make a subset of the original vector and assign it to a new variable: first.two.elements &lt;- heights[c(1, 2)] first.two.elements ## [1] 203 148 --> 3.1.3 Processing vectors Many of R’s most useful functions process vectors of numbers in some way. For example, if we want to calculate the average of our vector of heights we just type: mean(heights) ## [1] 167.6923 R contains lots of built in functions which we can use to summarise a vector of numbers. For example: median(heights) ## [1] 167 sd(heights) ## [1] 17.59443 min(heights) ## [1] 134 max(heights) ## [1] 203 range(heights) ## [1] 134 203 IQR(heights) ## [1] 17 length(heights) ## [1] 13 All of these functions accept a vector as input, do some proccesing, and then return a single number which gets displayed by RStudio. But not all functions return a single number in the way that mean did above. Some return a new vector, or some other type of object instead. For example, the quantile function returns the values at the 0, 25th, 50th, 75th and 100th percentiles (by default). height.quantiles &lt;- quantile(heights) height.quantiles ## 0% 25% 50% 75% 100% ## 134 158 167 175 203 If a function returns a vector, we can use it just like any other vector: height.quantiles &lt;- quantile(heights) # grab the third element, which is the median height.quantiles[3] ## 50% ## 167 # assign the first element to a variable min.height &lt;- height.quantiles[1] min.height ## 0% ## 134 But other functions process a vector without returning any numbers. For example, the hist function returns a histogram: hist(heights) We’ll cover lots more plotting and visualisation later on. 3.1.4 Processing vectors to make new vectors So far we’ve seen R functions which process a vector of numbers and produce a single number, a new vector of a different length (like quantile or fivenum), or some other object (like hist which makes a plot). However many other functions accept a single input, do something to it, and return a single processed value. For example, the square root function, sqrt, accepts a single value and returns a single value: running sqrt(10) will return 3.1623. In R, if a function accepts a single value as input and returns a single value as output (like sqrt(10)), then you can usually give a vector as input too. Some people find this surprising7, but R assumes that if you’re processing a vector of numbers, you want the function applied to each of them in the same way. This turns out to be very useful. For example, let’s say we want the square root of each of the elements of our height data: # these are the raw values heights ## [1] 203 148 156 158 167 162 172 164 172 187 134 182 175 # takes the sqrt of each value and returns a vector of all the square roots sqrt(heights) ## [1] 14.24781 12.16553 12.49000 12.56981 12.92285 12.72792 13.11488 ## [8] 12.80625 13.11488 13.67479 11.57584 13.49074 13.22876 This also works with simple arithmetic So, if we wanted to convert all the heights from cm to meters we could just type: heights / 100 ## [1] 2.03 1.48 1.56 1.58 1.67 1.62 1.72 1.64 1.72 1.87 1.34 1.82 1.75 This trick also works with other functions like paste, which combines the inputs you send it to produce an alphanumeric string: paste(&quot;Once&quot;, &quot;upon&quot;, &quot;a&quot;, &quot;time&quot;) ## [1] &quot;Once upon a time&quot; If we send a vector to paste it assumes we want a vector of results, with each element in the vector pasted next to each other: bottles &lt;- c(100, 99, 98, &quot;...&quot;) paste(bottles, &quot;green bottles hanging on the wall&quot;) ## [1] &quot;100 green bottles hanging on the wall&quot; ## [2] &quot;99 green bottles hanging on the wall&quot; ## [3] &quot;98 green bottles hanging on the wall&quot; ## [4] &quot;... green bottles hanging on the wall&quot; In other programming languages we might have had to write a ‘loop’ to create each line of the song, but R lets us write short statements to summarise what needs to be done; we don’t need to worry worrying about how it gets done. 3.1.5 Making up data (new vectors) Sometimes you’ll need to create vectors containing regular sequences or randomly selected numbers. To create regular sequences a convenient shortcut is the ‘colon’ operator. For example, if we type 1:10 then we get a vector of numbers from 1 to 10: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 The seq function allows you to create more specific sequences: # make a sequence, specifying the interval between them seq(from=0.1, to=2, by=.1) ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 ## [18] 1.8 1.9 2.0 We can also use random number-generating functions built into R to create vectors: # 10 uniformly distributed random numbers between 0 and 1 runif(10) ## [1] 0.1859517 0.1403806 0.6693456 0.8472666 0.2309495 0.2300545 0.8666919 ## [8] 0.4884143 0.9654400 0.4645153 # 1,000 uniformly distributed random numbers between 1 and 100 my.numbers &lt;- runif(1000, 1, 10) # 10 random-normal numbers with mean 10 and SD=1 rnorm(10, mean=10) ## [1] 8.859302 9.779185 10.357482 10.937483 8.555067 10.594929 10.930529 ## [8] 8.958609 9.831223 8.265265 # 10 random-normal numbers with mean 10 and SD=5 rnorm(10, 10, 5) ## [1] 10.8100825 8.9958930 2.7580386 7.4021015 7.5288401 0.1225604 ## [7] 8.4658622 8.2993016 16.7534052 4.5453275 We can then use these numbers in our code, for example plotting them: random.numbers &lt;- rnorm(10000) hist(random.numbers) 3.1.6 Useful functions to learn now There are thousands of functions built into R. Below are a few examples which are likely to be useful as you work with your data: # repeat something N times rep(&quot;Apple pie&quot;, 10) ## [1] &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; ## [6] &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; # repeat a short vector, combining into a single longer vector rep(c(&quot;Custard&quot;, &quot;Gravy&quot;), 5) ## [1] &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; ## [8] &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; # make a sequence and then sort it countdown &lt;- 100:1 sort(countdown) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # generate some random data (here, ages in years) ages &lt;- round(rnorm(10, mean=40, sd=10)) # get the rank order of elements (i.e. what their positions would be if the vector was sorted) ages ## [1] 52 36 49 48 42 53 35 38 52 28 rank(ages, ties.method=&quot;first&quot;) ## [1] 8 3 7 6 5 10 2 4 9 1 # you could even label the vector of ages if you wanted labels &lt;- paste(&quot;Position:&quot;, rank(ages, ties.method=&quot;first&quot;)) names(ages) &lt;- labels ages ## Position: 8 Position: 3 Position: 7 Position: 6 Position: 5 ## 52 36 49 48 42 ## Position: 10 Position: 2 Position: 4 Position: 9 Position: 1 ## 53 35 38 52 28 # return the unique values in a vector unique( rep(1:10, 100) ) ## [1] 1 2 3 4 5 6 7 8 9 10 Try and experiment with each of these functions. Check the output against what you expected to happen, and make sure you understand what they do. 3.2 Lists Try running the code below: confusing.vector &lt;- c(1, 2, 3, &quot;Wibble&quot;) first.element &lt;- confusing.vector[1] sqrt(first.element) ## Error in sqrt(first.element): non-numeric argument to mathematical function Take a minute to try and make a guess at what went wrong. Why does R complain that ‘1’ is non-numeric? When we built the vector we used c to combine the elements 1, 2, 3 and &quot;Wibble&quot;. Although our first and second elements are numbers, &quot;Wibble&quot; is not - it’s made up of letters (this is called a character string). Remember that vectors can only contain one type of thing. And so R automatically converts all the elements to the same type, if it can. Because R can’t reliably convert &quot;Wibble&quot; to a number, everything in the vector was converted to the character type instead. We get an error because R can’t mutiply words together. If you’re not sure what type of thing your vector contains, you can use the typeof command: typeof(1:10) ## [1] &quot;integer&quot; typeof(runif(10)) ## [1] &quot;double&quot; typeof(c(1, 2, &quot;Wibble&quot;)) ## [1] &quot;character&quot; Here the meaning of integer should be self explanatory. The vector runif(10) has type double, because it contains ‘double-precision’ floating point numbers. For our purposes you can just think of double as meaning any number with decimal places. The last vector has the type character because it includes the character string Wibble, and all the other numbers in it were coerced to become character strings too. If we want to (safely) mix up different types of object without them being converted we need a proper list, rather than a vector. In R we would write: my.list &lt;- list(2, 2, &quot;Wibble&quot;) We can still access elements from lists as we do for vectors, although now we need to use double square brackets, for example: my.list[[1]] ## [1] 2 But now our numbers haven’t been converted to character strings, and we can still multiply them. my.list[[1]] * my.list[[2]] ## [1] 4 Square brackets are ugly and can be confusing though, so we often give names to the elements of our list when we create it: my.party &lt;- list(number.guests=8, when=&quot;Friday&quot;, drinks = c(&quot;Juice&quot;, &quot;Beer&quot;, &quot;Whisky&quot;)) Which means we can then access the elements by name later on. To do this, you write the name of the vector, then a $ sign, and then the name of the element you want to access: my.party$when ## [1] &quot;Friday&quot; You might have spotted that we included a vector inside the party list. This is not a problem, and we can still access individual elements of this vector too: my.party$drinks[1] ## [1] &quot;Juice&quot; 3.3 Questions on vectors and lists Create a vector containing 3 numbers then: Access just the last number Create a new list containing the first and last number Create a list containing, your address and your age in years. Then: Multiply your age in years by your flat or house number (by accessing the relevant elements in the list) Run the following R code and explain what has happened: sqrt(1:10) * 10 ## [1] 10.00000 14.14214 17.32051 20.00000 22.36068 24.49490 26.45751 ## [8] 28.28427 30.00000 31.62278 3.3.1 Extended questions: What is the average of the 9 times table, up to and including 9 x 12? Use the paste and c(...) functions to create a vector which contains the sequence “1 elephant”, “2 elephants”, …, “1000 elephants”. e.g. one run by your university.↩ Note: this isn’t the final version.. it will be more polished!↩ If you wanted, you could view and download the source for just this document here: https://github.com/benwhalley/just-enough-r/blob/master/start_here.Rmd but it will save time to download the whole project now.↩ i.e. edit the RMarkdown document↩ Mostly people who already know other programming languages like C. It’s not that surprising if you read the R code as you would English.↩ "],
["datasets.html", "4 Datasets and dataframes 4.1 Using ‘built in’ data 4.2 Importing and exporting data 4.3 Importing data over the web 4.4 Importing from SPSS and other packages", " 4 Datasets and dataframes A dataframe is an object which can store data as you might encounter it in SPSS, Stata or other statistics packages. It’s much like a spreadsheet, but with some constraints applied ‘Constraints’ sounds bad, but are helpful here: they make dataframes more structured and predictable to work with: Each column is a vector, and so can only store one type of data8 Every column has to be the same length (although missing values are allowed). Each column should have a name. Put another way, a dataframe behaves like a list of vectors, which means we can use a lot of the same rules to access elements within them (more on this below). 4.1 Using ‘built in’ data The quickest way to see a dataframe in action is to use one that is built in9 to R. For example: head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 Or we can use dplyr::glimpse() function to take a different look at the first few rows of the mtcars data. This flips the dataframe so the variables are listed in the first column of the output: glimpse(mtcars) ## Observations: 32 ## Variables: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2... ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ... In both these examples the datasets (airquality and mtcars) are already loaded and available to be used in the head() or glimpse() functions. Other useful functions for looking at datasets include: summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Or the more compact and useful output from describe() which is in the pysch package: psych::describe(airquality) ## vars n mean sd median trimmed mad min max range skew ## Ozone 1 116 42.13 32.99 31.5 37.80 25.95 1.0 168.0 167 1.21 ## Solar.R 2 146 185.93 90.06 205.0 190.34 98.59 7.0 334.0 327 -0.42 ## Wind 3 153 9.96 3.52 9.7 9.87 3.41 1.7 20.7 19 0.34 ## Temp 4 153 77.88 9.47 79.0 78.28 8.90 56.0 97.0 41 -0.37 ## Month 5 153 6.99 1.42 7.0 6.99 1.48 5.0 9.0 4 0.00 ## Day 6 153 15.80 8.86 16.0 15.80 11.86 1.0 31.0 30 0.00 ## kurtosis se ## Ozone 1.11 3.06 ## Solar.R -1.00 7.45 ## Wind 0.03 0.28 ## Temp -0.46 0.77 ## Month -1.32 0.11 ## Day -1.22 0.72 There are also some helpful plotting functions which accept a dataframe: ⊕These plots might not be worth including in a final write-up, but are very useful when exploring your data. boxplot(airquality) psych::cor.plot(airquality) 4.2 Importing and exporting data If you have data outside of R, the simplest way to import it is to first save it as a comma or tab-separated text file, normally with the file extension .csv or .txt10. Let’s say we have file called angry_moods.csv in the same directory as our .Rmd file. We can read this data using the read_csv() function from the readr package11: angry.moods &lt;- readr::read_csv(&#39;data/angry_moods.csv&#39;) ## Parsed with column specification: ## cols( ## Gender = col_integer(), ## Sports = col_integer(), ## Anger.Out = col_integer(), ## Anger.In = col_integer(), ## Control.Out = col_integer(), ## Control.In = col_integer(), ## Anger.Expression = col_integer() ## ) head(angry.moods) ## # A tibble: 6 × 7 ## Gender Sports Anger.Out Anger.In Control.Out Control.In Anger.Expression ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1 18 13 23 20 36 ## 2 2 1 14 17 25 24 30 ## 3 2 1 13 14 28 28 19 ## 4 2 1 17 24 23 23 43 ## 5 1 1 16 17 26 28 27 ## 6 1 1 16 22 25 23 38 As you can see, when loading the .csv file the read_csv() makes some assumptions about the type of data the file contains. In this case, all the columns contain integer values. It’s worth checking this message to make sure that stray cells in the file you are importing don’t cause problems when importing. Excel won’t complain about this sort of thing, but R is more strict and won’t mix text and numbers in the same column. A common error is for stray notes or text values in a spreadsheet to cause a column which should be numeric to be converted to the character type. Once it’s loaded, you can use this new dataset like any other: pairs(angry.moods) 4.3 Importing data over the web One neat feature of the readr package is that you can import data from the web, using a URL rather than a filename on your local computer. This can be really helpful when sharing data and code with colleagues. For example, we can load the angry_moods.csv file from a URL: angry.moods.from.url &lt;- readr::read_csv( &quot;https://raw.githubusercontent.com/benwhalley/just-enough-r/master/angry_moods.csv&quot;) head(angry.moods.from.url) 4.4 Importing from SPSS and other packages This is often more trouble than it’s worth (just use a csv file!) but if you really must see https://www.datacamp.com/community/tutorials/r-data-import-tutorial. Remember that in a previous chapter we created vectors, which are sequences that contain only one type of thing, and lists (which can contain a mix of different things.↩ To find a list of all the built in datasets you can type help(datasets)↩ This is easy to achieve in Excel and most other stats packages using the Save As... menu item↩ There are also standard functions built into R, such as read.csv() or read.table() for importing data. These are fine if you can’t install the readr package for some reason, but they are quite old and the default behaviour is sometimes counterintuitive. I recommend using the readr equivalents: readr::read_csv() or readr::read_tsv().↩ "],
["working-with-dataframes.html", "4.5 Working with dataframes 4.6 Introducing the tidyverse 4.7 Selecting columns from a dataframe 4.8 Selecting rows of data 4.9 Combining column selections and filters with dplyr 4.10 Modifying and creating new columns", " 4.5 Working with dataframes 4.6 Introducing the tidyverse R includes hundreds of built-in ways to select individual elements, rows or columns from a dataframe. This guide isn’t going to teach you many of them. The truth is that R can be overwhelming to new users, especially those new to programming. R is sometimes too powerful and flexible: there are too many different to accomplish the same end, and this can lead to confusion. Recently, a suite of packages has been developed for R which tries to provide a simple, consistent set of tools for working with data and graphics. This suite of packages is called the tidyverse, and you can load all of these pacakges by calling: library(tidyverse) In this guide we make much use of two components from the tidyverse: dplyr: to select, filter and summarise data ggplot2: to make plots It’s strongly recommended that you use these in your own code. 4.7 Selecting columns from a dataframe Selecting a single column: Because dataframes act like lists of vectors, we can access columns from them using the $ symbol. For example, here we select the Ozone column, which returns a vector of the observations made: airquality$Ozone ## [1] 41 36 12 18 NA 28 23 19 8 NA 7 16 11 14 18 14 34 ## [18] 6 30 11 1 11 4 32 NA NA NA 23 45 115 37 NA NA NA ## [35] NA NA NA 29 NA 71 39 NA NA 23 NA NA 21 37 20 12 13 ## [52] NA NA NA NA NA NA NA NA NA NA 135 49 32 NA 64 40 77 ## [69] 97 97 85 NA 10 27 NA 7 48 35 61 79 63 16 NA NA 80 ## [86] 108 20 52 82 50 64 59 39 9 16 78 35 66 122 89 110 NA ## [103] NA 44 28 65 NA 22 59 23 31 44 21 9 NA 45 168 73 NA ## [120] 76 118 84 85 96 78 73 91 47 32 20 23 21 24 44 21 28 ## [137] 9 13 46 18 13 24 16 13 23 36 7 14 30 NA 14 18 20 And we can pass this vector to functions, for example summary(): summary(airquality$Ozone) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.00 18.00 31.50 42.13 63.25 168.00 37 Selecting more than one column: To select multiple columns the select() function from dplyr is the simplest solution. You give select() a dataframe and the names of the columns you want, and it returns a new dataframe with just those columns, in the order you specified: head( select(mtcars, cyl, hp) ) ## cyl hp ## Mazda RX4 6 110 ## Mazda RX4 Wag 6 110 ## Datsun 710 4 93 ## Hornet 4 Drive 6 110 ## Hornet Sportabout 8 175 ## Valiant 6 105 Because all the main dplyr functions tend to return a new dataframe, we can assign the results to a variable, and use that as normal: cylandweight &lt;- select(mtcars, cyl, wt) summary(cylandweight) ## cyl wt ## Min. :4.000 Min. :1.513 ## 1st Qu.:4.000 1st Qu.:2.581 ## Median :6.000 Median :3.325 ## Mean :6.188 Mean :3.217 ## 3rd Qu.:8.000 3rd Qu.:3.610 ## Max. :8.000 Max. :5.424 You can also put a minus (-) sign in front of the column name to indicate which columns you don’t want: head( select(airquality, -Ozone, -Solar.R, -Wind) ) ## Temp Month Day ## 1 67 5 1 ## 2 72 5 2 ## 3 74 5 3 ## 4 62 5 4 ## 5 56 5 5 ## 6 66 5 6 You can use a patterns to match a subset of the columns you want. For example, here we select all the columns where the name contains the letter d: head( select(mtcars, contains(&quot;d&quot;)) ) ## disp drat ## Mazda RX4 160 3.90 ## Mazda RX4 Wag 160 3.90 ## Datsun 710 108 3.85 ## Hornet 4 Drive 258 3.08 ## Hornet Sportabout 360 3.15 ## Valiant 225 2.76 And you can combine these techniques to make more complex selections: head( select(mtcars, contains(&quot;d&quot;), -drat) ) ## disp ## Mazda RX4 160 ## Mazda RX4 Wag 160 ## Datsun 710 108 ## Hornet 4 Drive 258 ## Hornet Sportabout 360 ## Valiant 225 As a quick reference, you can use the following ‘verbs’ to select columns in different ways: starts_with() ends_with() contains() everything() There are other commands too, but these are probably the most useful to begin with. See the help files for more information. 4.8 Selecting rows of data To select particular rows from a dataframe, dplyr provides the very useful select() function. Let’s say we just want the 6-cylindered cars from the mtcars dataframe: filter(mtcars, cyl==6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 5 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 6 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 7 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Here we used the filter function to select rows matching a particular criteria: in this case, that cyl==6. We can match two criteria at once if needed12: filter(mtcars, cyl==6 &amp; gear==3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## 2 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 4.9 Combining column selections and filters with dplyr As you might have noticed above, we can ‘nest’ function calls in R. For example, we might want to both select some columns and filter rows. Taking the mtcars data, we might want to select the weights of only those cars with low mpg: gas.guzzlers &lt;- select(filter(mtcars, mpg &lt; 15), wt) summary(gas.guzzlers) ## wt ## Min. :3.570 ## 1st Qu.:3.840 ## Median :5.250 ## Mean :4.686 ## 3rd Qu.:5.345 ## Max. :5.424 This is OK, but can get quite confusing to read, and the more deeply functions are nested the easier it is to make a mistake. dplyr provides an alternative to nested function calls, called the pipe. Imagine your dataframe as a big bucket containing data. From this bucket, you can ‘pour’ your data down through a series of tubes and filters, until at the bottom of your screen you have a smaller bucket containing just the data you want. Think of your data ‘flowing’ down the screen. To make data flow from one bucket to another, we use the ‘pipe’ operator: %&gt;% big.bucket.of.data &lt;- mtcars big.bucket.of.data %&gt;% filter(mpg &lt;15) %&gt;% select(wt) %&gt;% summary ## wt ## Min. :3.570 ## 1st Qu.:3.840 ## Median :5.250 ## Mean :4.686 ## 3rd Qu.:5.345 ## Max. :5.424 So we have achieved the same outcome, but the code reads as a series of operations which the data flows through, connected by our pipes (the %&gt;%). At the end of the last pipe, our data gets dumped into the summary() function13 We could just has well have saved this smaller ‘bucket’ of data so we can use it later on: smaller.bucket &lt;- big.bucket.of.data %&gt;% filter(mpg &lt;15) %&gt;% select(wt) This turns out to be an incredibly useful pattern when processing and working with data. We can pour data through a series of filters and other operations, saving intermediate states where necessary. 4.10 Modifying and creating new columns Often when working with data we want to compute new values from columns we already have. Let’s say we have some data on the PHQ-9, which measures depression: phq9.df &lt;- readr::read_csv(&quot;phq.csv&quot;) ## Parsed with column specification: ## cols( ## patient = col_integer(), ## phq9_01 = col_integer(), ## phq9_02 = col_integer(), ## phq9_03 = col_integer(), ## phq9_04 = col_integer(), ## phq9_05 = col_integer(), ## phq9_06 = col_integer(), ## phq9_07 = col_integer(), ## phq9_08 = col_integer(), ## phq9_09 = col_integer(), ## month = col_integer(), ## group = col_integer() ## ) glimpse(phq9.df) ## Observations: 2,429 ## Variables: 12 ## $ patient &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ... ## $ phq9_01 &lt;int&gt; 3, 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 3, ... ## $ phq9_02 &lt;int&gt; 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_03 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, ... ## $ phq9_04 &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, ... ## $ phq9_05 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, ... ## $ phq9_06 &lt;int&gt; 3, 2, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_07 &lt;int&gt; 3, 3, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 1, 1, ... ## $ phq9_08 &lt;int&gt; 0, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, ... ## $ phq9_09 &lt;int&gt; 2, 2, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, ... ## $ month &lt;int&gt; 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 0, 1,... ## $ group &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ... We want to calculate the PHQ-9 score for each patient, at each month. This is easy with dplyr::mutate(): phq9.scored.df &lt;- phq9.df %&gt;% mutate(phq9 = phq9_01 + phq9_02 + phq9_03 + phq9_04 + phq9_05 + phq9_06 + phq9_07 + phq9_08 + phq9_09) phq9.scored.df %&gt;% select(patient, group, month, phq9) %&gt;% head ## # A tibble: 6 × 4 ## patient group month phq9 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 0 23 ## 2 2 0 0 21 ## 3 2 0 1 17 ## 4 2 0 2 18 ## 5 2 0 3 22 ## 6 2 0 4 21 Notice that we first stored the computed scores in phq9.scored.df and then used select() to get rid of the raw data columns to display only what we needed. Some notes on == and &amp;: You might have noted above that I wrote == rather than just = to define the criteria. This is because most programming languages, including R, use two = symbols to distinguish: comparison from assignment. Here we are doing comparison, so we use ==. In R normall use &lt;- to assign variables, which avoids any ambiguity. The &amp; symbol does what you probably expect — it simply means ‘AND’.↩ You might notice that when we write the select function we don’t explicitly name the dataframe to be used. This is because R implicitly passes the output of the pipe to the first argument of the function. So here, the output of filter(mpg&lt;15) is used as the dataframe in the select function.↩ "],
["summarising-data.html", "5 Summarising data 5.1 Summaries of dataframes 5.2 Split, apply, combine 5.3 Split: breaking the data into groups 5.4 Apply and combine 5.5 A ‘real’ example 5.6 Sorting data", " 5 Summarising data Before you begin this section, make sure you have fully understood the section on datasets and dataframes, and in particular that you are happy using the %&gt;% symbol to describe a flow of data. 5.1 Summaries of dataframes So far you have seen a number of functions which provide summaries of a dataframe. For example: summary(angry.moods) ## Gender Sports Anger.Out Anger.In ## Min. :1.000 Min. :1.000 Min. : 9.00 Min. :10.00 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:13.00 1st Qu.:15.00 ## Median :2.000 Median :2.000 Median :16.00 Median :18.50 ## Mean :1.615 Mean :1.679 Mean :16.08 Mean :18.58 ## 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:18.00 3rd Qu.:22.00 ## Max. :2.000 Max. :2.000 Max. :27.00 Max. :31.00 ## Control.Out Control.In Anger.Expression ## Min. :14.00 Min. :11.00 Min. : 7.00 ## 1st Qu.:21.00 1st Qu.:18.25 1st Qu.:27.00 ## Median :24.00 Median :22.00 Median :36.00 ## Mean :23.69 Mean :21.96 Mean :37.00 ## 3rd Qu.:27.00 3rd Qu.:24.75 3rd Qu.:44.75 ## Max. :32.00 Max. :32.00 Max. :68.00 Or psych::describe(angry.moods, skew=FALSE) ## vars n mean sd min max range se ## Gender 1 78 1.62 0.49 1 2 1 0.06 ## Sports 2 78 1.68 0.47 1 2 1 0.05 ## Anger.Out 3 78 16.08 4.22 9 27 18 0.48 ## Anger.In 4 78 18.58 4.70 10 31 21 0.53 ## Control.Out 5 78 23.69 4.69 14 32 18 0.53 ## Control.In 6 78 21.96 4.95 11 32 21 0.56 ## Anger.Expression 7 78 37.00 12.94 7 68 61 1.47 However, these functions operate on the dataset as a whole. What if we want to get summaries grouped by one of our variables, for example Gender? Or perhaps we want to use our summary data in a further analysis: for example, we might want to compute average reaction times in each block of an experiment to run an Anova or regression model. What we really want is a summary function which gives us back a dataframe. The dplyr::summarise() does just that: angry.moods %&gt;% summarise( mean.anger.out=mean(Anger.Out), sd.anger.out=sd(Anger.Out) ) ## # A tibble: 1 × 2 ## mean.anger.out sd.anger.out ## &lt;dbl&gt; &lt;dbl&gt; ## 1 16.07692 4.21737 This has returned a dataframe, which we could store and use as before, although in this instance the dataframe only has one row. What if we want the numbers for men and women separately? 5.2 Split, apply, combine Let’s think more about the case where we want to compute statistics on men and women separately. Although many packages would have functions with options to do this (for example, perhaps you would specify grouping variables in a summary function), there’s a more general pattern at work. We want to: Split our data (into men and women) Apply some function to them (e.g. calculate the mean) and then Combine it into a single table again (for more processing or analysis) It’s helpful to think of this split \\(\\rightarrow\\) apply \\(\\rightarrow\\) combine pattern whenever we are processing data because it makes explicit what it is that we want to do. 5.3 Split: breaking the data into groups The first task is to organise our dataframe into the relevant groups. To do this we use group_by(): angry.moods %&gt;% group_by(Gender) %&gt;% head ## Source: local data frame [6 x 7] ## Groups: Gender [2] ## ## Gender Sports Anger.Out Anger.In Control.Out Control.In Anger.Expression ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1 18 13 23 20 36 ## 2 2 1 14 17 25 24 30 ## 3 2 1 13 14 28 28 19 ## 4 2 1 17 24 23 23 43 ## 5 1 1 16 17 26 28 27 ## 6 1 1 16 22 25 23 38 Weirdly, this doesn’t seem to have done anything. The data aren’t sorted by Gender, and there is no visible sign of the grouping, but stick with it… 5.4 Apply and combine Continuing the example above, once we have grouped our data we can then apply a function to it — for exmaple, summarise: angry.moods %&gt;% group_by(Gender) %&gt;% summarise( mean.anger.out=mean(Anger.Out) ) ## # A tibble: 2 × 2 ## Gender mean.anger.out ## &lt;int&gt; &lt;dbl&gt; ## 1 1 16.56667 ## 2 2 15.77083 And R and dplyr have done as we asked: split the data by Gender, using group_by() apply the summarise() function combine the results into a new data frame 5.5 A ‘real’ example In the previous section on datasets, we saw some found some raw data from a study which had measured depression with the PHQ-9. Patients were measured on numerous occasions (month is recorded) and were split into treatment and control groups: phq9.df &lt;- readr::read_csv(&quot;phq.csv&quot;) ## Parsed with column specification: ## cols( ## patient = col_integer(), ## phq9_01 = col_integer(), ## phq9_02 = col_integer(), ## phq9_03 = col_integer(), ## phq9_04 = col_integer(), ## phq9_05 = col_integer(), ## phq9_06 = col_integer(), ## phq9_07 = col_integer(), ## phq9_08 = col_integer(), ## phq9_09 = col_integer(), ## month = col_integer(), ## group = col_integer() ## ) glimpse(phq9.df) ## Observations: 2,429 ## Variables: 12 ## $ patient &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ... ## $ phq9_01 &lt;int&gt; 3, 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 3, ... ## $ phq9_02 &lt;int&gt; 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_03 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, ... ## $ phq9_04 &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, ... ## $ phq9_05 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, ... ## $ phq9_06 &lt;int&gt; 3, 2, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_07 &lt;int&gt; 3, 3, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 1, 1, ... ## $ phq9_08 &lt;int&gt; 0, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, ... ## $ phq9_09 &lt;int&gt; 2, 2, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, ... ## $ month &lt;int&gt; 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 0, 1,... ## $ group &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ... If this were our data we might want to: Calculate the sum of the PHQ-9 variables (the PHQ-9 score) Calculate the average PHQ-9 score at each month, and in each group Show these means by group for months 0, 7 and 12 Using only the commands above14 we can write: phq9.summary.df &lt;- phq9.df %&gt;% mutate(phq9 = phq9_01 + phq9_02 + phq9_03 + phq9_04 + phq9_05 + phq9_06 + phq9_07 + phq9_08 + phq9_09 ) %&gt;% select(patient, group, month, phq9) %&gt;% # remove rows with missing values filter(!is.na(phq9)) %&gt;% # split group_by(month, group) %&gt;% # apply and combine summarise(phq.mean = mean(phq9)) phq9.summary.df %&gt;% filter(month %in% c(0, 7, 12)) %&gt;% pander::pander() month group phq.mean 0 0 19.75904 0 1 18.97368 7 0 16.61818 7 1 13.42056 12 0 16.15385 12 1 12.54082 5.6 Sorting data Sorting data is easy with dplyr::arrange(): airquality %&gt;% arrange(Ozone) %&gt;% head ## Ozone Solar.R Wind Temp Month Day ## 1 1 8 9.7 59 5 21 ## 2 4 25 9.7 61 5 23 ## 3 6 78 18.4 57 5 18 ## 4 7 NA 6.9 74 5 11 ## 5 7 48 14.3 80 7 15 ## 6 7 49 10.3 69 9 24 By default sorting is ascending, but you can use a minus sign to reverse this: airquality %&gt;% arrange(-Ozone) %&gt;% head ## Ozone Solar.R Wind Temp Month Day ## 1 168 238 3.4 81 8 25 ## 2 135 269 4.1 84 7 1 ## 3 122 255 4.0 89 8 7 ## 4 118 225 2.3 94 8 29 ## 5 115 223 5.7 79 5 30 ## 6 110 207 8.0 90 8 9 You can sort on multiple columns too: airquality %&gt;% select(Month, Ozone) %&gt;% arrange(Month, -Ozone) %&gt;% head ## Month Ozone ## 1 5 115 ## 2 5 45 ## 3 5 41 ## 4 5 37 ## 5 5 36 ## 6 5 34 You might have noticed I sneaked something new in here: the call to pander(). This is a weirdly named but useful function when writing RMarkdown documents. It converts any R object into more readable output: here it makes a nice table for us in the compiled document. We cover more tips and tricks for formatting RMarkdown documents here. You might also want to check this page on missing values to explain the filter which uses !is.na(), but you could leave it for later.↩ "],
["graphics.html", "6 Graphics 6.1 Benefits of visualising data 6.2 Choosing a plot type 6.3 Choosing, layering and facetting graphics 6.4 ‘Quick and dirty’ plotting", " 6 Graphics Graphics are the best thing about R. The base system alone provides lots of useful plotting functions, but the ggplot2 package is exceptional in the consistent and powerful approach it takes to visualising data. This chapter focusses mostly on ggplot, but does include some pointers to other useful plotting functions. The R Graphics Cookbook It’s worth pointing out that the O’Reilly R Graphics cookbook is available as a pdf download here and is a much more comprehensive source than this page. Below are just some pointers to get you started. 6.1 Benefits of visualising data XXX TODO Psychology and human factors of graphics + Tufte. Importance of graphs to communicate. Motivating examples from RCTs. 6.2 Choosing a plot type Typically when setting out to plot data in R it pays to ask yourself whether you need: A quick way to visualise something specific – for example to check some feature of your data before you continue your analysis – without worrying too much about the detail of the graphic. A plot that is specifically designed to communicate your data effectively, and where you do care about the details of the final output. For the first case, where you already know what you want — for example to visualise a distribution of a single variable or to check diagnostics from a linear model — there are many useful built-in functions in base-R. For the second case — for example where you want to visualise the main outcomes of your study, or draw attention to specific aspects of your data — there is ggplot2. We’ll deal with the second case first, because using ggplot highlights many important aspects of plotting in general. 6.3 Choosing, layering and facetting graphics If you’ve never given much thought to data visualisation before, you might be surprised at the sheer variety of graphs types available. One way to cut through the multituyde of options is to determine what the purpose of your plot is. It your goal to show: Relationships Distributions Comparison Composition Figure 6.1: Examples of charts showing comaprisons, relationships, distribution and composition. The comparison, distribution and composition plots show 2 variables, but the relationship plot includes 3, increasing the density of the information displayed. There are various simple chart selection guides available online, of which these are quite nice examples: Chart selection guide (pdf)] ‘Show me the numbers’ chart guide (pdf) However, guides which attempt to be comprehensive and show you a full range of plot types are perhaps not as useful as those which reflect our knowledge of which plots are the most effective forms of communication. For example, almost all guides to plotting, and especially R textbooks, will show you how to plot a simple bar graph. But bar graphs have numerous disadvantages over other plots which can show the same information. Specifically, they are: low in information density (and so inefficient in use of space); they make comparisons between multiple series very difficult (for example in interacion plots); and, perhaps most importantly, even when they include error bars, readers consistently misinterpret the quantitative information in bar graphs (specifically, when bar graphs are used to display estimates which contain error, readers assume points above the bar are less likely than points within the bar, even though this is typically not the case). You should be guided in choosing plots not simply by mechanical rules based on the number or type of variables you want to display. Instead, you should be guided by the evidence from basic studies of human perception, and applied data on how different types of infromation displays are really used by readers. This guide is restricted to examples likely to be useful to experiemental and applied psychologists. 6.3.1 Using ggplot When using ggplot it helps to think of five separate steps, of which 2 are optional Choose the data you want to plot. Map variables to axes or other features on the plot. (Optionally) use ggplot functions to summarise your data before the plot is drawn (e.g. to calulate means and standard errors for point-range plots). Add visual display layers. (Optionally) Split the plot up (repeat it) into multiple panels. You can then customise the plot labels and title, and tweak other presentation parameters, although this often isn’t necessary unless sending a graphic for publication. You can also export graphics in multiple high quality formats. The simplest way to demonstrate these steps is with an example, and we begin with a plot showing the relationship betwen variables: 6.3.2 Relationships Problem to be solved: You want to check/show whether variables are related in a linear fashion before running linear regression: Step 1 is to select our data. As is typical in R, ggplot works best with long-format data. In the examples below we will use the mtcars dataset for convenience, so our first line of code is to use the dplyr pipe symbol (operator) to send the mtcars dataset to the next line of code: mtcars %&gt;% ... Step 2 is to map the variables we want to axes or other features of the plot (e.g. the colours of points, or the linetypes used in line plots). Here we tell ggplot to use disp (engine size) on the x axis, and mpg on the y axis. We also tell it to colour the points differently depending on the value of hp (engine horsepower). At this point ggplot will create and label the axes and plot area, but doesn’t yet display any of our data. For this we need to add visual display layers (we skip step 3 in this example, but will cover it in other examples below): mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) Step 4 To display data, we have to add a visual layer to the plot. For example, let’s say we want to make a scatter plot, and so draw points for each row of data: mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point() And we have a pretty slick graph: ggplot has now added points for each pair of disp and mpg values, and coloured them according to the value of hp (see choosing colours below XXX). What’s even neater about ggplot though is how easy it is to layer different visualisations of the same data. These visual layers are called geom’s and the functions which add them are all prefixed with geom_, so geom_point() for scatter plots, or geom_line() for line plots, or geom_smooth() for a smoothed line plot. We can add this to the scatter plot like so: mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point(size=2) + geom_smooth(se=F, colour=&quot;grey&quot;) ## `geom_smooth()` using method = &#39;loess&#39; In the example above, I have also customised the smoothed line, making it grey to avoid over-intrusion into our perception of the points. Often less is more when plotting graphs: not everything can be emphasised at once, and it’s important to make decisions about what should be given visual priority. Step 5 ‘Splitting up’ or repeating the plot. Very often, you will have drawn plot and think things like I wonder what that would look like if I drew it for men and women separately?. In ggplot this is called facetting, and is easy to achieve, provided your data are in a long format. Using the same mtcars example, let’s say we wanted separate panels for American vs. Foreign cars (information held in the am variable). We simply add the facet_wrap(), and specify the &quot;am&quot; variable: mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point(size=2) + geom_smooth(se=F, colour=&quot;grey&quot;) + facet_wrap(&quot;am&quot;) ## `geom_smooth()` using method = &#39;loess&#39; One trick is to make sure factors are labelled nicely, because these labels appear on the final plot. Here the mutate() call relabels the factor which makes the plot easier to read: mtcars %&gt;% mutate(american = factor(am, labels=c(&quot;American&quot;, &quot;Foreign&quot;))) %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point(size=2) + geom_smooth(se=F, colour=&quot;grey&quot;) + facet_wrap(&quot;american&quot;) ## `geom_smooth()` using method = &#39;loess&#39; 6.3.3 Distributions lme4::sleepstudy %&gt;% ggplot(aes(Reaction)) + geom_density() Imagine we wanted to compare distributions for individuals. Simply overlaying the lines is confusing: lme4::sleepstudy %&gt;% ggplot(aes(Reaction, group=Subject)) + geom_density() Facetting produces a nicer result: lme4::sleepstudy %&gt;% ggplot(aes(Reaction)) + geom_density() + facet_wrap(&quot;Subject&quot;) But we could present the same information more compactly, and with better facility to compare between subjects, if we use a bottleplot: lme4::sleepstudy %&gt;% ggplot(aes(Subject, Reaction)) + geom_violin() We might want to plot our Subjects in order of their mean RT: mean.ranked.sleep &lt;- lme4::sleepstudy %&gt;% group_by(Subject) %&gt;% # calculate mean RT mutate(RTm = mean(Reaction)) %&gt;% # sort by mean RT arrange(RTm, Days) %&gt;% ungroup() %&gt;% # create a rank score but conert to factor right away mutate(SubjectRank = factor(dense_rank(RTm))) mean.ranked.sleep %&gt;% ggplot(aes(SubjectRank, Reaction)) + geom_violin() + theme(aspect.ratio = .33) # change the aspect ratio to make long and wide Or we might want to compare individuals against the combined distribution: # duplicate all the data, assigning one-replication to a single subject, &quot;All&quot; sleep.repeat &lt;- bind_rows(lme4::sleepstudy, lme4::sleepstudy %&gt;% mutate(Subject=&quot;All&quot;)) ## Warning in bind_rows_(x, .id): binding factor and character vector, ## coercing into character vector sleep.repeat %&gt;% mutate(all = Subject==&quot;All&quot;) %&gt;% ggplot(aes(Subject, Reaction, color=all)) + geom_violin() + guides(colour=FALSE) + # turn off he legend because we don&#39;t really need it theme(aspect.ratio = .25) # change the aspect ratio to make long and wide Boxplots can also work well to show distributions, and have the advantage of showing the median explicitly: mean.ranked.sleep %&gt;% ggplot(aes(SubjectRank, Reaction)) + geom_boxplot() If we plot the same data by-day, we can clearly see the effect of sleep deprivation, and the increase in variability between subjects as sime goes on: the lack of sleeps seems to be affecting some subjects more than others lme4::sleepstudy %&gt;% ggplot(aes(factor(Days), Reaction)) + geom_boxplot() 6.3.4 Comparisons 6.3.5 Composition 6.3.6 Combining plots in a grid 6.3.7 Exporting high quality ggplots for print See page 323 of the R Graphics Cookbook 6.4 ‘Quick and dirty’ plotting When exploring a dataset, often useful to use built in functions or helpers from other libraries. These help you quickly visualise relationships, but aren’t always exactly what you need and can be hard to customise. 6.4.1 Distributions hist(mtcars$mpg) plot(density(mtcars$mpg)) boxplot(mpg~cyl, data=mtcars) Hmisc::hist.data.frame(mtcars) Even for simple plots, ggplot has some useful helper functions though: qplot(mpg, data=mtcars, geom=&quot;density&quot;) + xlab(&quot;Miles per gallon&quot;) qplot(x=factor(cyl), y=mpg, data=mtcars, geom=&quot;boxplot&quot;) 6.4.2 Relationships with(mtcars, plot(mpg, wt)) pairs(select(mtcars, wt, disp, mpg)) Again, for quick plots ggplot also has useful shortcut functions: qplot(mpg, wt, color=factor(cyl), data = mtcars) 6.4.3 Quantities I don’t think the base R plots are that convenient here. ggplot2:: and the stat_summary() function makes life much simpler: ggplot(mtcars, aes(factor(cyl), mpg)) + stat_summary(geom=&quot;bar&quot;) ## No summary function supplied, defaulting to `mean_se() And if you are plotting quantities, as disussed above, showing a range is sensible (a boxplot would also fill both definitions): ggplot(mtcars, aes(factor(cyl), mpg)) + stat_summary(geom=&quot;pointrange&quot;) ## No summary function supplied, defaulting to `mean_se() "],
["real-data.html", "7 Working with ‘real’ data 7.1 How to ‘tidy’ data 7.2 Deal with multiple files 7.3 Error checking 7.4 Missing values", " 7 Working with ‘real’ data Note: If you already have nicely formatted data ready for use in R then you could skip this section and revisit it later. Most tutorials and textbooks use neatly formatted example datasets to illustrate particular techniques. However in the real-world our data can be: In the wrong format Spread across multiple files Badly coded, or with errors Incomplete, with values missing for many different reasons This chapter shows you how to address each of these problems. 7.1 How to ‘tidy’ data Melt, spread … 7.2 Deal with multiple files File handling and import Writing a function for do() which returns a dataframe Joins and merges 7.3 Error checking 999, 666 and *: the marks of the beast! 7.4 Missing values is.na() and is.finite expand() "],
["correlations.html", "8 Correlations 8.1 Obtaining a correlation matrix 8.2 Making correlation tables for publication 8.3 Other types of correlation", " 8 Correlations The base R cor() function provides a simple way to get Pearson correlations, but to get a correlation matrix as you might expect from SPSS or Stata it’s best to use the corr.test() function in the psych package. Before you start though, plotting the correlations might be the best way of getting to grips with the patterns of relationship in your data. A pairs plot is a nice way of doing this: airquality %&gt;% select(-Month, -Day) %&gt;% pairs If we were satisfied the relationships were (reasonably) linear, we could also visualise correlations themselves with a ‘corrgram’, using the corrgram library: library(&quot;corrgram&quot;) airquality %&gt;% select(-Month, -Day) %&gt;% corrgram(lower.panel=corrgram::panel.ellipse, upper.panel=panel.cor, diag.panel=panel.density) Figure 6.1: A corrgram, showing pearson correlations (above the diagonal), variable distributions (on the diagonal) and ellipses and smoothed lines of best fit (below the diagnonal). Long, narrow ellipses denote large correlations; circular ellipses indicate small correlations. The ggpairs function from the GGally package is also a nice way of plotting relationships between a combination of categorical and continuous data - it packs a lot of information into a limited space: mtcars %&gt;% mutate(cyl = factor(cyl)) %&gt;% select(mpg, wt, drat, cyl) %&gt;% GGally::ggpairs() 8.1 Obtaining a correlation matrix The psych::corr.test() function is a quick way to obtain a pairwise correlation matrix for an entire dataset, along with p values and confidence intervals which the base R cor() function will not provide: mycorrelations &lt;- psych::corr.test(airquality) mycorrelations ## Call:psych::corr.test(x = airquality) ## Correlation matrix ## Ozone Solar.R Wind Temp Month Day ## Ozone 1.00 0.35 -0.60 0.70 0.16 -0.01 ## Solar.R 0.35 1.00 -0.06 0.28 -0.08 -0.15 ## Wind -0.60 -0.06 1.00 -0.46 -0.18 0.03 ## Temp 0.70 0.28 -0.46 1.00 0.42 -0.13 ## Month 0.16 -0.08 -0.18 0.42 1.00 -0.01 ## Day -0.01 -0.15 0.03 -0.13 -0.01 1.00 ## Sample Size ## Ozone Solar.R Wind Temp Month Day ## Ozone 116 111 116 116 116 116 ## Solar.R 111 146 146 146 146 146 ## Wind 116 146 153 153 153 153 ## Temp 116 146 153 153 153 153 ## Month 116 146 153 153 153 153 ## Day 116 146 153 153 153 153 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Ozone Solar.R Wind Temp Month Day ## Ozone 0.00 0.00 0.00 0.00 0.56 1.00 ## Solar.R 0.00 0.00 1.00 0.01 1.00 0.56 ## Wind 0.00 0.50 0.00 0.00 0.25 1.00 ## Temp 0.00 0.00 0.00 0.00 0.00 0.65 ## Month 0.08 0.37 0.03 0.00 0.00 1.00 ## Day 0.89 0.07 0.74 0.11 0.92 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option One thing to be aware of is that by default corr.test() produces p values that are adjusted for multiple comparisons in the top right hand triangle (i.e. above the diagonal). If you want the uncorrected values use the values below the diagonal (or pass adjust=FALSE when calling the function). 8.2 Making correlation tables for publication 8.2.1 apaTables If you want to produce output tables for publication the apaTables package might be useful. This block saves an APA formatted correlation table to an external Word document like this. library(apaTables) apa.cor.table(airquality, filename=&quot;Table1_APA.doc&quot;, show.conf.interval=F) ## ## ## Means, standard deviations, and correlations ## ## ## Variable M SD 1 2 3 4 5 ## 1. Ozone 42.13 32.99 ## ## 2. Solar.R 185.93 90.06 .35** ## ## 3. Wind 9.96 3.52 -.60** -.06 ## ## 4. Temp 77.88 9.47 .70** .28** -.46** ## ## 5. Month 6.99 1.42 .16 -.08 -.18* .42** ## ## 6. Day 15.80 8.86 -.01 -.15 .03 -.13 -.01 ## ## ## Note. * indicates p &lt; .05; ** indicates p &lt; .01. ## M and SD are used to represent mean and standard deviation, respectively. ## 8.2.2 By hand If you’re not bothered about strict APA foramt, you might still want to extract the r and p values as dataframes which can then be saved to a csv and opened in excel, or converted to a table by some other means. You can do this by storing the corr.test output in a variable, and the accessing the $r and $p values within it: mycorrelations &lt;- psych::corr.test(airquality) write.csv(mycorrelations$p, file=&quot;airquality-r-values.csv&quot;) mycorrelations$p ## Ozone Solar.R Wind Temp Month ## Ozone 0.000000e+00 0.0019724194 1.298162e-11 0.000000e+00 5.617870e-01 ## Solar.R 1.793109e-04 0.0000000000 1.000000e+00 7.517729e-03 1.000000e+00 ## Wind 9.272583e-13 0.4959552068 0.000000e+00 3.434076e-08 2.471060e-01 ## Temp 0.000000e+00 0.0007517729 2.641597e-09 0.000000e+00 7.231443e-07 ## Month 7.760010e-02 0.3663533509 2.745622e-02 6.026202e-08 0.000000e+00 ## Day 8.879425e-01 0.0702233769 7.387466e-01 1.076164e-01 9.221900e-01 ## Day ## Ozone 1.0000000 ## Solar.R 0.5617870 ## Wind 1.0000000 ## Temp 0.6456986 ## Month 1.0000000 ## Day 0.0000000 mycorrelations$r ## Ozone Solar.R Wind Temp Month ## Ozone 1.00000000 0.34834169 -0.60154653 0.6983603 0.164519314 ## Solar.R 0.34834169 1.00000000 -0.05679167 0.2758403 -0.075300764 ## Wind -0.60154653 -0.05679167 1.00000000 -0.4579879 -0.178292579 ## Temp 0.69836034 0.27584027 -0.45798788 1.0000000 0.420947252 ## Month 0.16451931 -0.07530076 -0.17829258 0.4209473 1.000000000 ## Day -0.01322565 -0.15027498 0.02718090 -0.1305932 -0.007961763 ## Day ## Ozone -0.013225647 ## Solar.R -0.150274979 ## Wind 0.027180903 ## Temp -0.130593175 ## Month -0.007961763 ## Day 1.000000000 You can also access the CI for each pariwise correlation as a table: mycorrelations$ci %&gt;% head() %&gt;% pander() lower r upper p Ozone-Slr.R 0.17 0.35 0.5 0 Ozone-Wind -0.71 -0.6 -0.47 0 Ozone-Temp 0.59 0.7 0.78 0 Ozone-Month -0.018 0.17 0.34 0.078 Ozone-Day -0.2 -0.013 0.17 0.89 Slr.R-Wind -0.22 -0.057 0.11 0.5 8.3 Other types of correlation By default corr.test produces Pearson correlations, but You can pass the method argument psych::corr.test(): psych::corr.test(airquality, method=&quot;spearman&quot;) psych::corr.test(airquality, method=&quot;kendall&quot;) "],
["t-tests.html", "9 t-tests 9.1 Running t-tests 9.2 2 independent groups: 9.3 Unequal variances 9.4 Paired samples 9.5 One-sample test", " 9 t-tests Before you run any tests it’s worth plotting your data. Assuming you have a continuous outcome and categorical (binary) predictor (here we use a subset of the built in chickwts data), a boxplot can work well: chicks.eating.beans &lt;- chickwts %&gt;% filter(feed %in% c(&quot;horsebean&quot;, &quot;soybean&quot;)) chicks.eating.beans %&gt;% ggplot(aes(feed, weight)) + geom_boxplot() Figure 9.1: The box in a boxplot indictes the IQR; the whisker indicates the min/max values or 1.5 * the IQR, whichever is the smaller. If there are outliers beyond 1.5 * the IQR then they are shown as points. Or a violin or bottle plot, which shows the distributions within each group: chicks.eating.beans %&gt;% ggplot(aes(feed, weight)) + geom_violin() Layering boxes and bottles can work well too because it combines information about the distribution with key statistics like the median and IQR, and also because it scales reasonably well to multiple categories: chickwts %&gt;% ggplot(aes(feed, weight)) + geom_violin() + geom_boxplot(width=.1) + xlab(&quot;&quot;) Bottleplots are just density plots, turned 90 degrees. Density plots might be more familiar to some, but it’s hard to show more than 2 or 3 categories: chicks.eating.beans %&gt;% ggplot(aes(weight, fill=feed)) + geom_density(alpha=.5) And density plots are just smoothed histograms (which you might prefer if you’re a fan of 80’s computer games): chicks.eating.beans %&gt;% ggplot(aes(weight)) + geom_histogram(bins=7) + facet_grid(feed ~ .) 9.1 Running t-tests Assuming you really do still want to run a null hypothesis test on one or two means, the t.test() function performs most common variants, illustrated below. 9.2 2 independent groups: # the t.test function is a bit crufty and won&#39;t accept a data= parameter # for all types of test. Instead we use the with(...) function to make # the variables in the chicks.eating.beans datafram available to the # t.test function. with(chicks.eating.beans, t.test(weight ~ feed)) ## ## Welch Two Sample t-test ## ## data: weight by feed ## t = -4.5543, df = 21.995, p-value = 0.0001559 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -125.49476 -46.96238 ## sample estimates: ## mean in group horsebean mean in group soybean ## 160.2000 246.4286 Or equivalently, if your data are untidy and each group has it’s own column: untidy.chicks &lt;- chicks.eating.beans %&gt;% mutate(chick = row_number()) %&gt;% reshape2::dcast(chick~feed, value.var = &#39;weight&#39;) with(untidy.chicks, t.test(horsebean, soybean)) ## ## Welch Two Sample t-test ## ## data: horsebean and soybean ## t = -4.5543, df = 21.995, p-value = 0.0001559 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -125.49476 -46.96238 ## sample estimates: ## mean of x mean of y ## 160.2000 246.4286 9.3 Unequal variances By default R assumes your groups have unequal variances and applies an appropriate correction. If you don’t want this you can add var.equal = TRUE and get a vanilla t-test: with(untidy.chicks, t.test(horsebean, soybean, var.equal=TRUE)) ## ## Two Sample t-test ## ## data: horsebean and soybean ## t = -4.3037, df = 22, p-value = 0.0002873 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -127.78018 -44.67696 ## sample estimates: ## mean of x mean of y ## 160.2000 246.4286 9.4 Paired samples a &lt;- rnorm(50, 2.5, 1) b = a + rnorm(50, .5, 1) t.test(a, b, paired=TRUE) ## ## Paired t-test ## ## data: a and b ## t = -3.9526, df = 49, p-value = 0.0002481 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.7844081 -0.2556303 ## sample estimates: ## mean of the differences ## -0.5200192 Note that we could also ‘melt’ the data into long format and use the paired=TRUE argument with a formula: long.form.data &lt;- data_frame(a=a, b=b) %&gt;% tidyr::gather() with(long.form.data, t.test(value~key, paired=TRUE)) ## ## Paired t-test ## ## data: value by key ## t = -3.9526, df = 49, p-value = 0.0002481 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.7844081 -0.2556303 ## sample estimates: ## mean of the differences ## -0.5200192 9.5 One-sample test i.e. comparing sample mean with a specific value: # test if mean of `outcome` variable is different from 2 somedata &lt;- rnorm(50, 2.5, 1) t.test(somedata, mu=2) ## ## One Sample t-test ## ## data: somedata ## t = 4.4114, df = 49, p-value = 5.623e-05 ## alternative hypothesis: true mean is not equal to 2 ## 95 percent confidence interval: ## 2.313335 2.837658 ## sample estimates: ## mean of x ## 2.575496 "],
["linear-models-simple.html", "10 Introduction to linear models in R 10.1 Differences between R and other packages it’s important to be aware of 10.2 Describing statistical models using formulae 10.3 Running a linear model 10.4 More on formulas 10.5 What next", " 10 Introduction to linear models in R This section assumes most readers will have done an introductory statistics course and had practce running multiple regression and or Anova in SPSS or a similar package. 10.1 Differences between R and other packages it’s important to be aware of It’s important to note up-front that R has different defaults to many other statistical packages. This is especially relevant for the anova() function: R’s default settings will not match those of SPSS or Stata, and many researchers will want to change these defaults. Specifically, R will require that you think about: How should factors be coded when calculating contrasts for categorical variables? What type of sums of squares (I, II, or III) do you want when calculating the F test for an Anova? How should the error term be specified for repeated measured Anova? These issues are covered later in detail in later sections. 10.2 Describing statistical models using formulae R requires that you are explicit about the statistical model you want to run but provides a neat, concise way of describing models, called a formula. For multiple regression and simple Anova, the formulas we write map closely onto the underlying linear model. The formula syntax provides shortcuts to quickly describe all the models you are likely to need. Formulas have two parts: the left hand side and the right hand side, which are separated by the tilde symbol: ~. Here, the tilde just means ‘is predicted by’. For example, this formula specifies a regression model where height is the outcome, and age and gender are the predictor variables.15 height ~ age + gender There are lots more useful tricks to learn when writing formulas, which are covered below. But in the interests of instant gratification let’s work through a simple example first: 10.3 Running a linear model Linear models (including Anova and multiple regression) are run using the lm(...) function, short for ‘linear model’. We will use the mtcars dataset, which is built into R, for our first example. First, we have a quick look at the data. The pairs plot suggests that mpg might be related to a number of the other variables including disp (engine size) and wt (car weight): mtcars %&gt;% select(mpg, disp, wt) %&gt;% pairs Before running any model, we should ask outselves: “what question we are trying to answer?” In this instance, we can see that both weight (wt) and engine size (disp) are related to mpg, but they are also correlated with one another. We might want to know, then, “are weight and engine size independent predictors of mpg?” That is, if we know a car’s weight, do we gain additional information about it’s mpg by measuring engine size? To answer this, we could use multiple regression, including both wt and disp as predictors of mpg. The formula for this model would be mpg ~ wt + disp. The command below runs the model: lm(mpg ~ wt + disp, data=mtcars) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Coefficients: ## (Intercept) wt disp ## 34.96055 -3.35083 -0.01772 For readers used to wading through reams of SPSS output R might seem concise to the point of rudeness. By default, the lm commands displays very little, only repeating the formula and listing the coefficients for each predictor in the model. So what next? Unlike SPSS, we must be explicit and tell R exactly what we want. The most convenient way to do this is to first store the results of the lm() function: m.1 &lt;- lm(mpg ~ wt + disp, data=mtcars) This stores the results of lm in a variable named m.1. As an aside, this is a pretty terrible variable name — try to give descriptive names to your variables because this will prevent errors and make your code easier to read. We can then use other functions to get more information about the model. For example: summary(m.1) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## wt -3.35082 1.16413 -2.878 0.00743 ** ## disp -0.01773 0.00919 -1.929 0.06362 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Although still compact, the summary function provides some familiar output, including the estimate, SE, and p value for each parameter. Take a moment to find the following statistics in the output above: The coefficients and p values for each predictor The R2 for the overall model. What % of variance in mpg is explained? Answer the original question: ‘accounting for weight (wt), does engine size (disp) tell us anything extra about a car’s mpg?’ 10.4 More on formulas Above we briefly introduced R’s formula syntax. Formulas for linear models have the following structure: left_hand_side ~ right_hand_side For linear models the left side is our outcome, which is must be a continous variable. For categorical or binary outcomes you need to use glm() function, rather than lm(). See the section on generalised linear models) for more details. The right hand side of the formula lists our predictors. In the example above we used the + symbol to separate the predictors wt and disp. This told R to simply add each predictor to the model. However, many times we want to specify relationships between our predictors, as well as between predictors and outcomes. For example, we might want to run an Anova with 2 categorical predictors, each with 2 levels — that is, a 2x2 between-subjects design. Below, we define and run a linear model with both vs and am as predictors, along with the interaction of vs:am. We save this model as m.2, and use the summary command to print the coefficients. m.2 &lt;- lm(mpg ~ vs + am + vs:am, data=mtcars) summary(m.2) ## ## Call: ## lm(formula = mpg ~ vs + am + vs:am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.971 -1.973 0.300 2.036 6.250 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.050 1.002 15.017 6.34e-15 *** ## vs 5.693 1.651 3.448 0.0018 ** ## am 4.700 1.736 2.708 0.0114 * ## vs:am 2.929 2.541 1.153 0.2589 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.472 on 28 degrees of freedom ## Multiple R-squared: 0.7003, Adjusted R-squared: 0.6682 ## F-statistic: 21.81 on 3 and 28 DF, p-value: 1.735e-07 10.4.1 Other formula shortcuts In addition to the + symbol, we can use other shortcuts to create linear models. As seen above, the colon (:) operator indicates the interaction between two terms. So a:b is equivalent to creating a new variable in the data frame where a is multiplied by b. The * symbol indicates the expansion of other terms in the model. So, a*b is the equivalent of a + b + a:b. Finally, it’s good to know that other functions can be used within R formulas to save work. For example, if you wanted to transform your dependent variable then log(y) ~ x will do what you might expect, and saves creating temporary variables in your dataset. The formula syntax is very powerful, and the above only shows the basics, but you can read the formulae help pages in RStudio for more details. As an exercise, run the following models using the mtcars dataset: With mpg as the outcome, and with cyl and hp as predictors As above, but adding the interaction of cyl and hp. Repeat the model above, but write the formula a different way (make the formula either more or less explicit, but retaining the same predictors in the model). 10.5 What next It is strongly recommended that you read the section on Anova before doing anything else. As noted above, R has a number of important differences in it’s default settings, as compared with packages like Stata or SPSS. These can make important differences to the way you interpret the output of linear models, especially Anova-type models with categorical predictors. I avoid the terms dependent/independent variables because they are confusing to many students, and because they are misleading when discussing non-experimental data.↩ "],
["anova.html", "11 Anova in R 11.1 Differences between Anova in R v.s. SPSS or Stata Important difference 1: Variable codings and contrasts Important difference 2: Which sums of squares? Anova in R for SPSS users What’s wrong with the base R anova() function? What’s wrong with the SPSS defaults? Important difference 3: R works on long-format data (so you must be careful with repeated measures models) 11.2 General Anova recommendations", " 11 Anova in R This section attempts to cover in a high level way how to specify anova models in R and some of the issues in interpreting the model output. If you just want the ‘answers’ — i.e. the syntax to specify common Anova models – you could skip to the next section: Anova cookbook 11.1 Differences between Anova in R v.s. SPSS or Stata As noted previously: R can run (pretty much) any Anova model you want but. R’s default settings for Anova are different to those SPSS or Stata. Most people probably do want to change these default settings, or use an R package that does it for you. Important difference 1: Variable codings and contrasts You may have heard it said before that Anova is just a special case of regression, but not fully explored the links between Anova and regression models. One of the distinctive features of R is that, unlike SPSS, it tends to do things in a ‘regressiony’ way. In fact, you typically specify your Anova model in regression first, then call the anova() function on it to calculate the familiar F tests, etc. This means: You need to work out kind of model you actually want (this is a good thing!) You sometimes have to work harder to make sure R uses categorical variables properly By default R assumes all numeric variables are continuous linear predictors of the outcome. If you have a categorical variable coded as numbers you need to use the factor() function to make sure R knows it is categorical. As a conseuqence, this model: # model 1 summary(lm(mpg ~ cyl, data=mtcars)) ## ## Call: ## lm(formula = mpg ~ cyl, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.9814 -2.1185 0.2217 1.0717 7.5186 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.8846 2.0738 18.27 &lt; 2e-16 *** ## cyl -2.8758 0.3224 -8.92 6.11e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.206 on 30 degrees of freedom ## Multiple R-squared: 0.7262, Adjusted R-squared: 0.7171 ## F-statistic: 79.56 on 1 and 30 DF, p-value: 6.113e-10 Is completely different to this model: # model 2 summary(lm(mpg ~ factor(cyl), data=mtcars)) ## ## Call: ## lm(formula = mpg ~ factor(cyl), data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.2636 -1.8357 0.0286 1.3893 7.2364 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 26.6636 0.9718 27.437 &lt; 2e-16 *** ## factor(cyl)6 -6.9208 1.5583 -4.441 0.000119 *** ## factor(cyl)8 -11.5636 1.2986 -8.905 8.57e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.223 on 29 degrees of freedom ## Multiple R-squared: 0.7325, Adjusted R-squared: 0.714 ## F-statistic: 39.7 on 2 and 29 DF, p-value: 4.979e-09 The differences are: In model 1, cyl is treated as a continuous predictor. One parameter is used, and cyl is assumed to be a linear predictor of mpg. In model 2, cyl is treated as a categorical predictor and is dummy coded (sometimes called treatment-coded). If there are 3 categories (i.e. 4, 6 or 8 cylinders) then there will be 2 parameters used which relate to 2 of the 3 categories. The intercept represents the estimate for the category which has been left out — i.e., it is the base or reference category. Important difference 2: Which sums of squares? Let’s imagine we have an experiment in which we measure reaction times (rt) under 2 between-subjects conditions: A and B. A has two levels, low or high. fake.experiment &lt;- expand.grid( # expand grid will create 1 row for every combination # of these variables i = 1:20, A = c(&quot;low&quot;, &quot;high&quot;), B = c(&quot;low&quot;, &quot;high&quot;)) %&gt;% # make the outcome variable as a function of A and B mutate(outcome = rnorm(n()) + # main effect of A 1 * (A == &quot;high&quot;) + # main effect of B # .5 * (B == &quot;med&quot;) + 1 * (B == &quot;high&quot;) + # interaction between A:B -2 * (A==&quot;high&quot;) * (B == &quot;high&quot;)) To add a twist though, let’s say our data not balanced: for example, that we don’t collect as many cases where both A and B are high. The following code deletes approximately 50% of these cases: fake.experiment.unbalanced &lt;- fake.experiment %&gt;% # create a marker, which is==1 50% of the time, # when both A and B == high mutate(droprow = rbinom(n(), 1, prob = (A==&quot;high&quot; &amp; B == &quot;high&quot;) * .5)) %&gt;% # get rid of marked rows filter(droprow != 1) To check that the data really are unbalanced now, we can use the table() function which builds a contingency table: fake.experiment.unbalanced %&gt;% select(A, B) %&gt;% table ## B ## A low high ## low 20 20 ## high 20 11 We can plot these data and see that the (large) interaction we specified is obvious: fake.experiment.unbalanced %&gt;% ggplot(aes(A, outcome, color=B, group=B)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_se) + stat_summary(geom=&quot;line&quot;, fun.data=mean_se) After plotting the data, we might want to run a 2x2 Anova to test whether this interaction is statistically significant. This call to lm() specifies the model, and summary() shows that the lm() model included: The intercept, which represents cases where A and B are ‘low’. The parameter Ahigh, which represents the difference between A==low v.s. A==high when B==low The parameter Bhigh, which represents the difference between B==low v.s. B==high, when A==low The interaction term, Ahigh:Bhigh, which represents the difference in outcome when both A==high AND B==high. The model output is below: fake.expt.model &lt;- lm(outcome ~ A * B, data=fake.experiment.unbalanced) summary(fake.expt.model) ## ## Call: ## lm(formula = outcome ~ A * B, data = fake.experiment.unbalanced) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.71684 -0.62706 -0.04711 0.58437 2.71553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1389 0.2382 -0.583 0.56168 ## Ahigh 0.8476 0.3368 2.516 0.01426 * ## Bhigh 0.9920 0.3368 2.945 0.00444 ** ## Ahigh:Bhigh -1.1504 0.5228 -2.200 0.03123 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.065 on 67 degrees of freedom ## Multiple R-squared: 0.1318, Adjusted R-squared: 0.09294 ## F-statistic: 3.391 on 3 and 67 DF, p-value: 0.02291 The problem with this regression output (i.e. from the summary() function) is that we can only see what are often called the simple contrasts. The parameters represent pairwise contrasts between two cells of the design, and don’t test the overall effect of A or B. Rather than using multiple, pairwise comparisons, we use Anova to provide a test of the combined effect of A, B, and the interaction of A and B. Anova in R for SPSS users At this point I recognise SPSS users’ main concern will be to reproduce the output SPSS would give by default. The code below uses the Anova() function from the car:: package. This function is a replacement for the built in anova() function (note lowercase ‘a’) but adds some additional features, including the ability to specify the type of sums of squares used for the F tests reported: car::Anova(fake.expt.model, type=3) ## Anova Table (Type III tests) ## ## Response: outcome ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 0.386 1 0.3402 0.561678 ## A 7.184 1 6.3318 0.014261 * ## B 9.840 1 8.6720 0.004439 ** ## A:B 5.494 1 4.8418 0.031230 * ## Residuals 76.022 67 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 This table should match exactly the result obtained from a 2x2 Anova in SPSS. If, however, we had instead used the base anova() function we would see this output: anova(fake.expt.model) ## Analysis of Variance Table ## ## Response: outcome ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## A 1 1.524 1.5240 1.3431 0.25060 ## B 1 4.524 4.5242 3.9873 0.04991 * ## A:B 1 5.494 5.4937 4.8418 0.03123 * ## Residuals 67 76.022 1.1347 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here the ‘main effects’ are not significant, and the interaction has a different p value from that SPSS would produce. What’s wrong with the base R anova() function? This is a good question. The short answer is that: There’s nothing wrong with it per-se, but R tests a different hypothesis than SPSS by default, and It’s probably not the hypothesis you want You might prefer to report F tests using type 2 or 3 sums of squares You can get these with the car::Anova() function instead of the base anova() function. A longer answer is given in this online discussion on stats.stackechange.com and practical implications are shown in this worked example. An even longer answer, including a much deeper exploration of the philosophical questions involved is given by Venables (1998). To hammer home the importance of not using the default anova() settings without careful thought, consider the following model output: Example 1, mirroring SPSS default settings, and using type 3 sums of squares: car::Anova(lm(outcome ~ A + B + A:B, data=fake.experiment.unbalanced), type=3) ## Anova Table (Type III tests) ## ## Response: outcome ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 0.386 1 0.3402 0.561678 ## A 7.184 1 6.3318 0.014261 * ## B 9.840 1 8.6720 0.004439 ** ## A:B 5.494 1 4.8418 0.031230 * ## Residuals 76.022 67 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Example 2. using the standard anova() function. The model specifies A first: anova(lm(outcome ~ A * B, data=fake.experiment.unbalanced)) ## Analysis of Variance Table ## ## Response: outcome ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## A 1 1.524 1.5240 1.3431 0.25060 ## B 1 4.524 4.5242 3.9873 0.04991 * ## A:B 1 5.494 5.4937 4.8418 0.03123 * ## Residuals 67 76.022 1.1347 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Example 3: as above, but with B specified first anova(lm(outcome ~ B * A, data=fake.experiment.unbalanced)) ## Analysis of Variance Table ## ## Response: outcome ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## B 1 3.707 3.7068 3.2669 0.07518 . ## A 1 2.341 2.3415 2.0636 0.15551 ## B:A 1 5.494 5.4937 4.8418 0.03123 * ## Residuals 67 76.022 1.1347 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 In the first example, car::Anova() provides the same output as SPSS. This is probably what you want, at least in the first instance. In examples 2 and 3, the main effects are ‘not significant’, even though in our simulated data we did create ‘main effects’ of A and B. To make matters worse,in examples 2 and 3, the order in which we included the predictors A and B changes the F statistics for the main effects. Using a method where our results depend on the order in which we include the factors can lead to confusion, and is best avoided (although see the discussion below for why interpreting main effects in the presence of interactions might be a bad idea anyway). What’s wrong with the SPSS defaults? Interpreting main effects when there is a significant interaction between predictors doesn’t always make sense, so it’s debateable whether the p values for the main effects in a model containing interactions are useful. For example, if we look at a bar plot of outcome by A, the confidence intervals are so wide that we wouldn’t want to draw any firm conclusions: fake.experiment.unbalanced %&gt;% ggplot(aes(A, outcome, group=1)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_cl_normal) + stat_summary(geom=&quot;line&quot;, fun.data=mean_cl_normal) + ylab(&quot;Outcome (95% CI)&quot;) But if we draw the same plot for each level of B, it becomes clear that it wouldn’t make sense to argue there is a ‘main effect’ of A anyway, because the direction of the effect is different for each level of B: fake.experiment.unbalanced %&gt;% ggplot(aes(A, outcome, group=1)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_cl_normal) + stat_summary(geom=&quot;line&quot;, fun.data=mean_cl_normal) + ylab(&quot;Outcome (95% CI)&quot;) + facet_grid(~paste(&quot;B&quot;, B)) Where there is an interaction, the meaning of the main effects changes. The moral of the story is that plotting your data and restating the effect in substantive terms are always crucial. Important difference 3: R works on long-format data (so you must be careful with repeated measures models) In R, data tend to be most useful in long format where: each row of the dataframe corresponds to a single measurement occasion each column corresponds to a variable which is measured For example, in R we might have data like this: df %&gt;% head %&gt;% pander person time predictor outcome 1 1 2 6 1 2 2 11 1 3 2 10 2 1 2 9 2 2 2 5 2 3 2 4 Whereas in SPSS we might have the same data structured like this: df.wide %&gt;% head %&gt;% pander person predictor Time 1 Time 2 Time 3 1 2 6 11 10 2 2 9 5 4 3 2 6 6 8 4 4 10 11 7 5 4 11 3 7 6 1 7 12 16 What this means is that there is no automatic way for R to know which rows belong to which person (assuming individual people are the unit of error in your model). If you are running a model where individuals are sampled more than once — for example in a repeated measures design — then you need to be explicit about which variables identifies subjects in your data. For example, in the lme4::sleepstudy data, Reaction time is measuresd for each Subject on 10 Day’s: lme4::sleepstudy %&gt;% head(12) %&gt;% pander Reaction Days Subject 249.6 0 308 258.7 1 308 250.8 2 308 321.4 3 308 356.9 4 308 414.7 5 308 382.2 6 308 290.1 7 308 430.6 8 308 466.4 9 308 222.7 0 309 205.3 1 309 lme4::sleepstudy %&gt;% ggplot(aes(factor(Days), Reaction)) + geom_boxplot() + xlab(&quot;Days&quot;) + ylab(&quot;RT (ms)&quot;) Let’s say we want to test whether there were differences in distance at different ages. The following model is inappropriate because it ignores that distance is measured repeatedly from the same subject: stupid.model &lt;- lm(Reaction~factor(Days), data=lme4::sleepstudy) anova(stupid.model) ## Analysis of Variance Table ## ## Response: Reaction ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## factor(Days) 9 166235 18470.6 7.8164 1.317e-09 *** ## Residuals 170 401719 2363.1 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Instead, I’d recommend you run a linear mixed model using the lmer() function. more.sensible.model &lt;- lmer(Reaction ~ factor(Days) + (1|Subject), data=lme4::sleepstudy) lmerTest::anova(more.sensible.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## factor(Days) 166235 18471 9 153 18.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 It is possible to run traditional repeat-measures Anova in R using the afex package (and others, athough this is the best) but it probably isn’t worth the effort (see the Anova cookbook section on repeat measures data for details). 11.2 General Anova recommendations As should be obvious from the above: Use lm() to specify your model. Use the car::Anova() function to get the Anova table, using type=3 until you have thought more about it, and want something different. Where there is an interaction (even if it is not quite statistically significant) be careful about interpreting main effects in your model. Keep your data in long form and be careful when specifying repeat measures Anova Use mixed models rather than repeat measures Anova unless you have a special reason. References "],
["anova-cookbook.html", "12 Anova ‘Cookbook’ 12.1 Between subjects designs 12.2 Repeated measures or ‘split plot’ designs", " 12 Anova ‘Cookbook’ This section is intended as a shortcut to running Anova for a variety of common types of model. If you want to understand more about what you are doing, read the section on principles of Anova in R. 12.1 Between subjects designs 12.1.1 Factorial anova, no bigger than 2x2 12.1.2 Factorial anova, where one factor has &gt; 2 levels. We are using a dataset from Howell (REF), chapter 13 which recorded Recall among young v.s. older adults (Age) for each of 5 conditions. This data would commonly be plotted something like this: eysenck &lt;- readRDS(&quot;data/eysenck.Rdata&quot;) eysenck %&gt;% ggplot(aes(Condition, Recall, group=Age, color=Age)) + stat_summary(geom=&quot;pointrange&quot;, fun.data = mean_cl_boot) + ylab(&quot;Recall (95% CI)&quot;) + xlab(&quot;&quot;) Or alternatively if we wanted to provde a better summary of the distribution of the raw data we could use a boxplot: eysenck %&gt;% ggplot(aes(Age, Recall)) + geom_violin() + geom_boxplot(width=.33) + facet_grid(~Condition) + ylab(&quot;Recall (95% CI)&quot;) + xlab(&quot;&quot;) Figure 12.1: Combined box and density plot for recall in older and young adults, by condition. We can run a linear model including the effect of Age and Condition and the interaction of these variables, and calculate the Anova: eysenck.model &lt;- lm(Recall~Age*Condition, data=eysenck) car::Anova(eysenck.model, type=3) ## Anova Table (Type III tests) ## ## Response: Recall ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 490.00 1 61.0550 9.85e-12 *** ## Age 1.25 1 0.1558 0.6940313 ## Condition 351.52 4 10.9500 2.80e-07 *** ## Age:Condition 190.30 4 5.9279 0.0002793 *** ## Residuals 722.30 90 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If we want to check assumptions of the model are met, these tables and plots would be a reasonable place to start: car::leveneTest(eysenck.model) %&gt;% pander() Levene’s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 9 1.031 0.4217 90 NA NA car::qqPlot(eysenck.model) Figure 12.2: QQ plot to assess normality of model residuals # we have to make a dataframe containing the fitted values and residuals first data_frame( fitted = predict(eysenck.model), residual = residuals(eysenck.model)) %&gt;% # and then plot points and a smoothed line ggplot(aes(fitted, residual)) + geom_point() + geom_smooth(se=F) ## `geom_smooth()` using method = &#39;loess&#39; Figure 12.3: Residual vs fitted (spread vs. level) plot to check homogeneity of variance. If we want to look at post-hoc pairwise tests we can use the the lsmeans() function from the lsmeans:: package: lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition) ## $lsmeans ## Age Condition lsmean SE df lower.CL upper.CL ## Young Counting 7.0 0.8958547 90 5.220228 8.779772 ## Older Counting 6.5 0.8958547 90 4.720228 8.279772 ## Young Rhyming 6.9 0.8958547 90 5.120228 8.679772 ## Older Rhyming 7.6 0.8958547 90 5.820228 9.379772 ## Young Adjective 11.0 0.8958547 90 9.220228 12.779772 ## Older Adjective 14.8 0.8958547 90 13.020228 16.579772 ## Young Imagery 13.4 0.8958547 90 11.620228 15.179772 ## Older Imagery 17.6 0.8958547 90 15.820228 19.379772 ## Young Intention 12.0 0.8958547 90 10.220228 13.779772 ## Older Intention 19.3 0.8958547 90 17.520228 21.079772 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Young,Counting - Older,Counting 0.5 1.26693 90 0.395 1.0000 ## Young,Counting - Young,Rhyming 0.1 1.26693 90 0.079 1.0000 ## Young,Counting - Older,Rhyming -0.6 1.26693 90 -0.474 1.0000 ## Young,Counting - Young,Adjective -4.0 1.26693 90 -3.157 0.0633 ## Young,Counting - Older,Adjective -7.8 1.26693 90 -6.157 &lt;.0001 ## Young,Counting - Young,Imagery -6.4 1.26693 90 -5.052 0.0001 ## Young,Counting - Older,Imagery -10.6 1.26693 90 -8.367 &lt;.0001 ## Young,Counting - Young,Intention -5.0 1.26693 90 -3.947 0.0058 ## Young,Counting - Older,Intention -12.3 1.26693 90 -9.709 &lt;.0001 ## Older,Counting - Young,Rhyming -0.4 1.26693 90 -0.316 1.0000 ## Older,Counting - Older,Rhyming -1.1 1.26693 90 -0.868 0.9970 ## Older,Counting - Young,Adjective -4.5 1.26693 90 -3.552 0.0205 ## Older,Counting - Older,Adjective -8.3 1.26693 90 -6.551 &lt;.0001 ## Older,Counting - Young,Imagery -6.9 1.26693 90 -5.446 &lt;.0001 ## Older,Counting - Older,Imagery -11.1 1.26693 90 -8.761 &lt;.0001 ## Older,Counting - Young,Intention -5.5 1.26693 90 -4.341 0.0015 ## Older,Counting - Older,Intention -12.8 1.26693 90 -10.103 &lt;.0001 ## Young,Rhyming - Older,Rhyming -0.7 1.26693 90 -0.553 0.9999 ## Young,Rhyming - Young,Adjective -4.1 1.26693 90 -3.236 0.0511 ## Young,Rhyming - Older,Adjective -7.9 1.26693 90 -6.236 &lt;.0001 ## Young,Rhyming - Young,Imagery -6.5 1.26693 90 -5.131 0.0001 ## Young,Rhyming - Older,Imagery -10.7 1.26693 90 -8.446 &lt;.0001 ## Young,Rhyming - Young,Intention -5.1 1.26693 90 -4.025 0.0044 ## Young,Rhyming - Older,Intention -12.4 1.26693 90 -9.787 &lt;.0001 ## Older,Rhyming - Young,Adjective -3.4 1.26693 90 -2.684 0.1963 ## Older,Rhyming - Older,Adjective -7.2 1.26693 90 -5.683 &lt;.0001 ## Older,Rhyming - Young,Imagery -5.8 1.26693 90 -4.578 0.0006 ## Older,Rhyming - Older,Imagery -10.0 1.26693 90 -7.893 &lt;.0001 ## Older,Rhyming - Young,Intention -4.4 1.26693 90 -3.473 0.0260 ## Older,Rhyming - Older,Intention -11.7 1.26693 90 -9.235 &lt;.0001 ## Young,Adjective - Older,Adjective -3.8 1.26693 90 -2.999 0.0950 ## Young,Adjective - Young,Imagery -2.4 1.26693 90 -1.894 0.6728 ## Young,Adjective - Older,Imagery -6.6 1.26693 90 -5.209 0.0001 ## Young,Adjective - Young,Intention -1.0 1.26693 90 -0.789 0.9986 ## Young,Adjective - Older,Intention -8.3 1.26693 90 -6.551 &lt;.0001 ## Older,Adjective - Young,Imagery 1.4 1.26693 90 1.105 0.9830 ## Older,Adjective - Older,Imagery -2.8 1.26693 90 -2.210 0.4578 ## Older,Adjective - Young,Intention 2.8 1.26693 90 2.210 0.4578 ## Older,Adjective - Older,Intention -4.5 1.26693 90 -3.552 0.0205 ## Young,Imagery - Older,Imagery -4.2 1.26693 90 -3.315 0.0411 ## Young,Imagery - Young,Intention 1.4 1.26693 90 1.105 0.9830 ## Young,Imagery - Older,Intention -5.9 1.26693 90 -4.657 0.0005 ## Older,Imagery - Young,Intention 5.6 1.26693 90 4.420 0.0011 ## Older,Imagery - Older,Intention -1.7 1.26693 90 -1.342 0.9409 ## Young,Intention - Older,Intention -7.3 1.26693 90 -5.762 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 10 estimates By default Tukey correction is applied for multiple comparisons which is a reasonable default. If you want to use other methods (e.g. to use false discovery rate adjustment) you can use the adjust argument. In the code below we use the broom::tidy() function to convert the table into a dataframe, and then show only the first 6 rows as a table in RMarkdown: # calculate pairwise contrasts eysenck.fdr &lt;- lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition, adjust=&quot;fdr&quot;) # show first 6 rows from this long table eysenck.fdr$contrasts %&gt;% broom::tidy() %&gt;% head(6) %&gt;% pander(caption=&quot;First 6 rows of the pairwise contrasts with FDR-adjusted p values&quot;) First 6 rows of the pairwise contrasts with FDR-adjusted p values level1 level2 estimate std.error df statistic p.value Young,Counting Older,Counting 0.5 1.267 90 0.3947 0.7263 Young,Counting Young,Rhyming 0.1 1.267 90 0.07893 0.9373 Young,Counting Older,Rhyming -0.6 1.267 90 -0.4736 0.6824 Young,Counting Young,Adjective -4 1.267 90 -3.157 0.003251 Young,Counting Older,Adjective -7.8 1.267 90 -6.157 7.626e-08 Young,Counting Young,Imagery -6.4 1.267 90 -5.052 5.698e-06 You should note that the FDR adjusted p values do not represent probabilities in the normal sense. Instead, the p value now indicates the false discovery rate at which the p value should be considered statistically significant. So, for example, if the adjusted p value 0.09, then this indicates the contrast would be significant if the acceptable false discovery rate is 10% (people often set their acceptable false discover rate to be 5% out of habit, but this is not always appropriate). # Set our acceptable false discovery rate to 10% FDR &lt;- .1 lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition, adjust=&quot;none&quot;)$contrast %&gt;% broom::tidy() %&gt;% select(level1, level2, p.value) %&gt;% arrange(p.value) %&gt;% mutate(`q (10% FDR)` = (rank(p.value)/length(p.value))*FDR) %&gt;% mutate(p.fdr.adjust=p.adjust(p.value, method=&quot;BH&quot;)) %&gt;% mutate(significant = as.numeric(p.value &lt; `q (10% FDR)`)) %&gt;% # just show some of the results, at the break between sig and ns contrast filter(p.fdr.adjust &gt; .01 &amp; p.fdr.adjust &lt; .4) %&gt;% pander(caption=&quot;Subset of contrasts, showing the break between significant and ns results, as determined by an FDR of 10%.&quot;, split.tables=Inf) Subset of contrasts, showing the break between significant and ns results, as determined by an FDR of 10%. level1 level2 p.value q (10% FDR) p.fdr.adjust significant Older,Rhyming Young,Adjective 0.008667 0.07111 0.01219 1 Older,Adjective Young,Intention 0.02964 0.07333 0.03923 1 Older,Adjective Older,Imagery 0.02964 0.07556 0.03923 1 Young,Adjective Young,Imagery 0.06139 0.07778 0.07893 1 Older,Imagery Older,Intention 0.183 0.08 0.2288 0 Older,Adjective Young,Imagery 0.2721 0.08222 0.3222 0 Young,Imagery Young,Intention 0.2721 0.08444 0.3222 0 Note, that when you use adjust='fdr' then the p values returned are The Biostat Handbook has a good 12.2 Repeated measures or ‘split plot’ designs It might be controversial to say so, but the tools to run traditional repeat measures Anova in R are a pain to use. It’s not easy to run repeated measures Anova models using base packages alone and, although there are numerous packages which do simplify this a little, their syntax can be obtuse or confusing, and the output sometimes cryptic. To make matters worse, various textbooks, online guides and the R help files themselves show many ways to achieve the same ends, and it can be difficult to follow the differences between the underlying models that are run. At this point, given the many other advantages of linear mixed models over traditional repeated measures Anova, and given that many researchers abuse traditional Anova in practice (e.g. using it for unbalanced data, or where some data are missing), the recommendation here is to simply give up and learn how to run linear mixed models. These can (very closely) replicate traditional Anova approaches, but also: Handle missing data or unbalanced designs gracefully and efficiently. Be expanded to include multiple levels of nesting. For example, allowing pupils to be nested within classes, within schools. Alternatively multiple measurements of individual patients might be clustered by hospital or therapist. Allow time to be treated as a continuous variable. For example, time can be modelled as a slope or some kind of curve, rather than a fixed set of observation-points. This can be more parsimonious, and more flexible when dealing with real-world data (e.g. from clinical trials). It would be best at this point to jump straight to the main section multilevel or mixed-effects models, but to give one brief example of mixed models in use: The sleepstudy dataset in the lme4 package provides reaction time data recorded from participants over a period of 10 days, during which time they were deprived of sleep. lme4::sleepstudy %&gt;% head(12) %&gt;% pander Reaction Days Subject 249.6 0 308 258.7 1 308 250.8 2 308 321.4 3 308 356.9 4 308 414.7 5 308 382.2 6 308 290.1 7 308 430.6 8 308 466.4 9 308 222.7 0 309 205.3 1 309 We can plot these data to show the increase in RT as sleep deprivation continues: lme4::sleepstudy %&gt;% ggplot(aes(factor(Days), Reaction)) + geom_boxplot() + xlab(&quot;Days&quot;) + ylab(&quot;RT (ms)&quot;) + geom_label(aes(y=400, x=2, label=&quot;you start to\\nfeel bad here&quot;), color=&quot;red&quot;) + geom_label(aes(y=450, x=9, label=&quot;imagine how bad\\nyou feel by this point&quot;), color=&quot;red&quot;) If we want to test whether there are significant differences in RTs between Days, we could fit something very similar to a traditional repeat measures Anova using the lme4::lmer() function, and obtain an Anova table for the model using the special lmerTest::anova() function: sleep.model &lt;- lmer(Reaction ~ factor(Days) + (1 | Subject), data=lme4::sleepstudy) lmerTest::anova(sleep.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## factor(Days) 166235 18471 9 153 18.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you had really wanted to fit the traditional repeated measures Anova, the closest equivalent would be: afex::aov_car(Reaction ~ Days + Error(Subject/(Days)), data=lme4::sleepstudy) ## Anova Table (Type 3 tests) ## ## Response: Reaction ## Effect df MSE F ges p.value ## 1 Days 3.32, 56.46 2676.18 18.70 *** .29 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG This gives almost-identical results. You may find that in other cases the lmer and traditional anova models diverge slightly, but this is likely to be caused by factors including imbalances in the data, partially missing data (only complete cases can be analyses by traditional anova) or other violations of the assumptions of one or both of the models. There is no clear steer in the literature as to which model is ‘best’ in the general sense, and it is likely that the linear model will be a better fit for a greater range of datasets. See the multilevel models section for details of more interesting models using this dataset which: Fit a simple slope for Days Fit curves or other functions for Days Allow the effect of sleep deprivation to vary for different participants "],
["understanding-interactions.html", "13 Understanding interactions (part 1) 13.1 What is an interaction? 13.2 Visualising interactions in raw data 13.3 Continuous predictors 13.4 What next?", " 13 Understanding interactions (part 1) Objectives of this section: Clarify/recap what an interaction is Appreciate the importance of visualising interactions Compare different methods of plotting interactions in raw data Deal with cases where predictors are both categorical and continuous (or a mix) 13.1 What is an interaction? For an interaction to occur we must measure: an outcome (severity of injury in a car crash, for example) at least 2 predictors of that outcome (e.g. age and gender) Let’s think of a scenario where we’ve measured severity of injury after road accidents, along with the age and gender of the drivers involved. Let’s assume16: Women are likely to be more seriously injured than men in a crash (a +10 point increase in severity) Drivers over 60 are more likely to injured than younger drivers (+10 point severity vs &lt;60 years) For an interaction to occur we have to show that, for example: If you ware old and also female then you are more severely injured than we would expect simply by adding the effects for being female (+10 points) and for being over 60 (+10 points). That is, if an interaction occurs the risk of being older and female is &gt; a 20 point increase in severity. Interactions capture the idea that the effect of one predictor changes across the range of another. 13.1.1 Example interaction visualised We can see this illustrated in the figure below: Figure 6.1: Bar plot of injury severity by age and gender. And this plot might be better re-drawn as a point and line plot: Figure 13.1: Point and line plot of injury severity by age and gender. The reason this plot improves on the bar graph is because: Readers tend to misinterpret bar plots by assuming that values ‘above’ the bar are less likely than values contained ‘within’ the bar, when this is not the case (Newman and Scholl 2012). The main effects are easy to distinguish in the line plot: just ask yourself if the lines are horizontal or not, and whether they are separated vertically. In contrast, reading the interction from the bar graph requires that we average pairs of bars (sometimes not adjacent to one another) and compare them - a much more difficult mental operation. The interaction is easy to spot: Ask yourself if the lines are parallel. If they are parallel then the difference between men and women is constant for individuals of different ages. 13.2 Visualising interactions in raw data Before setting out to test for an interaction using some kind of statistical model, it’s a good idea to first visualise the relationships between outcomes and predictors. A student dissertation project investigated the analgesic quality of music during an experimental pain stimulus. Music was selected to be either liked (or disliked) by participants and was either familiar or unfamiliar to them. Pain was rated without music (no.music) and with music (with.music) using a 10cm visual analog scale anchored with the labels “no pain” and “worst pain ever”. Before modelling the outcome, it would be helpful to see if the data are congruent with the study prediction that liked and familiar music would be more effective than disliked or unfamiliar music We can do this in many different ways. The most common would be a simple bar plot, which we can create using the stat_summary() function from ggplot2. painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = familiar, y=change.in.pain)) + facet_wrap(~liked) + stat_summary(geom=&quot;bar&quot;) + xlab(&quot;&quot;) This gives a pretty clear indication that something is going on, but we have no idea about the distriburion of the underlying data, and so how much confidence to place in the finding. If we want to preserve more information about the underlying distribution we can use density plots, boxplots, or pointrange plots, among others. Here we use a grouped17 density plot: painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = change.in.pain, color = interaction(familiar:liked))) + geom_density() + scale_color_discrete(name=&quot;&quot;) And here we use a boxplot to achieve similar ends: painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = interaction(familiar:liked), y = change.in.pain)) + geom_boxplot() + geom_hline(yintercept = 0, linetype=&quot;dotted&quot;) + xlab(&quot;&quot;) The advantage of both these plots is that they preserve quite a bit of infrmation about the variable of interest. However, they don’t make it easy to read the main effects and interaction as we saw for the point-line plot above. We can combine some benefits of both plots by adding an error bar to the point-line plot: painmusic %&gt;% ggplot(aes(liked, with.music - no.music, group=familiar, color=familiar)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_se) + stat_summary(geom=&quot;line&quot;, fun.data=mean_se) + ylab(&quot;Pain (VAS) with.music - no.music&quot;) + scale_color_discrete(name=&quot;&quot;) + xlab(&quot;&quot;) This plot doesn’t include all of the information about the distribution of effects that the density or boxplots do (for example, we can’t see any asymmetry in the distributions any more), but we still get some information about the variability of the effect of the experimental conditions on pain by plotting the SE of the mean over the top of each point18 At this point, especially if your current data include only categorical predictors, you might want to move on to the section on making predictions from models and visualising these. 13.3 Continuous predictors … 13.4 What next? You might like to move on to making predictions from statistical models, and plotting these. References "],
["predictions-and-margins.html", "14 Making predictions 14.1 Predictions vs margins 14.2 Predicted means 14.3 Effects (margins) 14.4 Continuous predictors 14.5 Predicted means and margins using lm() 14.6 Making prdictions for margins (effects of predictors) 14.7 Marginal effects 14.8 With continuous covariates 15 Visualising interactions from linear models", " 14 Making predictions Objectives of this section: Distingish predicted means (predictions) from predicted effects (‘margins’) Calculate both predictions and marginal effects for a lm() Plot predictions and margins Think about how to plot effects in meaningful ways 14.1 Predictions vs margins Before we start, let’s consider what we’re trying to achieve in making predictions from our models. We need to make a distinction between: Predicted means Predicted effects or marginal effects Consider the example used in a previous section where we measured injury.severity after road accidents, plus two predictor variables: gender and age. 14.2 Predicted means ‘Predicted means’ (or predictions) refers to our best estimate for each category of person we’re interested in. For example, if age were categorical (i.e. young vs. older people) then might have 4 predictions to calculate from our model: Age Gender mean Young Male ? Old Male ? Young Female ? Old Female ? And as before, we might plot these data: Figure 6.1: Point and line plot of injury severity by age and gender. This plot uses the raw data, but these points could equally have been estimated from a statistical model which adjusted for other predictors. 14.3 Effects (margins) Terms like: predicted effects, margins or marginal effects refer, instead, to the effect of one predictor. There may be more than one marginal effect because the effect of one predictor can change across the range of another predictor. Extending the example above, if we take the difference between men and women for each category of age, we can plot these differences. The steps we need to go through are: Reshape the data to be wide, including a separate column for injury scores for men and women Subtract the score for men from that of women, to calculate the effect of being female Plot this difference score margins.plot &lt;- inter.df %&gt;% # reshape the data to a wider format reshape2::dcast(older~female) %&gt;% # calculate the difference between men and women for each age mutate(effect.of.female = Female - Male) %&gt;% # plot the difference ggplot(aes(older, effect.of.female, group=1)) + geom_point() + geom_line() + ylab(&quot;Effect of being female&quot;) + xlab(&quot;&quot;) + geom_hline(yintercept = 0) ## Using severity.of.injury as value column: use value.var to override. margins.plot As before, these differences use the raw data, but could have been calculated from a statistical model. In the section below we do this, making predictions for means and marginal effects from a lm(). 14.4 Continuous predictors In the examples above, our data were all categorical, which mean that it was straightforward to identify categories of people for whom we might want to make a prediction (i.e. young men, young women, older men, older women). However, age is typically measured as a continuous variable, and we would want to use a grouped scatter plot to see this: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(size=1) + scale_color_discrete(name=&quot;&quot;) But to make predictions from this continuous data we need to fit a line through the points (i.e. run a model). We can do this graphically by calling geom_smooth() which attempts to fit a smooth line through the data we observe: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(alpha=.2, size=1) + geom_smooth(se=F)+ scale_color_discrete(name=&quot;&quot;) Figure 14.1: Scatter plot overlaid with smooth best-fit lines And if we are confident that the relationships between predictor and outcome are sufficiently linear, then we can ask ggplot to fit a straight line using linear regression: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(alpha = .1, size = 1) + geom_smooth(se = F, linetype=&quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;, se = F) + scale_color_discrete(name=&quot;&quot;) Figure 12.2: Scatter plot overlaid with smoothed lines (dotted) and linear predictions (coloured) What these plots illustrate is the steps a researcher might take before fitting a regression model. The straight lines in the final plot represent our best guess for a person of a given age and gender, assuming a linear regression. We can read from these lines to make a point prediction for men and women of a specific age, and use the information about our uncertainty in the prediction, captured by the model, to estimate the likely error. To make our findings simpler to communicate, we might want to make estimates at specific ages and plot these. These ages could be: Values with biological or cultural meaning: for example 18 (new driver) v.s. 65 (retirement age) Statistical convention (e.g. median, 25th, and 75th centile, or mean +/- 1 SD) We’ll see examples of both below. 14.5 Predicted means and margins using lm() The section above details two types of predictions: predictions for means, and predictions for margins (effects). We can use the figure below as a way of visualising the difference: gridExtra::grid.arrange(means.plot+ggtitle(&quot;Means&quot;), margins.plot+ggtitle(&quot;Margins&quot;), ncol=2) Figure 12.3: Example of predicted means vs. margins. Note, the margin plotted in the second panel is the difference between the coloured lines in the first. A horizontal line is added at zero in panel 2 by convention. 14.5.1 Running the model Lets say we want to run a linear model predicts injury severity from gender and a categorical measurement of age (young v.s. old). Our model formula would be: severity.of.injury ~ age.category * gender. Here we fit it an request the Anova table which enables us to test the main effects and interaction19: injurymodel &lt;- lm(severity.of.injury ~ age.category * gender, data=injuries) anova(injurymodel) ## Analysis of Variance Table ## ## Response: severity.of.injury ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age.category 1 4173.3 4173.3 154.573 &lt; 2.2e-16 *** ## gender 1 8488.5 8488.5 314.404 &lt; 2.2e-16 *** ## age.category:gender 1 1141.5 1141.5 42.279 1.25e-10 *** ## Residuals 996 26890.8 27.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Having saved the regression model in the variable injurymodel we can use this to make predictions for means and estimate marginal effects: 14.5.2 Making predictions for means When making predictions, they key question to bear in mind is ‘predictions for what?’ That is, what values of the predictor variables are we going to use to estimate the outcome? It goes like this: Create a new dataframe which contains the values of the predictors we want to make predictions at Make the predictions using the predict() function. Convert the output of predict() to a dataframe and plot the numbers. 14.5.3 Step 1: Make a new dataframe prediction.data &lt;- data_frame( age.category = c(&quot;young&quot;, &quot;older&quot;, &quot;young&quot;, &quot;older&quot;), gender = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;) ) prediction.data ## # A tibble: 4 × 2 ## age.category gender ## &lt;chr&gt; &lt;chr&gt; ## 1 young Male ## 2 older Male ## 3 young Female ## 4 older Female 14.5.4 Step 2: Make the predictions The R predict() function has two useful arguments: newdata, which we set to our new data frame containing the predictor values of interest interval which we here set to confidence20 injury.predictions &lt;- predict(injurymodel, newdata=prediction.data, interval=&quot;confidence&quot;) injury.predictions ## fit lwr upr ## 1 57.14239 56.49360 57.79117 ## 2 59.19682 58.56688 59.82676 ## 3 60.79554 60.14278 61.44830 ## 4 67.12521 66.47642 67.77399 14.6 Making prdictions for margins (effects of predictors) library(&#39;tidyverse&#39;) m &lt;- lm(mpg~vs+wt, data=mtcars) m.predictions &lt;- predict(m, interval=&#39;confidence&#39;) mtcars.plus.predictions &lt;- bind_cols( mtcars, m.predictions %&gt;% as.data.frame() ) prediction.frame &lt;- expand.grid(vs=0:1, wt=2) %&gt;% as.data.frame() prediction.frame.plus.predictions &lt;- bind_cols( prediction.frame, predict(m, newdata=prediction.frame, interval=&#39;confidence&#39;) %&gt;% as.data.frame() ) mtcars.plus.predictions %&gt;% ggplot(aes(vs, fit, ymin=lwr, ymax=upr)) + stat_summary(geom=&quot;pointrange&quot;) ## No summary function supplied, defaulting to `mean_se() prediction.frame.plus.predictions %&gt;% ggplot(aes(vs, fit, ymin=lwr, ymax=upr)) + geom_pointrange() prediction.frame.plus.predictions ## vs wt fit lwr upr ## 1 0 2 24.11860 21.61207 26.62514 ## 2 1 2 27.27297 25.57096 28.97499 mtcars.plus.predictions %&gt;% group_by(vs) %&gt;% summarise_each(funs(mean), fit, lwr, upr) ## # A tibble: 2 × 4 ## vs fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 16.61667 14.93766 18.29568 ## 2 1 24.55714 22.81586 26.29843 14.7 Marginal effects What is the effect of being black or female on the chance of you getting diabetes? Two ways of computing, depending on which of these two you hate least: Calculate the effect of being black for someone who is 50% female (marginal effect at the means, MEM) Calculate the effect first pretending someone is black, then pretending they are white, and taking the difference between these estimate (average marginal effect, AME) library(margins) margins(m, at = list(wt = 1:2)) ## Warning in check_values(data, at): A &#39;at&#39; value for &#39;wt&#39; is outside ## observed data range (1.513,5.424)! ## Average marginal effects at specified values ## lm(formula = mpg ~ vs + wt, data = mtcars) ## at(wt) vs wt ## 1 3.154 -4.443 ## 2 3.154 -4.443 m2 &lt;- lm(mpg~vs*wt, data=mtcars) summary(m2) ## ## Call: ## lm(formula = mpg ~ vs * wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9950 -1.7881 -0.3423 1.2935 5.2061 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.5314 2.6221 11.263 6.55e-12 *** ## vs 11.7667 3.7638 3.126 0.0041 ** ## wt -3.5013 0.6915 -5.063 2.33e-05 *** ## vs:wt -2.9097 1.2157 -2.393 0.0236 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.578 on 28 degrees of freedom ## Multiple R-squared: 0.8348, Adjusted R-squared: 0.8171 ## F-statistic: 47.16 on 3 and 28 DF, p-value: 4.497e-11 m2.margins &lt;- margins(m2, at = list(wt = 1.5:4.5)) ## Warning in check_values(data, at): A &#39;at&#39; value for &#39;wt&#39; is outside ## observed data range (1.513,5.424)! summary(m2.margins) ## factor wt AME SE z p lower upper ## vs 1.5 7.4021 2.0902 3.5414 0.0004 3.3054 11.4988 ## vs 2.5 4.4924 1.2375 3.6303 0.0003 2.0670 6.9178 ## vs 3.5 1.5827 1.2845 1.2321 0.2179 -0.9349 4.1003 ## vs 4.5 -1.3270 2.1736 -0.6105 0.5415 -5.5871 2.9331 ## wt 1.5 -4.7743 0.5853 -8.1569 0.0000 -5.9215 -3.6271 ## wt 2.5 -4.7743 0.5854 -8.1563 0.0000 -5.9216 -3.6270 ## wt 3.5 -4.7743 0.5854 -8.1560 0.0000 -5.9216 -3.6270 ## wt 4.5 -4.7743 0.5854 -8.1561 0.0000 -5.9216 -3.6270 summary(m2.margins) %&gt;% as.data.frame() %&gt;% filter(factor==&quot;vs&quot;) %&gt;% ggplot(aes(wt, AME)) + geom_point() + geom_line() 14.8 With continuous covariates Run 2 x Continuous Anova Predict at different levels of 15 Visualising interactions from linear models check this: https://strengejacke.wordpress.com/2013/10/31/visual-interpretation-of-interaction-terms-in-linear-models-with-ggplot-rstats/ Steps this page will work through: Running the the model (first will be a 2x2 between Anova) Using predict(). Creating predictions at specific values Binding predictions and the original data together. Using GGplot to layer points, lines and error bars. Because this is simulated data, the main effects and interactions all have tiny p values.↩ This gives us the confidence interval for the prediction, which is the range within which we would expect the true value to fall, 95% of the time, if we replicated the study. We could ask instead for the prediction interval, which would be the range within which 95% of new observations with the same predictor values would fall. For more on this see the section on confidence v.s. prediction intervals↩ "],
["mediation.html", "16 Mediation", " 16 Mediation This chapter will assume: You know what mediation analyses are and why you want to run them You a basic understanding of the Baron and Kenny approach, but an inkling that there are more up to date ways of testing mediation hypotheses. "],
["multilevel-models.html", "17 Multilevel models 17.1 The sleepstudy data and traditional RM Anova", "disschecks &lt;- statcheck::checkdir(&#39;~/Downloads/dissertations/&#39;) ## Importing PDF files... ## | | | 0% | |= | 1% | |= | 2% | |== | 3% | |== | 4% | |=== | 4% | |=== | 5% | |==== | 6% | |==== | 7% | |===== | 7% | |===== | 8% | |====== | 9% | |====== | 10% | |======= | 10% | |======= | 11% | |======== | 12% | |======== | 13% | |========= | 13% | |========= | 14% | |========== | 15% | |========== | 16% | |=========== | 16% | |=========== | 17% | |============ | 18% | |============ | 19% | |============= | 20% | |============== | 21% | |============== | 22% | |=============== | 23% | |=============== | 24% | |================ | 24% | |================ | 25% | |================= | 26% | |================= | 27% | |================== | 27% | |================== | 28% | |=================== | 29% | |=================== | 30% | |==================== | 30% | |==================== | 31% | |===================== | 32% | |===================== | 33% | |====================== | 33% | |====================== | 34% | |======================= | 35% | |======================= | 36% | |======================== | 36% | |======================== | 37% | |========================= | 38% | |========================= | 39% | |========================== | 40% | |=========================== | 41% | |=========================== | 42% | |============================ | 43% | |============================ | 44% | |============================= | 44% | |============================= | 45% | |============================== | 46% | |============================== | 47% | |=============================== | 47% | |=============================== | 48% | |================================ | 49% | |================================ | 50% | |================================= | 50% | |================================= | 51% | |================================== | 52% | |================================== | 53% | |=================================== | 53% | |=================================== | 54% | |==================================== | 55% | |==================================== | 56% | |===================================== | 56% | |===================================== | 57% | |====================================== | 58% | |====================================== | 59% | |======================================= | 60% | |======================================== | 61% | |======================================== | 62% | |========================================= | 63% | |========================================= | 64% | |========================================== | 64% | |========================================== | 65% | |=========================================== | 66% | |=========================================== | 67% | |============================================ | 67% | |============================================ | 68% | |============================================= | 69% | |============================================= | 70% | |============================================== | 70% | |============================================== | 71% | |=============================================== | 72% | |=============================================== | 73% | |================================================ | 73% | |================================================ | 74% | |================================================= | 75% | |================================================= | 76% | |================================================== | 76% | |================================================== | 77% | |=================================================== | 78% | |=================================================== | 79% | |==================================================== | 80% | |===================================================== | 81% | |===================================================== | 82% | |====================================================== | 83% | |====================================================== | 84% | |======================================================= | 84% | |======================================================= | 85% | |======================================================== | 86% | |======================================================== | 87% | |========================================================= | 87% | |========================================================= | 88% | |========================================================== | 89% | |========================================================== | 90% | |=========================================================== | 90% | |=========================================================== | 91% | |============================================================ | 92% | |============================================================ | 93% | |============================================================= | 93% | |============================================================= | 94% | |============================================================== | 95% | |============================================================== | 96% | |=============================================================== | 96% | |=============================================================== | 97% | |================================================================ | 98% | |================================================================ | 99% | |=================================================================| 100% ## Extracting statistics... ## | | | 0% | |= | 1% | |= | 2% | |== | 3% | |== | 4% | |=== | 4% | |=== | 5% | |==== | 6% | |==== | 7% | |===== | 7% | |===== | 8% | |====== | 9% | |====== | 10% | |======= | 10% ## Warning in sqrt((1 - r^2)/df): NaNs produced ## | |======= | 11% | |======== | 12% | |======== | 13% | |========= | 13% | |========= | 14% | |========== | 15% | |========== | 16% | |=========== | 16% | |=========== | 17% | |============ | 18% | |============ | 19% | |============= | 20% | |============== | 21% | |============== | 22% | |=============== | 23% | |=============== | 24% | |================ | 24% | |================ | 25% | |================= | 26% | |================= | 27% | |================== | 27% | |================== | 28% | |=================== | 29% | |=================== | 30% | |==================== | 30% | |==================== | 31% | |===================== | 32% | |===================== | 33% | |====================== | 33% | |====================== | 34% | |======================= | 35% | |======================= | 36% | |======================== | 36% | |======================== | 37% | |========================= | 38% | |========================= | 39% | |========================== | 40% | |=========================== | 41% | |=========================== | 42% | |============================ | 43% | |============================ | 44% | |============================= | 44% | |============================= | 45% | |============================== | 46% | |============================== | 47% | |=============================== | 47% | |=============================== | 48% | |================================ | 49% | |================================ | 50% | |================================= | 50% | |================================= | 51% | |================================== | 52% | |================================== | 53% | |=================================== | 53% | |=================================== | 54% | |==================================== | 55% | |==================================== | 56% | |===================================== | 56% | |===================================== | 57% | |====================================== | 58% | |====================================== | 59% | |======================================= | 60% | |======================================== | 61% | |======================================== | 62% | |========================================= | 63% | |========================================= | 64% | |========================================== | 64% | |========================================== | 65% | |=========================================== | 66% | |=========================================== | 67% | |============================================ | 67% | |============================================ | 68% | |============================================= | 69% | |============================================= | 70% | |============================================== | 70% | |============================================== | 71% | |=============================================== | 72% | |=============================================== | 73% | |================================================ | 73% | |================================================ | 74% | |================================================= | 75% | |================================================= | 76% | |================================================== | 76% | |================================================== | 77% | |=================================================== | 78% | |=================================================== | 79% | |==================================================== | 80% | |===================================================== | 81% | |===================================================== | 82% | |====================================================== | 83% | |====================================================== | 84% | |======================================================= | 84% | |======================================================= | 85% | |======================================================== | 86% | |======================================================== | 87% | |========================================================= | 87% | |========================================================= | 88% | |========================================================== | 89% | |========================================================== | 90% | |=========================================================== | 90% | |=========================================================== | 91% | |============================================================ | 92% | |============================================================ | 93% | |============================================================= | 93% | |============================================================= | 94% | |============================================================== | 95% | |============================================================== | 96% | |=============================================================== | 96% | |=============================================================== | 97% | |================================================================ | 98% | |================================================================ | 99% | |=================================================================| 100% ## ## Check the significance level. ## ## Some of the p value incongruencies are decision errors if the significance level is .1 or .01 instead of the conventional .05. It is recommended to check the actual significance level in the paper or text. Check if the reported p values are a decision error at a different significance level by running statcheck again with &#39;alpha&#39; set to .1 and/or .01. ## ## ## Check for one tailed tests. ## ## Some of the p value incongruencies might in fact be one tailed tests. It is recommended to check this in the actual paper or text. Check if the p values would also be incongruent if the test is indeed one sided by running statcheck again with &#39;OneTailedTests&#39; set to TRUE. To see which Sources probably contain a one tailed test, try unique(x$Source[x$OneTail]) (where x is the statcheck output). ## ## Warning in sqrt((1 - r^2)/df): NaNs produced ## Warning in sqrt((1 - r^2)/df): NaNs produced disschecks %&gt;% group_by(Statistic) %&gt;% reshape2::dcast(Statistic~Error) ## Using APAfactor as value column: use value.var to override. ## Aggregation function missing: defaulting to length ## Statistic FALSE TRUE NA ## 1 Chi2 72 9 0 ## 2 F 885 142 0 ## 3 r 128 18 1 ## 4 t 454 121 0 ## 5 Z 6 4 0 disschecks %&gt;% group_by(Statistic) %&gt;% reshape2::dcast(Statistic~DecisionError) ## Using APAfactor as value column: use value.var to override. ## Aggregation function missing: defaulting to length ## Statistic FALSE TRUE NA ## 1 Chi2 78 3 0 ## 2 F 1008 19 0 ## 3 r 146 0 1 ## 4 t 555 20 0 ## 5 Z 9 1 0 17 Multilevel models This chapter assumes: You know what a multilevel model is … 17.1 The sleepstudy data and traditional RM Anova XXX As noted in the Anova cookbook section, repeated measures anova can be approximated using linear mixed models. For example, using the same sleepstudy example, this model approximates a repeat measures anova in which multiple measurments of Reaction time are taken on multiple Days for each Subject: sleep.model &lt;- lmer(Reaction ~ factor(Days) + (1|Subject), data=lme4::sleepstudy) lmerTest::anova(sleep.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## factor(Days) 166235 18471 9 153 18.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If you really wanted to fit traditional RM Anova, this is the ‘real thing’: afex::aov_car(Reaction ~ Days + Error(Subject/(Days)), data=lme4::sleepstudy) ## Anova Table (Type 3 tests) ## ## Response: Reaction ## Effect df MSE F ges p.value ## 1 Days 3.32, 56.46 2676.18 18.70 *** .29 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG XXX ADD THESE EXAMPLES But see the multilevel models section for details of more interesting models which: 17.1.1 Fit a simple slope for Days lme4::sleepstudy %&gt;% ggplot(aes(Days, Reaction)) + geom_point() + geom_jitter() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; slope.model &lt;- lmer(Reaction ~ Days + (1|Subject), data=lme4::sleepstudy) lmerTest::anova(slope.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Days 162703 162703 1 161 169.4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 slope.model.summary &lt;- summary(slope.model) slope.model.summary$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 251.40510 9.7467163 22.8102 25.79383 0 ## Days 10.46729 0.8042214 161.0036 13.01543 0 17.1.2 Allow the effect of sleep deprivation to vary for different participants It looks like sleep deprivation hits some participants worse than others: set.seed(1234) lme4::sleepstudy %&gt;% filter(Subject %in% sample(levels(Subject), 10)) %&gt;% ggplot(aes(Days, Reaction, group=Subject, color=Subject)) + geom_smooth(method=&quot;lm&quot;, se=F) + geom_jitter(size=1) + theme_minimal() If we wanted to test whether there was significant variation in the effects of sleep deprivation between subjects, we could add a random slope to the model. The random slope allows the effect of Days to vary between subjects. So we can think of an overall slope (i.e. RT goes up over the days), from which individuals deviate by some amount (e.g. a resiliant person will have a negative deviation or residual from the overall slope). Adding the random slope doesn’t change the F test for Days that much: random.slope.model &lt;- lmer(Reaction ~ Days + (Days|Subject), data=lme4::sleepstudy) lmerTest::anova(random.slope.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Days 30031 30031 1 17 45.853 3.264e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nor the overall slope coefficient: random.slope.model.summary &lt;- summary(random.slope.model) slope.model.summary$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 251.40510 9.7467163 22.8102 25.79383 0 ## Days 10.46729 0.8042214 161.0036 13.01543 0 But we can use the lmerTest::rand() function to show that there is statistically significant variation in slopes between individuals, using the likelihood ratio test: lmerTest::rand(random.slope.model) ## Analysis of Random effects Table: ## Chi.sq Chi.DF p.value ## Days:Subject 42.8 2 5e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Because the random slope for Days is statistically significant, we know it improves the model. One way to see that improvement is to plot residuals (unexplained error for each datapoint) against predicted values. To extract residual and fitted values we use the residuals() and predict() functions. These are then combined in a data_frame, to enable us to use ggplot for the subsequent figures. # create data frames containing residuals and fitted # values for each model we ran above a &lt;- data_frame( model = &quot;random.slope&quot;, fitted = predict(random.slope.model), residual = residuals(random.slope.model)) b &lt;- data_frame( model = &quot;random.intercept&quot;, fitted = predict(slope.model), residual = residuals(slope.model)) # join the two data frames together residual.fitted.data &lt;- bind_rows(a,b) We can see that the residuals from the random slope model are much more evenly distributed across the range of fitted values, which suggests that the assumption of homogeneity of variance is met in the random slope model: # plots residuals against fitted values for each model residual.fitted.data %&gt;% ggplot(aes(fitted, residual)) + geom_point() + geom_smooth(se=F) + facet_wrap(~model) ## `geom_smooth()` using method = &#39;loess&#39; We can plot both of the random effects from this model (intercept and slope) to see how much the model expects individuals to deviate from the overall (mean) slope. # extract the random effects from the model (intercept and slope) ranef(random.slope.model)$Subject %&gt;% # implicitly convert them to a dataframe and add a column with the subject number rownames_to_column(var=&quot;Subject&quot;) %&gt;% # plot the intercept and slobe values with geom_abline() ggplot(aes()) + geom_abline(aes(intercept=`(Intercept)`, slope=Days, color=Subject)) + # add axis label xlab(&quot;Day&quot;) + ylab(&quot;Residual RT&quot;) + # set the scale of the plot to something sensible scale_x_continuous(limits=c(0,10), expand=c(0,0)) + scale_y_continuous(limits=c(-100, 100)) Inspecting this plot, there doesn’t seem to be any strong correlation between the RT value at which an individual starts (their intercept residual) and the slope describing how they change over the days compared with the average slope (their slope residual). That is, we can’t say that knowing whether a person has fast or slow RTs at the start of the study gives us a clue about what will happen to them after they are sleep deprived: some people start slow and get faster; other start fast but suffer and get slower. However we can explicitly check this correlation (between individuals’ intercept and slope residuals) using the VarCorr() function: VarCorr(random.slope.model) ## Groups Name Std.Dev. Corr ## Subject (Intercept) 24.7404 ## Days 5.9221 0.066 ## Residual 25.5918 The correlation between the random intercept and slopes is only 0.066, and so very low. We might, therefore, want to try fitting a model without this correlation. lmer includes the correlation by default, so we need to change the model formula to make it clear we don’t want it: uncorrelated.reffs.model &lt;- lmer( Reaction ~ Days + (1 | Subject) + (0 + Days|Subject), data=lme4::sleepstudy) VarCorr(uncorrelated.reffs.model) ## Groups Name Std.Dev. ## Subject (Intercept) 25.0513 ## Subject.1 Days 5.9882 ## Residual 25.5653 The variance components don’t change much when we constrain the covariance of intercepts and slopes to be zero, and we can explicitly compare these two models using the anova() function, which is somewhat confusingly named because in this instance it is performing a likelihood ratio test to compare the two models: anova(random.slope.model, uncorrelated.reffs.model) ## refitting model(s) with ML (instead of REML) ## Data: lme4::sleepstudy ## Models: ## ..1: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject) ## object: Reaction ~ Days + (Days | Subject) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## ..1 5 1762.0 1778.0 -876.00 1752.0 ## object 6 1763.9 1783.1 -875.97 1751.9 0.0639 1 0.8004 Model fit is not significantly worse with the constrained model, so for parsimony’s sake we prefer it to the more complex model. 17.1.3 Fitting a curve for the effect of Days In theory, we could also fit additional parameters for the effect of Days, although a combined smoothed line plot/scatterplot indicates that a linear function fits the data reasonably well. lme4::sleepstudy %&gt;% ggplot(aes(Days, Reaction)) + geom_point() + geom_jitter() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; If we insisted on testing a curved (quadratic) function of Days, we could: quad.model &lt;- lmer(Reaction ~ Days + I(Days^2) + (1|Subject), data=lme4::sleepstudy) quad.model.summary &lt;- summary(quad.model) quad.model.summary$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 255.4493728 10.4656310 30.04063 24.408406 0.00000000 ## Days 7.4340850 2.9707978 160.00374 2.502387 0.01334034 ## I(Days^2) 0.3370223 0.3177733 160.00374 1.060575 0.29048148 Here, the p value for I(Days^2) is not significant, suggesting (as does the plot) that a simple slope model is sufficient. "],
["cfa.html", "18 Confirmatory factor analysis 18.1 Defining the model 18.2 Model fit 18.3 Modification indices 18.4 Missing data", " 18 Confirmatory factor analysis This section adapted from a guide originally produced by Jon May First make sure the lavaan package is installed21; it stands for Latent Variable Analysis and is the most popular package for CFA in R. install.packages(lavaan) library(lavaan) ## This is lavaan 0.5-23.1097 ## lavaan is BETA software! Please report any bugs. Open your data and check that all looks well: hz &lt;- lavaan::HolzingerSwineford1939 hz %&gt;% glimpse() ## Observations: 301 ## Variables: 15 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, ... ## $ sex &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2... ## $ ageyr &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12,... ## $ agemo &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5,... ## $ school &lt;fctr&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, ... ## $ grade &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7... ## $ x1 &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.333... ## $ x2 &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5... ## $ x3 &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875,... ## $ x4 &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.000... ## $ x5 &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5... ## $ x6 &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286, ... ## $ x7 &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.347... ## $ x8 &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4... ## $ x9 &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.500... 18.1 Defining the model Define the CFA model you want to run using ‘lavaan model syntax’22. For example: hz.model &lt;- &#39; visual =~ x1 + x2 + x3 writing =~ x4 + x5 + x6 maths =~ x7 + x8 + x9 &#39; The model is defined in text, and can be broken over lines for clarity. Here was save it in a variable named hz.model. The model is defined by naming latent variables, and then specifying which observed variables measure them. The latent variable names are followed by =~ which means ‘is manifested by’. To run the analysis: hz.fit &lt;- cfa(hz.model, data=hz) To display the results: hz.fit.summary &lt;- summary(hz.fit, standardized=TRUE) ## lavaan (0.5-23.1097) converged normally after 35 iterations ## ## Number of observations 301 ## ## Estimator ML ## Minimum Function Test Statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual =~ ## x1 1.000 0.900 0.772 ## x2 0.554 0.100 5.554 0.000 0.498 0.424 ## x3 0.729 0.109 6.685 0.000 0.656 0.581 ## writing =~ ## x4 1.000 0.990 0.852 ## x5 1.113 0.065 17.014 0.000 1.102 0.855 ## x6 0.926 0.055 16.703 0.000 0.917 0.838 ## maths =~ ## x7 1.000 0.619 0.570 ## x8 1.180 0.165 7.152 0.000 0.731 0.723 ## x9 1.082 0.151 7.155 0.000 0.670 0.665 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual ~~ ## writing 0.408 0.074 5.552 0.000 0.459 0.459 ## maths 0.262 0.056 4.660 0.000 0.471 0.471 ## writing ~~ ## maths 0.173 0.049 3.518 0.000 0.283 0.283 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.549 0.114 4.833 0.000 0.549 0.404 ## .x2 1.134 0.102 11.146 0.000 1.134 0.821 ## .x3 0.844 0.091 9.317 0.000 0.844 0.662 ## .x4 0.371 0.048 7.779 0.000 0.371 0.275 ## .x5 0.446 0.058 7.642 0.000 0.446 0.269 ## .x6 0.356 0.043 8.277 0.000 0.356 0.298 ## .x7 0.799 0.081 9.823 0.000 0.799 0.676 ## .x8 0.488 0.074 6.573 0.000 0.488 0.477 ## .x9 0.566 0.071 8.003 0.000 0.566 0.558 ## visual 0.809 0.145 5.564 0.000 1.000 1.000 ## writing 0.979 0.112 8.737 0.000 1.000 1.000 ## maths 0.384 0.086 4.451 0.000 1.000 1.000 hz.fit.summary ## NULL The output has three parts: Parameter estimates. The values in the first column are the standardised weights from the observed variables to the latent factors. Factor covariances. The values in the first column are the covariances between the latent factors. Error variances. The values in the first column are the estimates of each observed variable’s error variance. 18.2 Model fit To examine the model fit: fitmeasures(hz.fit, c(&#39;cfi&#39;, &#39;rmsea&#39;, &#39;rmsea.ci.upper&#39;, &#39;bic&#39;)) ## cfi rmsea rmsea.ci.upper bic ## 0.931 0.092 0.114 7595.339 This looks OK, but could be improved. 18.3 Modification indices To examine the modification indices (here sorted to see the largest first) we type: modificationindices(hz.fit) %&gt;% as.data.frame() %&gt;% arrange(-mi) %&gt;% filter(mi &gt; 10) %&gt;% select(lhs, op, rhs, mi, epc) %&gt;% pander::pander() lhs op rhs mi epc visual =~ x9 36.41 0.577 x7 ~~ x8 34.15 0.5364 visual =~ x7 18.63 -0.4219 x8 ~~ x9 14.95 -0.4231 Latent factor to variable links have =~ in the ‘op’ column. Error covariances for observed variables have ~~ as the op. These symbols match the symbols used to describe a path in the lavaan model syntax. If we add the largest MI path to our model it will look like this: hz.model.2 &lt;- &quot; visual =~ x1 + x2 + x3 writing =~ x4 + x5 + x6 + x9 maths =~ x7 + x8 + x9 &quot; hz.fit.2 &lt;- cfa(hz.model.2, data=hz) fitmeasures(hz.fit.2, c(&#39;cfi&#39;, &#39;rmsea&#39;, &#39;rmsea.ci.upper&#39;, &#39;bic&#39;)) ## cfi rmsea rmsea.ci.upper bic ## 0.936 0.091 0.113 7595.660 RMSEA has improved marginally, but we’d probably want to investigate this model further, and make additional improvements to it. 18.4 Missing data If you have missing data, add the argument estimator='MLM' to the cfa() function to use a robust method. There are no missing data in this dataset, but it would look like this: hz.fit.2.mlm &lt;- cfa(hz.model.2, data=hz, estimator=&quot;MLM&quot;) hz.fit.2.mlm ## lavaan (0.5-23.1097) converged normally after 35 iterations ## ## Number of observations 301 ## ## Estimator ML Robust ## Minimum Function Test Statistic 79.919 75.703 ## Degrees of freedom 23 23 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.056 ## for the Satorra-Bentler correction see: installing packages↩ A full guide is here: http://lavaan.ugent.be/tutorial/syntax1.html↩ "],
["installation.html", "19 Installing R and RStudio", " 19 Installing R and RStudio XXX To do: For the moment just use: http://r.thesignalbox.net (ask Ben Whalley for a user account). "],
["packages.html", "20 Loading packages", " 20 Loading packages R has been around for a very long time, but has remained popular because it is easy for people to add new functions to it. You can run almost any statistical model and produce a wide variety of graphics in R because people have contributed new functions and these extend the base language. These new features are distributed in bundles known as ‘packages’. For now we’ll assume someone has helped you install all the packages you need23. To access the features in packages, you normally load the package with the library() function. Running library(&lt;packagename&gt;) loads all the new functions within it, and it is then possible to call them from your code. For example, typing: library(ggplot2) Will load the ggplot2 package. You can then call the qplot function it provides: qplot(mtcars$mpg, bins=7) You don’t strictly need to load packages to use the features within them though. If a package is installed on your system you can also call a function it provides directly. In the example below we call the hist.data.frame from the Hmisc package, and obtain histograms of all the variables in the mtcars dataset: Hmisc::hist.data.frame(mtcars) The rule is to type package::function(parameters), where :: separates the package and function names. Parameters are just the inputs to the function. There are two reasons not to load a package before using it: Laziness: it can save typing if you just want to use one function from a package, and only once. Explicitness: It’s an unfortunate fact that some function names are repeated across a number of packages. This can be confusing if they work differently, and if you don’t know which package the version you are using comes from. Using package_name:function_name can help make things explicit. See the installation guide if this isn’t the case↩ "],
["help.html", "21 Getting help", " 21 Getting help If you don’t know or can’t remember what a function does, R provides help files which explain how they work. To access a help file for a function, just type ?command in the console, or run ?command command within an R block. For example, running ?mean would bring up the documentation for the mean function. You can also type CRTL-Shift-H while your cursor is over any R function in the RStudio interface. It’s fair to say R documentation isn’t always written for beginners. However the ‘examples’ sections are usually quite informative: you can normally see this by scrolling right to the end of the help file. "],
["missing-values.html", "22 Missing values", " 22 Missing values TODO Explain about NA and is.na() and is.finite() here "],
["rmarkdown-tricks.html", "23 Tips and tricks with RMarkdown", " 23 Tips and tricks with RMarkdown TODO, cover Using pander to format tables nicely Rounding of values with sprintf Accessing coefficients() Using broom to tidy model results Using bibtex and citing in text Calculate VPC/ICC from an lmer models using model %&gt;% summary %&gt;% as.data.frame()$varcor Hint at things hidden in R objects (e.g. formula in the lm object). Using @ and $ to autocomplete and find things "],
["confidence-vs-prediction-intervals.html", "24 Confidence, credible and prediction intervals 24.1 The problem with confidence intervals 24.2 Forgetting that the CI depends on sample size. 24.3 What does this mean for my work on [insert speciality here]?", "library(tidyverse) ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats 24 Confidence, credible and prediction intervals TODO: EXPAND ON THESE DEFINITIONS AND USE GRAPHICS AND PLOTS TO ILLUSTRATE Confidence interval: The range within which we would expect the true value to fall, 95% of the time, if we replicated the study. Prediction interval: the range within which we expect 95% of new observations to fall. If we’re considering the prediction interval for a specific point prediction (i.e. where we set predictors to specific values), then this interval woud be for new observations with the same predictor values. 24.1 The problem with confidence intervals Confidence intervals are helpful when we want to think about how precise our estimate is. For example, in an RCT we will want to estimate the difference between treatment groups, and it’s conceivable be reasonable to want to know, for example, the range within which the true effect would fall 95% of the time if we replicated our study many times. If we run a study with small N, intuitively we know that we have less information about the difference between our RCT treatments, and so we’d like the CI to expand accordingly. So — all things being equal — the confidence interval reduces as our sample size increases. The problem with confidence intervals come because many researchers and clinicians read them incorrectly. Typically, they either: Forget that the CI represents the precision of the estimate Misinterpret the CI as the range in which we are 95% sure the true value lies. 24.2 Forgetting that the CI depends on sample size. By forgetting that the CI contracts as the sample size increases, researchers can become overconfident about their ability to predict new observations. Imagine that we sample data from two populations with the same mean, but different variability: set.seed(1234) df &lt;- expand.grid(v=c(1,3,3,3), i=1:1000) %&gt;% as.data.frame %&gt;% mutate(y = rnorm(length(.$i), 100, v)) %&gt;% mutate(samp = factor(v, labels=c(&quot;Low variability&quot;, &quot;High variability&quot;))) df %&gt;% ggplot(aes(y)) + geom_histogram() + facet_grid(~samp) + scale_color_discrete(&quot;&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we sample 100 individuals from each population the confidence interval around the sample mean would be wider in the high variability group. If we increase our sample size we would become more confident about the location of the mean, and this confidence interval would shrink. But imagine taking a single new sample from either population. These samples would be new grey squares, which we place on the histograms above. It does not matter how much extra data we have collected in group B or how sure what the mean of the group is: We would always be less certain making predictions for new observations in the high variability group. The important insight here is that if our data are noisy and highly variable we can never make firm predictions for new individuals, even if we collect so much data that we are very certain about the location of the mean. 24.3 What does this mean for my work on [insert speciality here]? "],
["references.html", "25 References", " 25 References "]
]
