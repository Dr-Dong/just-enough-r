[
["index.html", "Just Enough R", " Just Enough R "],
["introduction.html", "Introduction", " Introduction R makes it easy to work with and learn from data. It also happens to be a complete programmming language, but if you’re reading this guide then that might not be of interest to you. That’s OK — the goal here is not to teach you how to program in R1. The goal is to teach you just enough R to be confident to explore your data. In this guide, we use R in the same way we use any other statistics software: To check and visualise data, run statistical analyses, and share our results with others. To do that it’s worth learning the absolute basics of the R language. The next few chapters walk you through those basics: what you need to know to be productive in R and RStudio. By analogy, imagine going on holiday and learning enough French to introduce yourself, count from one to ten, and order a baguette. You won’t be fluent - but you won’t starve, and your trip will be more fun. A warning This guide is extremely opinionated. There are many ways to get things done with R, but trying to learn them all at once causes unhappiness. In particular, lots of the base R functions are old, quirky, inconstently named, and hard to remember. This guide recommends that you use several new ‘packages’ that replicate and extend some of R’s basic functionality. Using this new set of packages, which are very thoughtfully designed and work together nicely, will help you form a more consistent mental model and workflow in R. You can learn the crufty old bits (which do still have their uses) later on. The guide also assumes you are using the RStudio editor and working in an RMarkdown document (see the next section). This is important because this guide itself is written in RMarkdown, and editing it will be an important part of the learning process. If you don’t have access to RStudio yet, see the installation guide. License These documents are licensed under the CC BY-SA licence. This is actually a lie, but I’m hoping you won’t notice until it’s too late.↩ "],
["r-basics.html", "1 Working with R ", " 1 Working with R "],
["installation-intro.html", "Installation", " Installation RStudio Server The simplest way to get started with RStudio is to get an account on a server installation of the software (if you are a Plymouth University student please see the guidance on the DLE). Installing on your own machine. Download RStudio 1.01 or later (I’d suggest using whatever version is most recent and upgrading as new versions become available because the software is fairly actively developed). Install the packages listed below If you want to ‘knit’ your work into a pdf format, you should also install LaTeX. On windows use this installer. Make sure to do a ‘full install’, not just a basic install. On a Mac install homebrew and type brew cask install mactex. Package dependencies As noted, this guide uses a number of recent R packages to make learning R simpler and more consistent. This requires that the packages are installed first. To install some of the recommended packages you will need a working C compiler. This script installs all dependencies on a recent Linux or Mac system. Workflow One of the biggest adjustments people need to make when moving away from SPSS or other tools is to work out a ‘way of working’. Good students often develop ways of working, saving and communicating their findings that become habitual. These habits are often attempts to work around limitations of these packages, so hopefully they will fade in time. But nevertheless habits are easier to replace than break, so here’s an alternative model to adopt: Work in RStudio, and specifically use RMarkdown documents (see next section) Always keep your raw data in .csv format. Avoid saving multiple ‘processed’ versions of your data, and never edit data by hand unless absolutely necessary. Use R to process data and RMarkdown to document the steps taken. ‘Knit’ (run) your RMarkdown documents for sharing with colleagues, or for publication. "],
["rmarkdown.html", "RMarkdown", " RMarkdown A major weakness of traditional GUI stats packages is that there is no simple way to document and share your analyses, and so repeating or editing your work later is very hard. RMarkdown is a format for documenting and sharing statistical analyses, and is one of the first things we learn in this guide. This it might seem an odd place to start the guide: we haven’t got anything to share yet! But RMarkdown provides a really nice way to work with data interactively, and share results, and so it’s worth starting as we mean to go on. You are currently reading the output of an ‘RMarkdown’ document. R is a computer language designed for working with data. Markdown is a simple text-based format which can include prose, hypertext links, images, and code (see http://commonmark.org/help/). An RMarkdown document mixes R code with Markdown. This means you can combine your analysis with text that explains and interprets it. RMarkdown includes all the details neeed to reproduce an analysis. Like computer code, RMarkdown can be ‘run’ or ‘executed’. But in the language of RStudio, you ‘knit’ your RMarkdown to produce a finished document. This combines analyses, graphs, and explanatory text in a single pdf, html, or Word document which can be shared. Writing and ‘knitting’ RMarkdown To include R code within RMarkdown we write 3 backticks (```), followed by {r}. We the include our R code, and close the block with 3 more backticks (how to find the backtick on your keyboard). A code chunk in the RMarkdown editor When a document including this chunk is run or ‘knitted’, the final result will include the the line 2+2 followed by the number 4 on the next line. We can use RMarkdown to ‘show our workings’: our analysis can be interleaved with narrative text to explain or interpret the calculations. "],
["rstudio.html", "RStudio", " RStudio RStudio is a text editor which has been customised to make working with R easy. It can be installed on your own computer, or you can login to a shared RStudio server (for example, one run by your university) from a web browser. Either way the interface is largely the same and contains 4 main panels: The figure above shows the main RStudio interface, comprising: The main R-script or RMarkdown editor window. This is where you write commands, which can then be executed (to run the current line type ctrl-Enter or cmd-Enter on a Mac). The R console, into which you can type R commands directly, and see the output of commands run in the script editor. The ‘environment’ panel, which lists all the variables you have defined and currently available to use. The files and help panel. Within this panel the ‘files’ tab enables you to open files stored on the server, in the current project, or elsewhere on your hard drive. You can see a short video demonstrating the RStudio interface here: The video: Shows you how to type commands into the Console and view the results. Run a plotting function, and see the result. Create RMarkdown file, and ‘Knit’ it to produce a document containing the results of your code and explanatory text. Once you have watched the video: Try creating a new RMarkdown document in RStudio. Edit some of the text, and press the Knit button to see the results. If you feel brave, edit one of the R blocks and see what happens! More about RMarkdown A more detailed guide to using RMarkdown, which covers many of the ‘chunk options’ available to customise output, is available here If you’d like to use RMarkdown to include manage your citations, see this guide "],
["first-commands.html", "First commands", " First commands You can type R commands directly into the console and see the result there, but you should make a habit of working in an RMarkdown file. This keeps a record of everything you try, and makes it easy to edit/amend commands which don’t work as you expect. Now would be a good time to open an RMarkdown document to see how it works. A good place to start would be to open the source to this document. The best way to do this is to download the source code for the ‘Just Enough R’ project, and then open the file start_here.Rmd. The source for this RMarkdown file is available here: https://raw.githubusercontent.com/benwhalley/just-enough-r/master/start_here.Rmd. Or you can download the whole project here: https://github.com/benwhalley/just-enough-r/archive/master.zip. This link downloads a ‘zip’ file, which is a compressed folder containing all the files in the project. To ‘unzip’ it on Mac or Windows just double-click the file in the Finder or Windows Explorer. To run code in the RStudio interface put your cursor on a line within an R Block (or select the code you want to run), and press Ctrl-Enter. The result will appear below the code block. The command in the R block below prints (shows on screen) the first few rows of the build-in mtcars example dataset. Place your cursor somewhere in the line the command is on and run it by typing Ctrl-Enter, shown in this brief video: Create an R block in RMarkdown, then run some simple commands. head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If you are reading this from within RStudio, running head(mtcars) will have included an interactive table in the document, which you can use this to view the mtcars dataset. If you are still reading the compiled html or pdf document you will see a table containing the same data, included within the body of the document. Hopefully at this point it’s obvious that RStudio and RMarkdown give you: A nice place to work with R and explore your data A nice format to share your workings (e.g. with other researchers or your tutor) A mechanism to save reports of your analysis, to share with other people who don’t use RStudio "],
["variables.html", "Naming things", " Naming things We can assign labels to the results of calculations and other parts of our analyses to keep track of them. To assign labels we use the &lt;- symbol. The &lt;- symbol points from the value we want to store, to the name we want to use. For example: the_magic_number &lt;- 3 This assigns the value 3 to the variable the_magic_number. This block wouldn’t display anything because assigning a variable doesn’t create any output. To both assign a variable and display it we would type: the_magic_number &lt;- 3 the_magic_number ## [1] 3 Or we can use a shortcut: if we wrap the line in parentheses this both makes the assignment and prints the result to the console: (i_am_a_new_variable &lt;- 22) ## [1] 22 Helpfully, we can do calculations as we assign variables: one_score &lt;- 20 (four_score_years_and_ten &lt;- one_score * 4 + 10) ## [1] 90 We can give anything a label by assigning it to a variable. It doesn’t have to be a number: we can also assign words, graphics and plots, the results of a statistical model, or lists of any of these things. "],
["vectors-and-lists.html", "Vectors and lists", " Vectors and lists When working with data, we often have lists or sequences of ‘things’. For example: a list of measurements we have made. When all the things are of the same type, R calls this a vector2. When there is a mix of different things R calls this a list. Vectors We can create a vector of numbers and display it like this: # this creates a vector of heights, in cm heights &lt;- c(203, 148, 156, 158, 167, 162, 172, 164, 172, 187, 134, 182, 175) The c() command is shorthand for combine, so the example above combines the individual elements (numbers) into a new vector. We can create a vector of alphanumeric names just as easily: names &lt;- c(&quot;Ben&quot;, &quot;Joe&quot;, &quot;Sue&quot;, &quot;Rosa&quot;) And we can check the values stored in these variables by printing them. You can either type print(heights), or just write the name of the variable alone, which will print it by default. E.g.: heights ## [1] 203 148 156 158 167 162 172 164 172 187 134 182 175 Try creating your own vector of numbers in a new code block below3 using the c(...) command. Then change the name of the variable you assign it to. Accessing elements Once we have created a vector, we often want to access the individual elements again. We do this based on their position. Let’s say we have created a vector: my.vector &lt;- c(10, 20, 30, 40) We can display the whole vector by just typing its name, as we saw above. But if we want to show only the first element of this vector, we type: my.vector[1] ## [1] 10 Here, the square brackets specify a subset of the vector we want - in this case, just the first element. Selecting more than one element A neat feature of subsetting is that we can grab more than one element at a time. To do this, we need to tell R the positions of the elements we want, and so we provide a vector of the positions of the elements we want. It might seem obvious, but the first element has position 1, the second has position 2, and so on. So, if we wanted to extract the 4th and 5th elements from the vector of heights we saw above we would type: elements.to.grab &lt;- c(4, 5) heights[elements.to.grab] ## [1] 158 167 We can also make a subset of the original vector and assign it to a new variable: first.two.elements &lt;- heights[c(1, 2)] first.two.elements ## [1] 203 148 Making and slicing with sequences One common task in R is to create sequences of numbers, letters or dates. The simplest way of doing this is to define a range, with the colon: onetoten &lt;- 1:10 onetoten ## [1] 1 2 3 4 5 6 7 8 9 10 This creates a vector which can be sliced like any other: onetoten[8] ## [1] 8 One common use of sequences is to slice other vectors: onetoten[1:3] ## [1] 1 2 3 Or the first 10 values in the heights vector we defined above: heights[1:10] ## [1] 203 148 156 158 167 162 172 164 172 187 This works backwards, and with negative numbers too: 5:-5 ## [1] 5 4 3 2 1 0 -1 -2 -3 -4 -5 When your sequence doesn’t contain only whole numbers, or non-consecutive numbers, you can use the seq function: seq(1,10,by=2) ## [1] 1 3 5 7 9 seq(0, 1, by=.2) ## [1] 0.0 0.2 0.4 0.6 0.8 1.0 Conditional slicing One neat feature of R is that you can create a sequence of TRUE or FALSE values, by asking whether each value in a sequence matches a particular condition. For example: 1:10 &gt; 5 ## [1] FALSE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE Re-using the heights vector from above, we can then use this to select values that are above the average: heights &gt; mean(heights) ## [1] TRUE FALSE FALSE FALSE FALSE FALSE TRUE FALSE TRUE TRUE FALSE ## [12] TRUE TRUE And we can use the vector of TRUE and FALSE values to select from the actual scores: heights[heights &gt; mean(heights)] ## [1] 203 172 172 187 182 175 It’s actually a matrix if has 2 dimensions, like a table, or an array if it has more than 2 dimensions.↩ i.e. edit the RMarkdown document↩ "],
["processing-vectors.html", "Processing vectors", " Processing vectors Many of R’s most useful functions process vectors of numbers in some way. For example (as we’ve already seen) if we want to calculate the average of our vector of heights we just type: mean(heights) ## [1] 167.6923 R contains lots of built in functions which we can use to summarise a vector of numbers. For example: median(heights) ## [1] 167 sd(heights) ## [1] 17.59443 min(heights) ## [1] 134 max(heights) ## [1] 203 range(heights) ## [1] 134 203 IQR(heights) ## [1] 17 length(heights) ## [1] 13 All of these functions accept a vector as input, do some proccesing, and then return a single number which gets displayed by RStudio. But not all functions return a single number in the way that mean did above. Some return a new vector, or some other type of object instead. For example, the quantile function returns the values at the 0, 25th, 50th, 75th and 100th percentiles (by default). height.quantiles &lt;- quantile(heights) height.quantiles ## 0% 25% 50% 75% 100% ## 134 158 167 175 203 If a function returns a vector, we can use it just like any other vector: height.quantiles &lt;- quantile(heights) # grab the third element, which is the median height.quantiles[3] ## 50% ## 167 # assign the first element to a variable min.height &lt;- height.quantiles[1] min.height ## 0% ## 134 But other functions process a vector without returning any numbers. For example, the hist function returns a histogram: hist(heights) We’ll cover lots more plotting and visualisation later on. Making new vectors So far we’ve seen R functions which process a vector of numbers and produce a single number, a new vector of a different length (like quantile or fivenum), or some other object (like hist which makes a plot). However many other functions accept a single input, do something to it, and return a single processed value. For example, the square root function, sqrt, accepts a single value and returns a single value: running sqrt(10) will return 3.1623. In R, if a function accepts a single value as input and returns a single value as output (like sqrt(10)), then you can usually give a vector as input too. Some people find this surprising4, but R assumes that if you’re processing a vector of numbers, you want the function applied to each of them in the same way. This turns out to be very useful. For example, let’s say we want the square root of each of the elements of our height data: # these are the raw values heights ## [1] 203 148 156 158 167 162 172 164 172 187 134 182 175 # takes the sqrt of each value and returns a vector of all the square roots sqrt(heights) ## [1] 14.24781 12.16553 12.49000 12.56981 12.92285 12.72792 13.11488 ## [8] 12.80625 13.11488 13.67479 11.57584 13.49074 13.22876 This also works with simple arithmetic So, if we wanted to convert all the heights from cm to meters we could just type: heights / 100 ## [1] 2.03 1.48 1.56 1.58 1.67 1.62 1.72 1.64 1.72 1.87 1.34 1.82 1.75 This trick also works with other functions like paste, which combines the inputs you send it to produce an alphanumeric string: paste(&quot;Once&quot;, &quot;upon&quot;, &quot;a&quot;, &quot;time&quot;) ## [1] &quot;Once upon a time&quot; If we send a vector to paste it assumes we want a vector of results, with each element in the vector pasted next to each other: bottles &lt;- c(100, 99, 98, &quot;...&quot;) paste(bottles, &quot;green bottles hanging on the wall&quot;) ## [1] &quot;100 green bottles hanging on the wall&quot; ## [2] &quot;99 green bottles hanging on the wall&quot; ## [3] &quot;98 green bottles hanging on the wall&quot; ## [4] &quot;... green bottles hanging on the wall&quot; In other programming languages we might have had to write a ‘loop’ to create each line of the song, but R lets us write short statements to summarise what needs to be done; we don’t need to worry worrying about how it gets done. The paste0 function does much the same, but leaves no spaces in the combined strings, which can be useful: paste0(&quot;N=&quot;, 1:10) ## [1] &quot;N=1&quot; &quot;N=2&quot; &quot;N=3&quot; &quot;N=4&quot; &quot;N=5&quot; &quot;N=6&quot; &quot;N=7&quot; &quot;N=8&quot; &quot;N=9&quot; &quot;N=10&quot; Making up data (new vectors) Sometimes you’ll need to create vectors containing regular sequences or randomly selected numbers. To create regular sequences a convenient shortcut is the ‘colon’ operator. For example, if we type 1:10 then we get a vector of numbers from 1 to 10: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 The seq function allows you to create more specific sequences: # make a sequence, specifying the interval between them seq(from=0.1, to=2, by=.1) ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 ## [18] 1.8 1.9 2.0 We can also use random number-generating functions built into R to create vectors: # 10 uniformly distributed random numbers between 0 and 1 runif(10) ## [1] 0.55039790 0.56671130 0.12170090 0.56943814 0.69115807 0.21251599 ## [7] 0.05639159 0.09458552 0.91188579 0.01170227 # 1,000 uniformly distributed random numbers between 1 and 100 my.numbers &lt;- runif(1000, 1, 10) # 10 random-normal numbers with mean 10 and SD=1 rnorm(10, mean=10) ## [1] 10.181492 9.828023 8.510946 9.787780 8.293745 10.873080 11.512254 ## [8] 10.535196 10.429489 8.632022 # 10 random-normal numbers with mean 10 and SD=5 rnorm(10, 10, 5) ## [1] 21.728540 2.915083 3.672261 15.874733 14.784080 8.720487 13.412209 ## [8] 12.094219 3.084931 8.405639 We can then use these numbers in our code, for example plotting them: random.numbers &lt;- rnorm(10000) hist(random.numbers) Mostly people who already know other programming languages like C. It’s not that surprising if you read the R code as you would English.↩ "],
["functions-to-learn-now.html", "Functions to learn now", " Functions to learn now There are thousands of functions built into R. Below are just a few examples which are likely to be useful as you work with your data: Repetition # repeat something N times rep(&quot;Apple pie&quot;, 10) ## [1] &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; ## [6] &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; # repeat a short vector, combining into a single longer vector rep(c(&quot;Custard&quot;, &quot;Gravy&quot;), 5) ## [1] &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; ## [8] &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; Sequences # make a sequence (countdown &lt;- 100:1) ## [1] 100 99 98 97 96 95 94 93 92 91 90 89 88 87 86 85 84 ## [18] 83 82 81 80 79 78 77 76 75 74 73 72 71 70 69 68 67 ## [35] 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 ## [52] 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 ## [69] 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 ## [86] 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 Make sequences with steps of a particular size: (tenths &lt;- seq(from=0, to=1, by=.1)) ## [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 (twelfths &lt;- seq(from=0, to=10, length.out=12)) ## [1] 0.0000000 0.9090909 1.8181818 2.7272727 3.6363636 4.5454545 ## [7] 5.4545455 6.3636364 7.2727273 8.1818182 9.0909091 10.0000000 Ranking # generate some random data (here, ages in years) ages &lt;- round(rnorm(10, mean=40, sd=10)) # get the rank order of elements (i.e. what their positions would be if the vector was sorted) ages ## [1] 31 54 52 14 32 29 30 44 38 39 rank(ages, ties.method=&quot;first&quot;) ## [1] 4 10 9 1 5 2 3 8 6 7 Unique values # return the unique values in a vector unique(rep(1:10, 100)) ## [1] 1 2 3 4 5 6 7 8 9 10 Lengths # return the unique values in a vector length(seq(1,100, 2)) ## [1] 50 Try and experiment with each of these functions. Check the output against what you expected to happen, and make sure you understand what they do. "],
["lists.html", "Lists", " Lists Try running the code below: (confusing.vector &lt;- c(1, 2, 3, &quot;Wibble&quot;)) ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;Wibble&quot; (first.element &lt;- confusing.vector[1]) ## [1] &quot;1&quot; sqrt(first.element) ## Error in sqrt(first.element): non-numeric argument to mathematical function Take a minute to try and make a guess at what went wrong. Why does R complain that the 1 is non-numeric? When we built the vector we used c to combine the elements 1, 2, 3 and &quot;Wibble&quot;. Although our first three elements are numbers, &quot;Wibble&quot; is not - it’s made up of letters (this is called a character string). Vectors can only contain one type of thing so R automatically converts all the elements to the same type, if it can. Because R can’t reliably convert &quot;Wibble&quot; to a number, everything in the vector was converted to the character type instead. We get an error because R can’t mutiply words together. If you’re not sure what type of thing your vector contains, you can use the typeof function: typeof(1:10) ## [1] &quot;integer&quot; typeof(runif(10)) ## [1] &quot;double&quot; typeof(c(1, 2, &quot;Wibble&quot;)) ## [1] &quot;character&quot; Here the meaning of integer should be self explanatory. The vector runif(10) has type double, because it contains ‘double-precision’ floating point numbers. For our purposes you can just think of double as meaning any number with decimal places. The last vector has the type character because it includes the character string Wibble, and all the other numbers in it were coerced to become character strings too. If we want to (safely) mix up different types of object without them being converted we need a proper list, rather than a vector. In R we would write: my.list &lt;- list(2, 2, &quot;Wibble&quot;) We can still access elements from lists as we do for vectors, although now we need to use double square brackets, for example: my.list[[1]] ## [1] 2 But now our numbers haven’t been converted to character strings, and we can still multiply them. my.list[[1]] * my.list[[2]] ## [1] 4 Square brackets are ugly and can be confusing though, so we often give names to the elements of our list when we create it: my.party &lt;- list(number.guests=8, when=&quot;Friday&quot;, drinks = c(&quot;Juice&quot;, &quot;Beer&quot;, &quot;Whisky&quot;)) Which means we can then access the elements by name later on. To do this, you write the name of the vector, then a $ sign, and then the name of the element you want to access: my.party$when ## [1] &quot;Friday&quot; You might have spotted that we included a vector inside the party list. This is not a problem, and we can still access individual elements of this vector too: my.party$drinks[1] ## [1] &quot;Juice&quot; Create a vector containing 3 numbers then: Access just the last number Create a new vector containing just the first and last number Create a list containing your address and your age in years. Then: Multiply your age in years by your flat or house number (by accessing the relevant elements in the list) Run the following R code and explain what has happened: sqrt(1:10) * 10 ## [1] 10.00000 14.14214 17.32051 20.00000 22.36068 24.49490 26.45751 ## [8] 28.28427 30.00000 31.62278 Extended questions: What is the average of the 9 times table, up to and including 9 x 1000? Use the paste and c(...) functions to create a vector which contains the sequence “1 elephant”, “2 elephants”, …, “1000 elephants”. "],
["packages.html", "Packages", " Packages R has been around for a very long time, but has remained popular because it is easy for people to add new functions to it. You can run almost any statistical model and produce a wide variety of graphics in R because people have contributed new functions and these extend the base language. These new features are distributed in bundles known as ‘packages’. For now we’ll assume someone has helped you install all the packages you need5. To access the features in packages, you normally load the package with the library() function. Running library(&lt;packagename&gt;) loads all the new functions within it, and it is then possible to call them from your code. For example, typing: library(ggplot2) Will load the ggplot2 package. You can then call the qplot function it provides: qplot(mtcars$mpg, bins=7) You don’t strictly need to load packages to use the features within them though. If a package is installed on your system you can also call a function it provides directly. In the example below we call the hist.data.frame from the Hmisc package, and obtain histograms of all the variables in the mtcars dataset: Hmisc::hist.data.frame(mtcars) The rule is to type package::function(parameters), where :: separates the package and function names. Parameters are just the inputs to the function. There are two reasons not to load a package before using it: Laziness: it can save typing if you just want to use one function from a package, and only once. Explicitness: It’s an unfortunate fact that some function names are repeated across a number of packages. This can be confusing if they work differently, and if you don’t know which package the version you are using comes from. Using package_name:function_name can help make things explicit. See the installation guide if this isn’t the case↩ "],
["datasets-dataframes.html", "2 Datasets and dataframes", " 2 Datasets and dataframes A dataframe is an object which can store data as you might encounter it in SPSS, Stata, or other statistics packages. It’s much like a spreadsheet, but with some constraints applied. ‘Constraints’ sound bad, but are helpful here: they make dataframes more structured and predictable to work with: Each column is a vector, and so can only store one type of data. Every column has to be the same length (although missing values are allowed). Each column should have a name. Put another way, a dataframe behaves like a list of vectors, which means we can use a lot of the same rules to access elements within them (more on this below). Using ‘built in’ data The quickest way to see a dataframe in action is to use one that is built in to R. For example: head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 Or head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 In both these examples the datasets (airquality and mtcars) are already loaded and available to be used in the head() function. To find a list of all the built in datasets you can type help(datasets) or see https://stat.ethz.ch/R-manual/R-devel/library/datasets/html/00Index.html. Familiarise yourself with some of the other included datasets, e.g. datasets::attitude. Watch out that not all the included datasets are dataframes: Some are just vectors of observations (e.g. the airmiles data) and some are ‘time-series’, (e.g. the co2 data) "],
["looking-at-data.html", "Looking at data", " Looking at data As we’ve already seen, using print(df) within an RMarkdown document creates a nice interactive table you can use to look at your data. However you won’t want to print your whole data file when you Knit your RMarkdown document. The head function can be useful if you just want to show a few rows: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 Or we can use glimpse() function from the dplyr:: package (see the section on loading and using packages) for a different view of the first few rows of the mtcars data. This flips the dataframe so the variables are listed in the first column of the output: glimpse(mtcars) ## Observations: 32 ## Variables: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2... ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ... You can use the pander() function (from the pander:: package) to format tables nicely, for when you Knit a document to HTML, Word or PDF. For example: library(pander) pander(head(airquality), caption=&quot;Tables always need a caption.&quot;) Tables always need a caption. Ozone Solar.R Wind Temp Month Day 41 190 7.4 67 5 1 36 118 8 72 5 2 12 149 12.6 74 5 3 18 313 11.5 62 5 4 NA NA 14.3 56 5 5 28 NA 14.9 66 5 6 See the section on sharing and publishing for more ways to format and present tables. Other useful functions for looking at and exploring datasets include: summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA&#39;s :37 NA&#39;s :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Or the more compact and useful output from describe() which is in the pysch package: # fast = T just skips some of the available stats (e.g. kurtosis) psych::describe(airquality, fast = T) %&gt;% pander(caption=&quot;Summary statistics generated by the `psych::describe()` function.&quot;) Summary statistics generated by the psych::describe() function. vars n mean sd min max range se Ozone 1 116 42.13 32.99 1 168 167 3.063 Solar.R 2 146 185.9 90.06 7 334 327 7.453 Wind 3 153 9.958 3.523 1.7 20.7 19 0.2848 Temp 4 153 77.88 9.465 56 97 41 0.7652 Month 5 153 6.993 1.417 5 9 4 0.1145 Day 6 153 15.8 8.865 1 31 30 0.7167 There are also some helpful plotting functions which accept a whole dataframe: boxplot(airquality) Figure 2.1: Box plot of all variables in a dataset. psych::cor.plot(airquality) Figure 2.2: Correlation heatmap of all variables in a dataset. Colours indicate size of the correlation between pairs of variables. These plots might not be worth including in a final write-up, but are very useful when exploring your data. Using individual columns Because dataframes are like a list of vectors, we can access individual columns using the $ symbol. This extracts the column as a vector, so we can pass it to functions: mean(mtcars$mpg) ## [1] 20.09062 Or slice it: (first.mpg &lt;- mtcars$mpg[1]) ## [1] 21 (worst.mpg &lt;- sort(mtcars$mpg)[1]) ## [1] 10.4 (best.mpg &lt;- rev(sort(mtcars$mpg))[1]) ## [1] 33.9 The problem with extracting individual columns in this way is that it’s easy for data to be taken out of context: for example, if you sort individual columns then your data might get mixed up. In general if you are accessing individual columns of data in this way it’s a sign your code may be brittle and vulnerable to errors. In later sections we introduce methods for working on the whole dataset, which tends to be safer. "],
["working-with-dataframes.html", "Working with dataframes", " Working with dataframes Introducing the tidyverse R includes hundreds of built-in ways to select individual elements, rows or columns from a dataframe. This guide isn’t going to teach you many of them. The truth is that R can be overwhelming to new users, especially those new to programming. R is sometimes too powerful and flexible: there are too many different to accomplish the same end, and this can lead to confusion. Recently, a suite of packages has been developed for R which tries to provide a simple, consistent set of tools for working with data and graphics. This suite of packages is called the tidyverse, and you can load all of these packages by calling: library(tidyverse) In this guide we make much use of two components from the tidyverse: dplyr: to select, filter and summarise data ggplot2: to make plots It’s strongly recommended that you use these in your own code. "],
["selecting-columns.html", "Selecting columns", " Selecting columns Selecting a single column Because dataframes act like lists of vectors, we can access columns from them using the $ symbol. For example, here we select the Ozone column, which returns a vector of the observations made: airquality$Ozone ## [1] 41 36 12 18 NA 28 23 19 8 NA 7 16 11 14 18 14 34 ## [18] 6 30 11 1 11 4 32 NA NA NA 23 45 115 37 NA NA NA ## [35] NA NA NA 29 NA 71 39 NA NA 23 NA NA 21 37 20 12 13 ## [52] NA NA NA NA NA NA NA NA NA NA 135 49 32 NA 64 40 77 ## [69] 97 97 85 NA 10 27 NA 7 48 35 61 79 63 16 NA NA 80 ## [86] 108 20 52 82 50 64 59 39 9 16 78 35 66 122 89 110 NA ## [103] NA 44 28 65 NA 22 59 23 31 44 21 9 NA 45 168 73 NA ## [120] 76 118 84 85 96 78 73 91 47 32 20 23 21 24 44 21 28 ## [137] 9 13 46 18 13 24 16 13 23 36 7 14 30 NA 14 18 20 And we can pass this vector to functions, for example summary(): summary(airquality$Ozone) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## 1.00 18.00 31.50 42.13 63.25 168.00 37 Selecting more than one column To select multiple columns the select() function from dplyr is the simplest solution. You give select() a dataframe plus the names of the columns you want, and it returns a new dataframe with just those columns, in the order you specified: head( select(mtcars, cyl, hp) ) ## cyl hp ## Mazda RX4 6 110 ## Mazda RX4 Wag 6 110 ## Datsun 710 4 93 ## Hornet 4 Drive 6 110 ## Hornet Sportabout 8 175 ## Valiant 6 105 Because all the main dplyr functions tend to return a new dataframe, we can assign the results to a variable, and use that as normal: cylandweight &lt;- select(mtcars, cyl, wt) summary(cylandweight) ## cyl wt ## Min. :4.000 Min. :1.513 ## 1st Qu.:4.000 1st Qu.:2.581 ## Median :6.000 Median :3.325 ## Mean :6.188 Mean :3.217 ## 3rd Qu.:8.000 3rd Qu.:3.610 ## Max. :8.000 Max. :5.424 You can also put a minus (-) sign in front of the column name to indicate which columns you don’t want: head(select(airquality, -Ozone, -Solar.R, -Wind)) ## Temp Month Day ## 1 67 5 1 ## 2 72 5 2 ## 3 74 5 3 ## 4 62 5 4 ## 5 56 5 5 ## 6 66 5 6 You can use a patterns to match a subset of the columns you want. For example, here we select all the columns where the name contains the letter d: head(select(mtcars, contains(&quot;d&quot;))) ## disp drat ## Mazda RX4 160 3.90 ## Mazda RX4 Wag 160 3.90 ## Datsun 710 108 3.85 ## Hornet 4 Drive 258 3.08 ## Hornet Sportabout 360 3.15 ## Valiant 225 2.76 And you can combine these techniques to make more complex selections: head(select(mtcars, contains(&quot;d&quot;), -drat)) ## disp ## Mazda RX4 160 ## Mazda RX4 Wag 160 ## Datsun 710 108 ## Hornet 4 Drive 258 ## Hornet Sportabout 360 ## Valiant 225 As a quick reference, you can use the following ‘verbs’ to select columns in different ways: starts_with() ends_with() contains() everything() There are other commands too, but these are probably the most useful to begin with. See the help files for more information. "],
["selecting-rows.html", "Selecting rows", " Selecting rows To select particular rows from a dataframe, dplyr provides the filter() function. If we only want to see the 6-cylindered cars from the mtcars dataframe: filter(mtcars, cyl==6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 5 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 6 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 7 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 The filter function selects rows matching criteria you set: in this case, that cyl==6. We can match two criteria at once if needed: filter(mtcars, cyl==6 &amp; gear==3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## 2 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 "],
["operators.html", "‘Operators’", " ‘Operators’ When selecting rows in the example above we used two equals signs == to compare values. However, there are other operators we can use to create filters. Rather than describe them, the examples below demonstrate what each of them do. Equality and matching To compare a single value we use == 2 == 2 ## [1] TRUE And in a filter: filter(mtcars, cyl==6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 5 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 6 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 7 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 You might have noted above that we write == rather than just = to define the criteria. This is because most programming languages, including R, use two = symbols to distinguish: comparison from assignment. Presence/absence To test if a value is in a vector of suitable matches we can use: %in%: 2 %in% 1:10 ## [1] TRUE 100 %in% 1:10 ## [1] FALSE And, perhaps less obviously, we can test whether each value in a vector is in a second vector. This returns a vector of TRUE/FALSE values as long as the first list: c(1, 2) %in% c(2, 3, 4) ## [1] FALSE TRUE And this is very useful in a dataframe filter: head(filter(mtcars, cyl %in% c(4, 6))) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108.0 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 6 24.4 4 146.7 62 3.69 3.190 20.00 1 0 4 2 Greater/less than The &lt; and &gt; symbols work as you’d expect: head(filter(mtcars, cyl &gt; 4)) head(filter(mtcars, cyl &lt; 5)) You can also use &gt;= and &lt;=: filter(mtcars, cyl &gt;= 6) filter(mtcars, cyl &lt;= 4) Negation (opposite of) The ! is very useful to tell R to reverse an expression; that is, take the opposite of the value. In the simplest example: !TRUE ## [1] FALSE This is helpful because we can reverse the meaning of other expressions: is.na(NA) ## [1] TRUE !is.na(NA) ## [1] FALSE And we can use in filters. Here we select rows where Ozone is missing (NA): filter(airquality, is.na(Ozone)) And here we use ! to reverse the expression and select rows which are not missing: filter(airquality, !is.na(Ozone)) Try running these commands for yourself and experiment with changing the operators to make select different combinations of rows Other logical operators There are operators for ‘and’/‘or’ which can combine other filters filter(mtcars, hp &gt; 200 | wt &gt; 4) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 2 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 3 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 4 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 5 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 6 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 7 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 8 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 Using &amp; (and) makes the filter more restrictive: filter(mtcars, hp &gt; 200 &amp; wt &gt; 4) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 ## 2 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4 ## 3 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 Finally, you can set the order in which operators are applied by using parentheses. This means these expressions are subtly different: # first filter(mtcars, (hp &gt; 200 &amp; wt &gt; 4) | cyl==8) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 18.7 8 360.0 175 3.15 3.440 17.02 0 0 3 2 ## 2 14.3 8 360.0 245 3.21 3.570 15.84 0 0 3 4 ## 3 16.4 8 275.8 180 3.07 4.070 17.40 0 0 3 3 ## 4 17.3 8 275.8 180 3.07 3.730 17.60 0 0 3 3 ## 5 15.2 8 275.8 180 3.07 3.780 18.00 0 0 3 3 ## 6 10.4 8 472.0 205 2.93 5.250 17.98 0 0 3 4 ## 7 10.4 8 460.0 215 3.00 5.424 17.82 0 0 3 4 ## 8 14.7 8 440.0 230 3.23 5.345 17.42 0 0 3 4 ## 9 15.5 8 318.0 150 2.76 3.520 16.87 0 0 3 2 ## 10 15.2 8 304.0 150 3.15 3.435 17.30 0 0 3 2 ## 11 13.3 8 350.0 245 3.73 3.840 15.41 0 0 3 4 ## 12 19.2 8 400.0 175 3.08 3.845 17.05 0 0 3 2 ## 13 15.8 8 351.0 264 4.22 3.170 14.50 0 1 5 4 ## 14 15.0 8 301.0 335 3.54 3.570 14.60 0 1 5 8 # second reordered evaluation filter(mtcars, hp &gt; 200 &amp; (wt &gt; 4 | cyl==8)) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 14.3 8 360 245 3.21 3.570 15.84 0 0 3 4 ## 2 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 ## 3 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4 ## 4 14.7 8 440 230 3.23 5.345 17.42 0 0 3 4 ## 5 13.3 8 350 245 3.73 3.840 15.41 0 0 3 4 ## 6 15.8 8 351 264 4.22 3.170 14.50 0 1 5 4 ## 7 15.0 8 301 335 3.54 3.570 14.60 0 1 5 8 Try writing in plain English the meaning of the two filter expressions above "],
["sorting.html", "Sorting", " Sorting You can sort dataframes with arrange(): airquality %&gt;% arrange(Ozone) %&gt;% head ## Ozone Solar.R Wind Temp Month Day ## 1 1 8 9.7 59 5 21 ## 2 4 25 9.7 61 5 23 ## 3 6 78 18.4 57 5 18 ## 4 7 NA 6.9 74 5 11 ## 5 7 48 14.3 80 7 15 ## 6 7 49 10.3 69 9 24 By default sorting is ascending, but you can use a minus sign to reverse this: airquality %&gt;% arrange(-Ozone) %&gt;% head ## Ozone Solar.R Wind Temp Month Day ## 1 168 238 3.4 81 8 25 ## 2 135 269 4.1 84 7 1 ## 3 122 255 4.0 89 8 7 ## 4 118 225 2.3 94 8 29 ## 5 115 223 5.7 79 5 30 ## 6 110 207 8.0 90 8 9 You can sort on multiple columns too, but the order of the variables makes a difference: airquality %&gt;% select(Month, Ozone) %&gt;% arrange(Month, -Ozone) %&gt;% head ## Month Ozone ## 1 5 115 ## 2 5 45 ## 3 5 41 ## 4 5 37 ## 5 5 36 ## 6 5 34 airquality %&gt;% select(Month, Ozone) %&gt;% arrange(-Ozone, Month) %&gt;% head ## Month Ozone ## 1 8 168 ## 2 7 135 ## 3 8 122 ## 4 8 118 ## 5 5 115 ## 6 8 110 "],
["pipes.html", "Pipes", " Pipes We often want to combine select and filter (and other functions) to return a subset of our original data. As you might have noticed above, we can ‘nest’ function calls in R. For example, we might want to select specific columns and filter out some rows. Taking the mtcars data, we might want to select the weights of only those cars with low mpg: gas.guzzlers &lt;- select(filter(mtcars, mpg &lt; 15), wt) summary(gas.guzzlers) ## wt ## Min. :3.570 ## 1st Qu.:3.840 ## Median :5.250 ## Mean :4.686 ## 3rd Qu.:5.345 ## Max. :5.424 This is OK, but can get quite confusing to read, and the more deeply functions are nested the easier it is to make a mistake. dplyr provides an alternative to nested function calls, called the pipe. Imagine your dataframe as a big bucket containing data. From this bucket, you can ‘pour’ your data down through a series of tubes and filters, until at the bottom of your screen you have a smaller bucket containing just the data you want. Think of your data ‘flowing’ down the screen. To make data flow from one bucket to another, we use the ‘pipe’ operator: %&gt;% # dataframes are just &#39;buckets&#39; of data big.bucket.of.data &lt;- mtcars big.bucket.of.data %&gt;% filter(mpg &lt;15) %&gt;% select(wt) %&gt;% summary ## wt ## Min. :3.570 ## 1st Qu.:3.840 ## Median :5.250 ## Mean :4.686 ## 3rd Qu.:5.345 ## Max. :5.424 So we have achieved the same outcome, but the code reads as a series of operations which the data flows through, connected by our pipes (the %&gt;%). At the end of the last pipe, our data gets dumped into the summary() function6 We could just as well have saved this smaller ‘bucket’ of data so we can use it later on: smaller.bucket &lt;- big.bucket.of.data %&gt;% filter(mpg &lt;15) %&gt;% select(wt) This turns out to be an incredibly useful pattern when processing and working with data. We can pour data through a series of filters and other operations, saving intermediate states where necessary. You can insert the %&gt;% symbol in RStdudio by typing cmd-shift-M, which saves a lot of typing. You might notice that when we write the select function we don’t explicitly name the dataframe to be used. This is because R implicitly passes the output of the pipe to the first argument of the function. So here, the output of filter(mpg&lt;15) is used as the dataframe in the select function.↩ "],
["mutate.html", "Modifying and creating new columns", " Modifying and creating new columns Often when working with data we want to compute new values from columns we already have. Let’s imagine we have collected data from patients using the PHQ-9 questionnaire, which measures depression: phq9.df &lt;- readr::read_csv(&quot;phq.csv&quot;) glimpse(phq9.df) ## Observations: 2,429 ## Variables: 12 ## $ patient &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ... ## $ phq9_01 &lt;int&gt; 3, 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 3, ... ## $ phq9_02 &lt;int&gt; 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_03 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, ... ## $ phq9_04 &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, ... ## $ phq9_05 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, ... ## $ phq9_06 &lt;int&gt; 3, 2, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_07 &lt;int&gt; 3, 3, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 1, 1, ... ## $ phq9_08 &lt;int&gt; 0, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, ... ## $ phq9_09 &lt;int&gt; 2, 2, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, ... ## $ month &lt;int&gt; 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 0, 1,... ## $ group &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ... Each patients’ PHQ-9 score is calculated by summing all of the individual item scores. So - we want to create a new column containing the sum of each row. This is fairly easy with the dplyr::mutate() function: phq9.scored.df &lt;- phq9.df %&gt;% mutate(phq9 = phq9_01 + phq9_02 + phq9_03 + phq9_04 + phq9_05 + phq9_06 + phq9_07 + phq9_08 + phq9_09) phq9.scored.df %&gt;% select(patient, group, month, phq9) %&gt;% head ## # A tibble: 6 x 4 ## patient group month phq9 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 0 23 ## 2 2 0 0 21 ## 3 2 0 1 17 ## 4 2 0 2 18 ## 5 2 0 3 22 ## 6 2 0 4 21 Notice that we first stored the computed scores in phq9.scored.df and then used select() to get rid of the raw data columns to display only what we needed. See this section on summarising and processing data for a neater way to create summary scores in this sort of situation. "],
["real-data.html", "3 ‘Real’ data", " 3 ‘Real’ data Note: If you already lucky enough to have nicely formatted data, ready for use in R, then you could skip this section and revisit it later, save for the section on factors and other variable types. Most tutorials and textbooks use neatly formatted example datasets to illustrate particular techniques. However in the real-world our data can be: In the wrong format Spread across multiple files Badly coded, or with errors Incomplete, with values missing for many different reasons This chapter will give you techniques to address each of these problems. "],
["importing-data.html", "Importing data", " Importing data If you have data outside of R, the simplest way to import it is to first save it as a comma or tab-separated text file, normally with the file extension .csv or .txt7. Let’s say we have file called angry_moods.csv in the same directory as our .Rmd file. We can read this data using the read_csv() function from the readr package8: angry.moods &lt;- readr::read_csv(&#39;data/angry_moods.csv&#39;) head(angry.moods) ## # A tibble: 6 x 7 ## Gender Sports Anger.Out Anger.In Control.Out Control.In Anger.Expression ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1 18 13 23 20 36 ## 2 2 1 14 17 25 24 30 ## 3 2 1 13 14 28 28 19 ## 4 2 1 17 24 23 23 43 ## 5 1 1 16 17 26 28 27 ## 6 1 1 16 22 25 23 38 As you can see, when loading the .csv file the read_csv() makes some assumptions about the type of data the file contains. In this case, all the columns contain integer values. It’s worth checking this message to make sure that stray cells in the file you are importing don’t cause problems when importing. Excel won’t complain about this sort of thing, but R is more strict and won’t mix text and numbers in the same column. A common error is for stray notes or text values in a spreadsheet to cause a column which should be numeric to be converted to the character type. Once it’s loaded, you can use this new dataset like any other: pairs(angry.moods) Importing data over the web One neat feature of the readr package is that you can import data from the web, using a URL rather than a filename on your local computer. This can be really helpful when sharing data and code with colleagues. For example, we can load the angry_moods.csv file from a URL: angry.moods.from.url &lt;- readr::read_csv( &quot;https://raw.githubusercontent.com/benwhalley/just-enough-r/master/angry_moods.csv&quot;) head(angry.moods.from.url) Importing from SPSS and other packages This is often more trouble than it’s worth. If using Excel for example, it’s best just to save your data a csv file first and import that. But if you really must use other formats see https://www.datacamp.com/community/tutorials/r-data-import-tutorial. This is easy to achieve in Excel and most other stats packages using the Save As... menu item↩ There are also standard functions built into R, such as read.csv() or read.table() for importing data. These are fine if you can’t install the readr package for some reason, but they are quite old and the default behaviour is sometimes counterintuitive. I recommend using the readr equivalents: read_csv() or read_tsv().↩ "],
["saving-and-exporting.html", "Saving and exporting", " Saving and exporting Before you start saving data in csv or any other format, as yourself: “Do I need to save this dataset, or should I simply save my raw data and code?” Oftentimes it’s best to keep only your raw datafiles, with the R code you used to process them. This keeps your disk tidier, and avoids confusion with multiple versions of files. If it takes a long time to process your data though you might want to save interim steps. And if you share your data (which you should) you might also want to save simplified or anonymised versions of it, in widely-accessible formats. Use CSV files Comma-separated-values files are a plain text format which are idea for storing and sharing your data. They are: Understood by almost every piece of software, ever Will be readable in future Perfect for storing 2D data (like dataframes) Readable by humans (just open them in Notepad) Commercial formats like Excel, SPSS (.sav) and Stata (.dta) don’t have these properties. Although CSV has some disadvantages, they are all easily overcome if you save the steps of your data processing and analysis in your R code, see below. Saving a dataframe to .csv is as simple as: readr::write_csv(mtcars, &#39;mtcars.csv&#39;) If you run this within an RMarkdown document, this will create the new csv file in the same directory as your .Rmd file. You can also use the write.csv() function in base R, but this version from readr is faster and has more sensible defalts (e.g. it doesn’t write rownames, but does save column names in the first row) Save processes, not just outcomes Many students (and academics) make errors in their analyses because they process data by hand (e.g. editing files in Excel) or use GUI tools to run analyses. In both cases these errors are hard to identify or rectify because only the outputs of the analysis can be saved, and no record has been made of how these outputs were produced. In contrast, if you do your data processing and analysis in R/RMarkdown you benefit from a concrete, repeatable series of steps which can be checked/verified by others. This can also save lots of time if you need to processing additional data later on (e.g. if you run more participants). Some principles to follow when working: Save your raw data in the simplest possible format, in CSV Always include column names in the file Use descriptive names, but with a regular strucuture. Never include spaces or special characters in the column names. Use underscores (_) if you want to make things more readable. Make names &lt;20 characters in length if possible Saving interim steps If you are saving data to use again later in R, the best format is RDS. Saving files to RDS is covered in a later section (click to see). If you are saving interim steps but think you might possibly want to access it from other programmes in future use csv though. To save something using RDS: # create a huge df of random numbers... massive.df &lt;- data_frame(nums = rnorm(1:1e6)) saveRDS(massive.df, file=&quot;massive.RDS&quot;) Then later on you can load it like this: restored.massive.df &lt;- readRDS(&#39;massive.RDS&#39;) If you do this in RMarkdown, by default the RDS files will be saved in the same directory as your .Rmd file. Archiving, publication and sharing If you want to share data with someone else, or open it in a different software package, using ‘.csv’ format is strongly recommended unless some other format is common in your field. When archiving data, or sharing with others, you must document what each column measures, and any processing steps used to create the file. RMarkdown is a good way of doing this because it can combine the processing with narrative explaining what is being done, and why. "],
["multiple-raw-data-files.html", "Dealing with multiple files", " Dealing with multiple files Often you will have multiple data files files - for example, those produced by experimental software. This is one of the few times when you might have to do something resembling ‘real programming’, but it’s still fairly straightforward. In the repeated measures Anova example later on in this guide we encounter some data from an experiment where reaction times were recorded in 25 trials (Trial) before and after (Time) one of 4 experimental manipulations (Condition = {1,2,3,4}). There were 48 participants in total: Let’s say all the files are in a single directory, and numbered sequentially. Using the list.files() function we can list the contents of a directory on the hard drive: list.files(&#39;data/multiple-file-example/&#39;) ## [1] &quot;person1.csv&quot; &quot;person10.csv&quot; &quot;person11.csv&quot; &quot;person12.csv&quot; ## [5] &quot;person13.csv&quot; &quot;person14.csv&quot; &quot;person15.csv&quot; &quot;person16.csv&quot; ## [9] &quot;person17.csv&quot; &quot;person18.csv&quot; &quot;person19.csv&quot; &quot;person2.csv&quot; ## [13] &quot;person20.csv&quot; &quot;person21.csv&quot; &quot;person22.csv&quot; &quot;person23.csv&quot; ## [17] &quot;person24.csv&quot; &quot;person25.csv&quot; &quot;person26.csv&quot; &quot;person27.csv&quot; ## [21] &quot;person28.csv&quot; &quot;person29.csv&quot; &quot;person3.csv&quot; &quot;person30.csv&quot; ## [25] &quot;person31.csv&quot; &quot;person32.csv&quot; &quot;person33.csv&quot; &quot;person34.csv&quot; ## [29] &quot;person35.csv&quot; &quot;person36.csv&quot; &quot;person37.csv&quot; &quot;person38.csv&quot; ## [33] &quot;person39.csv&quot; &quot;person4.csv&quot; &quot;person40.csv&quot; &quot;person41.csv&quot; ## [37] &quot;person42.csv&quot; &quot;person43.csv&quot; &quot;person44.csv&quot; &quot;person45.csv&quot; ## [41] &quot;person46.csv&quot; &quot;person47.csv&quot; &quot;person48.csv&quot; &quot;person5.csv&quot; ## [45] &quot;person6.csv&quot; &quot;person7.csv&quot; &quot;person8.csv&quot; &quot;person9.csv&quot; list.files() creates a vector of the names of all the files in the directory. At this point, there are many, many ways of importing the contents of these files, but below we use a technique which is concise, reliable, and less error-prone than many others. It also continues to use the dplyr library. This approach has 3 steps: Put all the names of the .csv files into a dataframe. For each row in the dataframe, run a function which imports the file as a dataframe. Combine all these dataframes together. Putting the filenames into a dataframe Because list.files produces a vector, we can make them a column in a new dataframe: raw.files &lt;- data_frame(filename = list.files(&#39;data/multiple-file-example/&#39;)) And we can make a new column with the complete path (i.e. including the directory holding the files), using the paste0 which combines strings of text. We wouldn’t have to do this if the raw files were in the same directory as our RMarkdown file, but that would get messy. raw.file.paths &lt;- raw.files %&gt;% mutate(filepath = paste0(&quot;data/multiple-file-example/&quot;, filename)) raw.file.paths %&gt;% head(3) ## # A tibble: 3 x 2 ## filename filepath ## &lt;chr&gt; &lt;chr&gt; ## 1 person1.csv data/multiple-file-example/person1.csv ## 2 person10.csv data/multiple-file-example/person10.csv ## 3 person11.csv data/multiple-file-example/person11.csv Using do() We can then use the do() function in dplyr:: to import the data for each file and combine the results in a single dataframe. The do() function allows us to run any R function for each group or row in a dataframe. The means that our original dataframe is broken up into chunks (either groups of rows, if we use group_by(), or individual rows if we use rowwise()) and each chunk is fed to the function we specify. This function must do it’s work and return a new dataframe, and these are then combined into a single larger dataframe. So in this example, we break our dataframe of filenames up into individual rows using rowwise and then specify the read_csv function which takes the name of a csv file, and returns the content as a dataframe (see the importing data section). For example: raw.data &lt;- raw.file.paths %&gt;% # &#39;do&#39; the function for each row in turn rowwise() %&gt;% do(., read_csv(file=.$filepath)) We can check these data look OK by sampling 10 rows at random: raw.data %&gt;% sample_n(10) %&gt;% pander() Condition trial time person RT 1 8 2 4 435.3 3 4 1 26 283.1 1 12 2 10 280.4 2 7 2 19 149.9 1 15 2 4 322.2 1 23 2 1 261.2 2 7 2 20 309.5 1 13 1 9 224 3 8 2 29 437.9 4 5 2 46 212.3 Using custom functions with do() In this example, each of the raw data files included the participant number (the person variable). However, this isn’t always the case. This isn’t a problem though, if we create our own helper function to import the data. Writing small functions in R is very easy, and the example below wraps the read.csv() function and adds a new colum, filename to the imported data frame which would enable us to keep track of where each row in the final combined dataset came from. This is the helper function: read.csv.and.add.filename &lt;- function(filepath){ read_csv(filepath) %&gt;% mutate(filepath=filepath) } In English, you should read this as: “Create a new R function called read.csv.and.add.filename which expects to be passed a path to a csv file as an input. This function reads the csv file at the path (converting it to a dataframe), and adds a new column containing the original file path it read from. It then returns this dataframe.” We can use our helper function with do() in place of the bare read_csv function we used before: raw.data.with.paths &lt;- raw.file.paths %&gt;% rowwise() %&gt;% do(., read.csv.and.add.filename(.$filepath)) raw.data.with.paths %&gt;% sample_n(10) %&gt;% pander() Table continues below Condition trial time person RT 4 22 1 42 346.8 3 8 1 36 256.8 4 22 1 45 277.1 4 18 1 42 236.6 1 21 1 11 230.6 3 25 2 31 299.3 4 13 2 44 212.6 1 2 2 4 246.2 2 6 2 20 297.3 3 4 2 34 358.3 filepath data/multiple-file-example/person42.csv data/multiple-file-example/person36.csv data/multiple-file-example/person45.csv data/multiple-file-example/person42.csv data/multiple-file-example/person11.csv data/multiple-file-example/person31.csv data/multiple-file-example/person44.csv data/multiple-file-example/person4.csv data/multiple-file-example/person20.csv data/multiple-file-example/person34.csv At this point you might need to use the extract() or separate() functions to post-process the filename and re-create the person variable from this (although in this case that’s already been done for us). "],
["joining-data.html", "Joining different datasets", " Joining different datasets Many analyses combine data from different sources. Even within a single study, you may find that you have different datafiles (e.g. spreadsheets) for different sorts of information. For example you might have Multiple csv files output by experimental software. A spreadsheet of demographic characteristics of participants (e.g. their age, gender, handedness) Questionnaire data, collected before the reaction time task. Your main analysis might want to model individual RTs using predictors including age, handedness or some personality variable. Thus, you probably want data which look something like this: df %&gt;% pander() person trial condition female left.handed age extraversion RT 1 1 A TRUE FALSE 19 50 251.7 1 2 A TRUE FALSE 19 50 311.1 1 3 A TRUE FALSE 19 50 343.4 1 … A TRUE FALSE 19 50 206.2 2 1 B FALSE FALSE 24 34 317.2 2 2 B FALSE FALSE 24 34 320.2 2 3 B FALSE FALSE 24 34 277 2 … B FALSE FALSE 24 34 278.1 The raw data In this example, we could imagine our raw data files looking like this: rts %&gt;% pander() person trial RT 1 1 251.7 1 2 311.1 1 3 343.4 1 … 206.2 2 1 317.2 2 2 320.2 2 3 277 2 … 278.1 demographics %&gt;% pander() person age left.handed female 1 19 FALSE TRUE 2 24 FALSE FALSE 3 33 TRUE FALSE And: personality %&gt;% pander() person extraversion 1 50 2 34 3 47 Joining the parts To create the combined data file we want, we have to join the different files together. As you might have noticed, though, the RT’s file has many observations per participant, whereas the demographics and personality data has one row per person. What we need is to smush this wide format data into the RTs file, such that values get repeated for every row of each participants’ data. To do this, the simplest method is to use the dplyr::left_join() function: left_join(rts, demographics, by=&quot;person&quot;) %&gt;% pander person trial RT age left.handed female 1 1 251.7 19 FALSE TRUE 1 2 311.1 19 FALSE TRUE 1 3 343.4 19 FALSE TRUE 1 … 206.2 19 FALSE TRUE 2 1 317.2 24 FALSE FALSE 2 2 320.2 24 FALSE FALSE 2 3 277 24 FALSE FALSE 2 … 278.1 24 FALSE FALSE In this example I explicitly set id=&quot;person&quot; to let R know which variable to use to match the rows of data, although you don’t have to, and dplyr can normally guess. You do have to use the same variable name in both files though (so, person and participant couldn’t be matched, for example). Other types of joins As the dplyr manual states: left joins return for two dataframes, x and y will return all rows from x, and all columns from both x and y. Rows in x with no match in y will have NA values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned. Left joins are probably the most useful, but there are other types which can be useful. To see all of them look at the help files (help('join', 'dplyr')). To show one common example, if we wanted to check whether we were missing RT data for one of our participants, we could use an anti_join: anti_join(demographics, rts, by=&quot;person&quot;) %&gt;% pander person age left.handed female 3 33 TRUE FALSE This table lists the data for all the people in the demographics file, for whom we don’t have RT data. This can be a useful way of checking you haven’t mislaid a raw datafile (e.g. forgot to copy it from the lab machine!). "],
["factors-and-numerics.html", "Types of variable", " Types of variable When working with data in Excel or other packages like SPSS you’ve probably become aware that different types of data get treated differently. For example, in Excel you can’t set up a formula like =SUM(...) on cells which include letters (rather than just numbers). It does’t make sensible. However, Excel and many other programmes will sometimes make guesses about what to do if you combine different types of data. For example, in Excel, if you add 28 to 1 Feb 2017 the result is 1 March 2017. This is sometimes what you want, but can often lead to unexpected results and errors in data analyses. R is much more strict about not mixing types of data. Vectors (or columns in dataframes) can only contain one type of thing. In general, there are probably 4 types of data you will encounter in your data analysis: Numeric variables Character variables Factors Dates The file data/lakers.RDS contains a dataset adapted from the lubridate::lakers dataset (this is a dataset built into an add-on package for R). This dataset contains four variables to illustrate the common variable types (a subset of the original dataset which provides scores and other information from each Los Angeles Lakers basketball game in the 2008-2009 season). We have the date, opponent, team, and points variables. lakers &lt;- readRDS(&quot;data/lakers.RDS&quot;) lakers %&gt;% glimpse() ## Observations: 34,624 ## Variables: 4 ## $ date &lt;date&gt; 2008-10-28, 2008-10-28, 2008-10-28, 2008-10-28, 2008... ## $ opponent &lt;chr&gt; &quot;POR&quot;, &quot;POR&quot;, &quot;POR&quot;, &quot;POR&quot;, &quot;POR&quot;, &quot;POR&quot;, &quot;POR&quot;, &quot;POR... ## $ team &lt;fctr&gt; OFF, LAL, LAL, LAL, LAL, LAL, POR, LAL, LAL, POR, LA... ## $ points &lt;int&gt; 0, 0, 0, 0, 0, 2, 0, 1, 0, 2, 2, 0, 0, 2, 2, 0, 0, 2,... One thing to note here is that the glimpse() command tells us the type of each variable. So we have points: type int, short for integer (i.e. whole numbers). date: type date opponent: type chr, short for ‘character’, or alaphanumeric data team: type fctr, short for factor and Differences in quantity: numeric variables We’ve already seem numeric variables in the section on vectors and lists. These behave pretty much as you’d expect, and we won’t expand on them here. There are different types of numeric variable. Integers (whole numbers) are stored as type int but other types, like dbl, can can store numbers with a decimal place. For most purposes (in doing analyses of psychological data) the differences won’t matter. Differences in quality or kind In many cases variables will be used to identify values which are qualitatively different. For example, different groups or measurement occasions in an experimental study, or perhaps different genders or countries in survey data. In practice, these qualitative differences get stored in a range of different variable types, including: Numeric variables (e.g. time = 1, or time = 2…) Character variables (e.g. time = &quot;time 1&quot;, time = &quot;time 2&quot;…) Boolean or logical variables (e.g. time1 == TRUE or time1 == FALSE) ‘Factors’ Storing categories as numeric variables can produce confusing results when running regression models. For this reason, it’s normally best to store your categorical variables as descriptive strings of letters and numbers (e.g. “Treatment”, “Control”) and avoid simple numbers (e.g. 1, 2, 3). Or as a factor. Factors for categorical data Factors are R’s answer to the problem of storing categorical data. Factors assign one number for each unique value in a variable, and allow you to attach a label to it. This means the categories are stored as numbers ‘under the hood’, but you can also work with factors as though they were strings of letters and numbers, and they display nicely when making tables and graphs. For example: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 group.factor &lt;- factor(1:10) group.factor ## [1] 1 2 3 4 5 6 7 8 9 10 ## Levels: 1 2 3 4 5 6 7 8 9 10 group.labelled &lt;- factor(1:10, labels = paste(&quot;Group&quot;, 1:10)) group.labelled ## [1] Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 Group 7 ## [8] Group 8 Group 9 Group 10 ## 10 Levels: Group 1 Group 2 Group 3 Group 4 Group 5 Group 6 ... Group 10 We can see this ‘underlying’ number which represents each category by using as.numeric: # note, there is no guarantee that &quot;Group 1&quot; == 1 (although it is here) as.numeric(group.labelled) ## [1] 1 2 3 4 5 6 7 8 9 10 For simple analyses it’s often best to store everything as the character type (letters and numbers), but factors can still be useful for making tables or graphs where the list of categories is known and needs to be in a particular order. For more about factors, and lots of useful functions for working with them, see the forcats:: package: https://github.com/tidyverse/forcats Dates Internally, R stores dates as the number of days since January 1, 1970. This means that we can work with dates just like other numbers, and it makes sense to have the min(), or max() of a series of dates: # the first few dates in the sequence head(lakers$date) ## [1] &quot;2008-10-28&quot; &quot;2008-10-28&quot; &quot;2008-10-28&quot; &quot;2008-10-28&quot; &quot;2008-10-28&quot; ## [6] &quot;2008-10-28&quot; # first and last dates min(lakers$date) ## [1] &quot;2008-10-28&quot; max(lakers$date) ## [1] &quot;2009-04-14&quot; Because dates are numbers we can also do arithmetic with them, and R will give us a difference (in this case, in days): max(lakers$date) - min(lakers$date) ## Time difference of 168 days However, R does treat dates slightly differently from other numbers, and will format plot axes appropriately, which is helpful (see more on this in the graphics section): hist(lakers$date, breaks=7) "],
["missingvalues.html", "Missing values", " Missing values Missing values aren’t a data type as such, but are an important concept in R; the way different functions handle missing values can be both helpful and frustrating in equal measure. Missing values in a vector are denoted by the letters NA, but notice that these letters are unquoted. That is to say NA is not the same as &quot;NA&quot;! To check for missing values in a vector (or dataframe column) we use the is.na() function: nums.with.missing &lt;- c(1, 2, NA) nums.with.missing ## [1] 1 2 NA is.na(nums.with.missing) ## [1] FALSE FALSE TRUE Here the is.na() function has tested whether each item in our vector called nums.with.missing is missing. It returns a new vector with the results of each test: either TRUE or FALSE. We can also use the negation operator, the ! symbol to reverse the meaning of is.na. So we can read !is.na(nums) as “test whether the values in nums are NOT missing”: # test if missing is.na(nums.with.missing) ## [1] FALSE FALSE TRUE # test if NOT missing (note the exclamation mark in front of the function) !is.na(nums.with.missing) ## [1] TRUE TRUE FALSE We can use the is.na() function as part of dplyr filters: airquality %&gt;% filter(is.na(Solar.R)) %&gt;% head(3) %&gt;% pander Ozone Solar.R Wind Temp Month Day NA NA 14.3 56 5 5 28 NA 14.9 66 5 6 7 NA 6.9 74 5 11 Or to select only cases without missing values for a particular variable: airquality %&gt;% filter(!is.na(Solar.R)) %&gt;% head(3) %&gt;% pander Ozone Solar.R Wind Temp Month Day 41 190 7.4 67 5 1 36 118 8 72 5 2 12 149 12.6 74 5 3 Complete cases Sometimes we want to select only rows which have no missing values — i.e. complete cases. The complete.cases function accepts a dataframe (or matrix) and tests whether each row is complete. It returns a vector with a TRUE/FALSE result for each row: complete.cases(airquality) %&gt;% head ## [1] TRUE TRUE TRUE TRUE FALSE FALSE This can also be useful in dplyr filters. Here we show all the rows which are not complete (note the exclamation mark): airquality %&gt;% filter(!complete.cases(airquality)) ## Ozone Solar.R Wind Temp Month Day ## 1 NA NA 14.3 56 5 5 ## 2 28 NA 14.9 66 5 6 ## 3 NA 194 8.6 69 5 10 ## 4 7 NA 6.9 74 5 11 ## 5 NA 66 16.6 57 5 25 ## 6 NA 266 14.9 58 5 26 ## 7 NA NA 8.0 57 5 27 ## 8 NA 286 8.6 78 6 1 ## 9 NA 287 9.7 74 6 2 ## 10 NA 242 16.1 67 6 3 ## 11 NA 186 9.2 84 6 4 ## 12 NA 220 8.6 85 6 5 ## 13 NA 264 14.3 79 6 6 ## 14 NA 273 6.9 87 6 8 ## 15 NA 259 10.9 93 6 11 ## 16 NA 250 9.2 92 6 12 ## 17 NA 332 13.8 80 6 14 ## 18 NA 322 11.5 79 6 15 ## 19 NA 150 6.3 77 6 21 ## 20 NA 59 1.7 76 6 22 ## 21 NA 91 4.6 76 6 23 ## 22 NA 250 6.3 76 6 24 ## 23 NA 135 8.0 75 6 25 ## 24 NA 127 8.0 78 6 26 ## 25 NA 47 10.3 73 6 27 ## 26 NA 98 11.5 80 6 28 ## 27 NA 31 14.9 77 6 29 ## 28 NA 138 8.0 83 6 30 ## 29 NA 101 10.9 84 7 4 ## 30 NA 139 8.6 82 7 11 ## 31 NA 291 14.9 91 7 14 ## 32 NA 258 9.7 81 7 22 ## 33 NA 295 11.5 82 7 23 ## 34 78 NA 6.9 86 8 4 ## 35 35 NA 7.4 85 8 5 ## 36 66 NA 4.6 87 8 6 ## 37 NA 222 8.6 92 8 10 ## 38 NA 137 11.5 86 8 11 ## 39 NA 64 11.5 79 8 15 ## 40 NA 255 12.6 75 8 23 ## 41 NA 153 5.7 88 8 27 ## 42 NA 145 13.2 77 9 27 Sometimes it’s convenient to use the . (period) to represent the output from the previous pipe command. For example, we could rewrite the previous example as: airquality %&gt;% filter(!complete.cases(.)) # note the . (period) here in place of `airmiles` ## Ozone Solar.R Wind Temp Month Day ## 1 NA NA 14.3 56 5 5 ## 2 28 NA 14.9 66 5 6 ## 3 NA 194 8.6 69 5 10 ## 4 7 NA 6.9 74 5 11 ## 5 NA 66 16.6 57 5 25 ## 6 NA 266 14.9 58 5 26 ## 7 NA NA 8.0 57 5 27 ## 8 NA 286 8.6 78 6 1 ## 9 NA 287 9.7 74 6 2 ## 10 NA 242 16.1 67 6 3 ## 11 NA 186 9.2 84 6 4 ## 12 NA 220 8.6 85 6 5 ## 13 NA 264 14.3 79 6 6 ## 14 NA 273 6.9 87 6 8 ## 15 NA 259 10.9 93 6 11 ## 16 NA 250 9.2 92 6 12 ## 17 NA 332 13.8 80 6 14 ## 18 NA 322 11.5 79 6 15 ## 19 NA 150 6.3 77 6 21 ## 20 NA 59 1.7 76 6 22 ## 21 NA 91 4.6 76 6 23 ## 22 NA 250 6.3 76 6 24 ## 23 NA 135 8.0 75 6 25 ## 24 NA 127 8.0 78 6 26 ## 25 NA 47 10.3 73 6 27 ## 26 NA 98 11.5 80 6 28 ## 27 NA 31 14.9 77 6 29 ## 28 NA 138 8.0 83 6 30 ## 29 NA 101 10.9 84 7 4 ## 30 NA 139 8.6 82 7 11 ## 31 NA 291 14.9 91 7 14 ## 32 NA 258 9.7 81 7 22 ## 33 NA 295 11.5 82 7 23 ## 34 78 NA 6.9 86 8 4 ## 35 35 NA 7.4 85 8 5 ## 36 66 NA 4.6 87 8 6 ## 37 NA 222 8.6 92 8 10 ## 38 NA 137 11.5 86 8 11 ## 39 NA 64 11.5 79 8 15 ## 40 NA 255 12.6 75 8 23 ## 41 NA 153 5.7 88 8 27 ## 42 NA 145 13.2 77 9 27 This is nice because we can apply the complete.cases function to the output of the previous pipe. For example, if we wanted to select complete cases for a subset of the variables we could write: airquality %&gt;% select(Ozone, Solar.R) %&gt;% filter(!complete.cases(.)) ## Ozone Solar.R ## 1 NA NA ## 2 28 NA ## 3 NA 194 ## 4 7 NA ## 5 NA 66 ## 6 NA 266 ## 7 NA NA ## 8 NA 286 ## 9 NA 287 ## 10 NA 242 ## 11 NA 186 ## 12 NA 220 ## 13 NA 264 ## 14 NA 273 ## 15 NA 259 ## 16 NA 250 ## 17 NA 332 ## 18 NA 322 ## 19 NA 150 ## 20 NA 59 ## 21 NA 91 ## 22 NA 250 ## 23 NA 135 ## 24 NA 127 ## 25 NA 47 ## 26 NA 98 ## 27 NA 31 ## 28 NA 138 ## 29 NA 101 ## 30 NA 139 ## 31 NA 291 ## 32 NA 258 ## 33 NA 295 ## 34 78 NA ## 35 35 NA ## 36 66 NA ## 37 NA 222 ## 38 NA 137 ## 39 NA 64 ## 40 NA 255 ## 41 NA 153 ## 42 NA 145 Or alternatively: rows.to.keep &lt;- !complete.cases(select(airquality, Ozone, Solar.R)) airquality %&gt;% filter(rows.to.keep) %&gt;% head(3) %&gt;% pander Ozone Solar.R Wind Temp Month Day NA NA 14.3 56 5 5 28 NA 14.9 66 5 6 NA 194 8.6 69 5 10 Missing data and R functions It’s normally good practice to pre-process your data and select the rows you want to analyse before passing dataframes to R functions. The reason for this is that different functions behave differently with missing data. For example: mean(airquality$Solar.R) ## [1] NA Here the default for mean() is to return NA if any of the values are missing. We can explicitly tell R to ignore missing values by setting na.rm=TRUE mean(airquality$Solar.R, na.rm=TRUE) ## [1] 185.9315 In contrast some other functions, for example the lm() which runs a linear regression will ignore missing values by default. If we run summary on the call to lm then we can see the line near the bottom of the output which reads: “(7 observations deleted due to missingness)” lm(Solar.R ~ Temp, data=airquality) %&gt;% summary ## ## Call: ## lm(formula = Solar.R ~ Temp, data = airquality) ## ## Residuals: ## Min 1Q Median 3Q Max ## -169.697 -59.315 6.224 67.685 186.083 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -24.431 61.508 -0.397 0.691809 ## Temp 2.693 0.782 3.444 0.000752 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 86.86 on 144 degrees of freedom ## (7 observations deleted due to missingness) ## Multiple R-squared: 0.07609, Adjusted R-squared: 0.06967 ## F-statistic: 11.86 on 1 and 144 DF, p-value: 0.0007518 Normally R will do the ‘sensible thing’ when there are missing values, but it’s always worth checking whether you do have any missing data, and addressing this explicitly in your code Patterns of missingness The mice package has some nice functions to describe patterns of missingness in the data. These can be useful both at the exploratory stage, when you are checking and validating your data, but can also be used to create tables of missingness for publication: mice::md.pattern(airquality) ## Wind Temp Month Day Solar.R Ozone ## 111 1 1 1 1 1 1 0 ## 35 1 1 1 1 1 0 1 ## 5 1 1 1 1 0 1 1 ## 2 1 1 1 1 0 0 2 ## 0 0 0 0 7 37 44 In this table, md.pattern list the number of cases with particular patterns of missing data. - Each row describes a misisng data ‘pattern’ - The first column indicates the number of cases - The central columns indicate whether a particular variable is missing for the pattern (0=missing) - The last column counts the number of values missing for the pattern - The final row counts the number of missing values for each variable. Visualising missingness Graphics can also be useful to explore patterns in missingness. rct.data contains data from an RCT of functional imagery training (FIT) for weight loss, which measured outcome (weight in kg) at baseline and two followups (kg1, kg2, kg3). The trial also measured global quality of life (gqol). As is common, there were some missing data at the follouwp: fit.data &lt;- readRDS(&quot;data/fit-weight.RDS&quot;) %&gt;% select(kg1, kg2, kg3, age, gqol1) mice::md.pattern(fit.data) ## kg1 age gqol1 kg2 kg3 ## 112 1 1 1 1 1 0 ## 2 1 1 1 1 0 1 ## 7 1 1 1 0 0 2 ## 8 0 0 0 0 0 5 ## 8 8 8 15 17 56 We might be interested to explore patterns in which observations were missing. Here we use colour to identify missing observations as a function of the data recorded at baseline: fit.data %&gt;% mutate(missing.followup = is.na(kg2)) %&gt;% ggplot(aes(kg1, age, color=missing.followup)) + geom_point() There’s a clear trend here for lighter patients (at baseline) to have more missing data at followup. There’s also a suggestion that younger patients are more likely to have been lost to followup. If needed, we could perform inferential tests for these differences: t.test(kg1 ~ is.na(kg2), data=fit.data) ## ## Welch Two Sample t-test ## ## data: kg1 by is.na(kg2) ## t = 4.7153, df = 11.132, p-value = 0.000614 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 7.005116 19.236238 ## sample estimates: ## mean in group FALSE mean in group TRUE ## 90.59211 77.47143 t.test(age ~ is.na(kg2), data=fit.data) ## ## Welch Two Sample t-test ## ## data: age by is.na(kg2) ## t = 1.2418, df = 6.5246, p-value = 0.2571 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.39455 23.25169 ## sample estimates: ## mean in group FALSE mean in group TRUE ## 43.50000 35.57143 However, given the small number of missing values and the post-hoc nature of these analyses these tests are rather underpowered and we might prefer to report and comment on the plot alone. For some nice missing data visualisation techniques, including those for repeated measures data, see @zhang2015missing. "],
["tidyingdata.html", "Tidying data", " Tidying data ‘Tidying’ data means converting it into the format that is most useful for data analyses, and so we have already covered many of the key techniques: selecting and filtering data, reshaping and summarising. However the ideas behind ‘tidying’ draw together other related concepts which link together the way we enter, store and process data: for example the idea of ‘relational data’ and techniques to join together related datasets. A philosophy of tidy data The chapter on tidying in ‘R for data science’ is well worth reading for it’s thoughtful explanation of why we want tidy data, and the core techniques to clean up untidy data: http://r4ds.had.co.nz/tidy-data.html "],
["reshaping.html", "Reshaping", " Reshaping This section will probably require more attention than any other in the guide, but will likely be one of the most useful things you learn in R. As previously discussed, most things work best in R if you have data in long format. This means we prefer data that look like this: person time outcome 1 Time 1 18.14 2 Time 1 20.95 3 Time 1 20.51 1 Time 2 24.08 2 Time 2 18.58 3 Time 2 19.69 1 Time 3 20.31 2 Time 3 19.88 3 Time 3 20.57 1 Time 4 20.66 2 Time 4 18.74 3 Time 4 22.66 And NOT like this: person Time 1 Time 2 Time 3 Time 4 1 18.14 24.08 20.31 20.66 2 20.95 18.58 19.88 18.74 3 20.51 19.69 20.57 22.66 In long format data: each row of the dataframe corresponds to a single measurement occasion each column corresponds to a variable which is measured Fortunately it’s fairly easy to move between the two formats, provided your variables are named in a consistent way. Wide to long format This is the most common requirement. Often you will have several columns which actually measure the same thing, and you will need to convert these two two columns - a ‘key’, and a value. For example, let’s say we measure patients on 10 days: sleep.wide %&gt;% head(4) %&gt;% pander(caption=&quot;Data for the first 4 subjects&quot;) Data for the first 4 subjects Subject Day.0 Day.1 Day.2 Day.3 Day.4 Day.5 Day.6 Day.7 Day.8 Day.9 1 249.6 258.7 250.8 321.4 356.9 414.7 382.2 290.1 430.6 466.4 2 222.7 205.3 203 204.7 207.7 216 213.6 217.7 224.3 237.3 3 199.1 194.3 234.3 232.8 229.3 220.5 235.4 255.8 261 247.5 4 321.5 300.4 283.9 285.1 285.8 297.6 280.2 318.3 305.3 354 We want to convert RT measurements on each Day to a single variable, and create a new variable to keep track of what Day the measurement was taken: The melt() function in the reshape2:: package does this for us: library(reshape2) sleep.long &lt;- sleep.wide %&gt;% melt(id.var=&quot;Subject&quot;) %&gt;% arrange(Subject, variable) sleep.long %&gt;% head(12) %&gt;% pander Subject variable value 1 Day.0 249.6 1 Day.1 258.7 1 Day.2 250.8 1 Day.3 321.4 1 Day.4 356.9 1 Day.5 414.7 1 Day.6 382.2 1 Day.7 290.1 1 Day.8 430.6 1 Day.9 466.4 2 Day.0 222.7 2 Day.1 205.3 Here melt has created two new variable: variable, which keeps track of what was measured, and value which contains the score. This is the format we need when plotting graphs and running regression and Anova models. Long to wide format To continue the example from above, these are long form data we just made: sleep.long %&gt;% head(3) %&gt;% pander(caption=&quot;First 3 rows in the long format dataset&quot;) First 3 rows in the long format dataset Subject variable value 1 Day.0 249.6 1 Day.1 258.7 1 Day.2 250.8 We can convert these back to the original wide format using dcast, again in the reshape2 package. The name of the dcast function indicates we can ‘cast’ a dataframe (the d prefix). So here, casting means the opposite of ‘melting’. Using dcast is a little more fiddly than melt because we have to say how we want the data spread wide. In this example we could either have: Columns for each day, with rows for each subject Columns for each subject, with rows for each day Although it’s obvious to us which format we want, we have to be explicit for R to get it right. We do this using a formula, which we’ll see again in the regression section. Each formula has two sides, left and right, separated by the tilde (~) symbol. On the left hand side we say which variable we want to keep in rows. On the right hand side we say which variables to convert to columns. So, for example: # rows per subject, columns per day sleep.long %&gt;% dcast(Subject~variable) %&gt;% head(3) ## Subject Day.0 Day.1 Day.2 Day.3 Day.4 Day.5 Day.6 ## 1 1 249.5600 258.7047 250.8006 321.4398 356.8519 414.6901 382.2038 ## 2 2 222.7339 205.2658 202.9778 204.7070 207.7161 215.9618 213.6303 ## 3 3 199.0539 194.3322 234.3200 232.8416 229.3074 220.4579 235.4208 ## Day.7 Day.8 Day.9 ## 1 290.1486 430.5853 466.3535 ## 2 217.7272 224.2957 237.3142 ## 3 255.7511 261.0125 247.5153 To compare, we can convert so each Subject has a column by reversing the formula: # note we select only the first 7 Subjects to # keep the table to a manageable size sleep.long %&gt;% filter(Subject &lt; 8) %&gt;% dcast(variable~Subject) ## variable 1 2 3 4 5 6 7 ## 1 Day.0 249.5600 222.7339 199.0539 321.5426 287.6079 234.8606 283.8424 ## 2 Day.1 258.7047 205.2658 194.3322 300.4002 285.0000 242.8118 289.5550 ## 3 Day.2 250.8006 202.9778 234.3200 283.8565 301.8206 272.9613 276.7693 ## 4 Day.3 321.4398 204.7070 232.8416 285.1330 320.1153 309.7688 299.8097 ## 5 Day.4 356.8519 207.7161 229.3074 285.7973 316.2773 317.4629 297.1710 ## 6 Day.5 414.6901 215.9618 220.4579 297.5855 293.3187 309.9976 338.1665 ## 7 Day.6 382.2038 213.6303 235.4208 280.2396 290.0750 454.1619 332.0265 ## 8 Day.7 290.1486 217.7272 255.7511 318.2613 334.8177 346.8311 348.8399 ## 9 Day.8 430.5853 224.2957 261.0125 305.3495 293.7469 330.3003 333.3600 ## 10 Day.9 466.3535 237.3142 247.5153 354.0487 371.5811 253.8644 362.0428 One neat trick when casting is to use paste to give your columns nicer names. So for example: sleep.long %&gt;% filter(Subject &lt; 4) %&gt;% dcast(variable~paste0(&quot;Person.&quot;, Subject)) ## variable Person.1 Person.2 Person.3 ## 1 Day.0 249.5600 222.7339 199.0539 ## 2 Day.1 258.7047 205.2658 194.3322 ## 3 Day.2 250.8006 202.9778 234.3200 ## 4 Day.3 321.4398 204.7070 232.8416 ## 5 Day.4 356.8519 207.7161 229.3074 ## 6 Day.5 414.6901 215.9618 220.4579 ## 7 Day.6 382.2038 213.6303 235.4208 ## 8 Day.7 290.1486 217.7272 255.7511 ## 9 Day.8 430.5853 224.2957 261.0125 ## 10 Day.9 466.3535 237.3142 247.5153 Notice we used paste0 rather than paste to avoid spaces in variable names, which is allowed but can be a pain. See more on working with character strings in a later section. For a more detailed explanation and various other methods for reshaping data, see: http://r4ds.had.co.nz/tidy-data.html Which package should you use to reshape data? There are three main options: tidyr::, which comes as part of the tidyverse, using gather and spread() reshape2:: using melt() and dcast() data.table:: also using functions called melt() and dcast() (but which are slightly different from those in reshape2) This post walks through some of the differences: https://www.r-bloggers.com/how-to-reshape-data-in-r-tidyr-vs-reshape2/ but the short answer is whichever you find simplest and easiest to remember (for me that’s melt and dcast). ` Aggregating and reshaping at the same time One common trick when reshaping is to convert a datafile which has multiple rows and columns per person to one with only a single row per person. That is, we aggregae by using a summary (perhaps the mean) and reshape at the same time. Although useful this isn’t covered in this section, because it is combining two techniques: Reshaping (i.e. from long to wide or back) Aggregating or summarising (converting multiple rows to one) In the next section we cover summarising data, and introduce the ‘split-apply-combine’ method for summarising. Once you have a good grasp of this, you could check out the ‘fancy reshaping’ section which does provide examples of aggregating and reshaping simultaneously. title: ‘Summarising data’ "],
["summarising-data.html", "4 Summaries", " 4 Summaries Before you begin this section, make sure you have fully understood the section on datasets and dataframes, and in particular that you are happy using the %&gt;% symbol to describe a flow of data. The chapter outlines several different approaches to obtaining summary statistics, and covers: Useful ‘utility’ functions Creating tables Using dplyr as a generalisable tool to make any kind of summary In particular, we emphasise functions that return dataframes. If a function returns a dataframe (rather than just printing output to the screen) then we can continue to process and present these results in useful ways. "],
["quick-and-dirty.html", "“Quick and dirty”", " “Quick and dirty” (Using utility functions built into R) Frequency tables Let’s say we ask 4 year olds and 6 year olds whether they prefer lego or duplo. We can use the table() command to get a cross tabulation of these age categories and what the child prefers. We wrap table(...) in the with() function to tell it which dataframe to use: lego.table &lt;- with(lego.duplo.df, table(age, prefers)) lego.table ## prefers ## age duplo lego ## 4 years 38 20 ## 6 years 12 30 4.0.0.1 xtab table is a simple way of calculating frequencies, but you can also use the xtabs function to make more complex sumamries. xtabs uses a formula type syntax to describe the table (formulas for linear models are explained here). In the simplest case, we just write a tilde symbol (~) and the the names of the variables we want to tablulate, separated with + symbols: xtabs(~age+prefers, lego.duplo.df) ## prefers ## age duplo lego ## 4 years 38 20 ## 6 years 12 30 The order of the variables changes the orientation of the table: xtabs(~prefers+age, lego.duplo.df) ## age ## prefers 4 years 6 years ## duplo 38 12 ## lego 20 30 Tables like this are useful in their own right, but can also be passed to inferential tests, like Chi squred Summary statistics In this guide so far you might have noticed functions which provide summaries of an entire dataframe. For example: summary(angry.moods) ## Gender Sports Anger.Out Anger.In ## Min. :1.000 Min. :1.000 Min. : 9.00 Min. :10.00 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:13.00 1st Qu.:15.00 ## Median :2.000 Median :2.000 Median :16.00 Median :18.50 ## Mean :1.615 Mean :1.679 Mean :16.08 Mean :18.58 ## 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:18.00 3rd Qu.:22.00 ## Max. :2.000 Max. :2.000 Max. :27.00 Max. :31.00 ## Control.Out Control.In Anger.Expression ## Min. :14.00 Min. :11.00 Min. : 7.00 ## 1st Qu.:21.00 1st Qu.:18.25 1st Qu.:27.00 ## Median :24.00 Median :22.00 Median :36.00 ## Mean :23.69 Mean :21.96 Mean :37.00 ## 3rd Qu.:27.00 3rd Qu.:24.75 3rd Qu.:44.75 ## Max. :32.00 Max. :32.00 Max. :68.00 Or: psych::describe(angry.moods, skew=FALSE) ## vars n mean sd min max range se ## Gender 1 78 1.62 0.49 1 2 1 0.06 ## Sports 2 78 1.68 0.47 1 2 1 0.05 ## Anger.Out 3 78 16.08 4.22 9 27 18 0.48 ## Anger.In 4 78 18.58 4.70 10 31 21 0.53 ## Control.Out 5 78 23.69 4.69 14 32 18 0.53 ## Control.In 6 78 21.96 4.95 11 32 21 0.56 ## Anger.Expression 7 78 37.00 12.94 7 68 61 1.47 Although useful, these functions miss two important elements: We have to operate on the whole dataframe at once The output is just printed to screen. We might prefer to get back a dataframe so that we can process the results further. Creating a data frame of summary statistics Thanksfully, many summary functions allow us to pass their results to the as_data_frame() function, which converts the output into a table which we can use like any other dataset. In this example, we create summary statistics with the psych::describe() function, then convert to a dataframe and format as a table in RMarkdown: psych::describe(angry.moods, skew=FALSE) %&gt;% as_data_frame %&gt;% pander() vars n mean sd min max range se Gender 1 78 1.615 0.4897 1 2 1 0.05544 Sports 2 78 1.679 0.4697 1 2 1 0.05318 Anger.Out 3 78 16.08 4.217 9 27 18 0.4775 Anger.In 4 78 18.58 4.697 10 31 21 0.5319 Control.Out 5 78 23.69 4.688 14 32 18 0.5309 Control.In 6 78 21.96 4.945 11 32 21 0.5599 Anger.Expression 7 78 37 12.94 7 68 61 1.465 This summary table can be processed like any other dataframe. For instance, we can select columns and rows from it using dplyr: psych::describe(angry.moods, skew=FALSE) %&gt;% # rownames_to_column converts to a df but saves the # row names in a new column for us, which can be useful rownames_to_column(var=&quot;variable&quot;) %&gt;% select(variable, mean, sd) %&gt;% filter(mean &gt; 20) %&gt;% pander variable mean sd Control.Out 23.69 4.688 Control.In 21.96 4.945 Anger.Expression 37 12.94 We can do the same with the output of table or xtabs too: xtabs(~prefers+age, lego.duplo.df) %&gt;% as_data_frame %&gt;% pander(caption=&quot;Using `xtabs` to make a frequency table; converting to a dataframe for presentation using `pander`.&quot;) Using xtabs to make a frequency table; converting to a dataframe for presentation using pander. prefers age n duplo 4 years 38 lego 4 years 20 duplo 6 years 12 lego 6 years 30 Rownames are evil Historically ‘row names’ were used on R to label individual rows in a dataframe. It turned out that this is generally a bad idea, because sorting and some summary functions would get very confused and mix up row names and the data itself. It’s generally considered best practice to avoid row names, and store everything as columns of data. If you find that rownames in your data have disappeared, see this guide for turning them into an extra column of data using tibble::rownames_to_column(). Computing statistics by-groups psych::describeBy(mtcars, &#39;cyl&#39;) ## ## Descriptive statistics by group ## group: 4 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 11 26.66 4.51 26.00 26.44 6.52 21.40 33.90 12.50 0.26 ## cyl 2 11 4.00 0.00 4.00 4.00 0.00 4.00 4.00 0.00 NaN ## disp 3 11 105.14 26.87 108.00 104.30 43.00 71.10 146.70 75.60 0.12 ## hp 4 11 82.64 20.93 91.00 82.67 32.62 52.00 113.00 61.00 0.01 ## drat 5 11 4.07 0.37 4.08 4.02 0.34 3.69 4.93 1.24 1.00 ## wt 6 11 2.29 0.57 2.20 2.27 0.54 1.51 3.19 1.68 0.30 ## qsec 7 11 19.14 1.68 18.90 18.99 1.48 16.70 22.90 6.20 0.55 ## vs 8 11 0.91 0.30 1.00 1.00 0.00 0.00 1.00 1.00 -2.47 ## am 9 11 0.73 0.47 1.00 0.78 0.00 0.00 1.00 1.00 -0.88 ## gear 10 11 4.09 0.54 4.00 4.11 0.00 3.00 5.00 2.00 0.11 ## carb 11 11 1.55 0.52 2.00 1.56 0.00 1.00 2.00 1.00 -0.16 ## kurtosis se ## mpg -1.65 1.36 ## cyl NaN 0.00 ## disp -1.64 8.10 ## hp -1.71 6.31 ## drat 0.12 0.11 ## wt -1.36 0.17 ## qsec -0.02 0.51 ## vs 4.52 0.09 ## am -1.31 0.14 ## gear -0.01 0.16 ## carb -2.15 0.16 ## -------------------------------------------------------- ## group: 6 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 7 19.74 1.45 19.70 19.74 1.93 17.80 21.40 3.60 -0.16 ## cyl 2 7 6.00 0.00 6.00 6.00 0.00 6.00 6.00 0.00 NaN ## disp 3 7 183.31 41.56 167.60 183.31 11.27 145.00 258.00 113.00 0.80 ## hp 4 7 122.29 24.26 110.00 122.29 7.41 105.00 175.00 70.00 1.36 ## drat 5 7 3.59 0.48 3.90 3.59 0.03 2.76 3.92 1.16 -0.74 ## wt 6 7 3.12 0.36 3.21 3.12 0.36 2.62 3.46 0.84 -0.22 ## qsec 7 7 17.98 1.71 18.30 17.98 1.90 15.50 20.22 4.72 -0.12 ## vs 8 7 0.57 0.53 1.00 0.57 0.00 0.00 1.00 1.00 -0.23 ## am 9 7 0.43 0.53 0.00 0.43 0.00 0.00 1.00 1.00 0.23 ## gear 10 7 3.86 0.69 4.00 3.86 0.00 3.00 5.00 2.00 0.11 ## carb 11 7 3.43 1.81 4.00 3.43 0.00 1.00 6.00 5.00 -0.26 ## kurtosis se ## mpg -1.91 0.55 ## cyl NaN 0.00 ## disp -1.23 15.71 ## hp 0.25 9.17 ## drat -1.40 0.18 ## wt -1.98 0.13 ## qsec -1.75 0.65 ## vs -2.20 0.20 ## am -2.20 0.20 ## gear -1.24 0.26 ## carb -1.50 0.69 ## -------------------------------------------------------- ## group: 8 ## vars n mean sd median trimmed mad min max range skew ## mpg 1 14 15.10 2.56 15.20 15.15 1.56 10.40 19.20 8.80 -0.36 ## cyl 2 14 8.00 0.00 8.00 8.00 0.00 8.00 8.00 0.00 NaN ## disp 3 14 353.10 67.77 350.50 349.63 73.39 275.80 472.00 196.20 0.45 ## hp 4 14 209.21 50.98 192.50 203.67 44.48 150.00 335.00 185.00 0.91 ## drat 5 14 3.23 0.37 3.12 3.19 0.16 2.76 4.22 1.46 1.34 ## wt 6 14 4.00 0.76 3.75 3.95 0.41 3.17 5.42 2.25 0.99 ## qsec 7 14 16.77 1.20 17.18 16.86 0.79 14.50 18.00 3.50 -0.80 ## vs 8 14 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 NaN ## am 9 14 0.14 0.36 0.00 0.08 0.00 0.00 1.00 1.00 1.83 ## gear 10 14 3.29 0.73 3.00 3.17 0.00 3.00 5.00 2.00 1.83 ## carb 11 14 3.50 1.56 3.50 3.25 0.74 2.00 8.00 6.00 1.48 ## kurtosis se ## mpg -0.57 0.68 ## cyl NaN 0.00 ## disp -1.26 18.11 ## hp 0.09 13.62 ## drat 1.08 0.10 ## wt -0.71 0.20 ## qsec -0.92 0.32 ## vs NaN 0.00 ## am 1.45 0.10 ## gear 1.45 0.19 ## carb 2.24 0.42 This is helpful, but there’s no simple way to convert the result to a dataframe, which we will want if we are creating tables for publication. describeBy actually returns a list of tables, one for each level of the cyl variable, so it is is possible to convert each table in turn: summary.tables &lt;- psych::describeBy(mtcars, &#39;cyl&#39;) summary.tables[[1]] %&gt;% as_data_frame() %&gt;% head(3) ## # A tibble: 3 x 13 ## vars n mean sd median trimmed mad min max ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 11 26.66364 4.509828 26 26.44444 6.52344 21.4 33.9 ## 2 2 11 4.00000 0.000000 4 4.00000 0.00000 4.0 4.0 ## 3 3 11 105.13636 26.871594 108 104.30000 42.99540 71.1 146.7 ## # ... with 4 more variables: range &lt;dbl&gt;, skew &lt;dbl&gt;, kurtosis &lt;dbl&gt;, ## # se &lt;dbl&gt; But this is pretty yucky. Not only are the column names all mangled up, but we also have to think about extracting each levell in turn, and need to check how many levels in cyl there are. What happens if an extra level gets added? Our code will likely break. Thankfully there is much nicer and more consistent way to compute exactly the summaries we want, sometimes termed the ‘split, apply, combine’ method. "],
["a-generalised-approach.html", "A generalised approach", " A generalised approach The ‘split, apply, combine’ model The dplyr:: package, and especially the summarise() function provides a generalised way to create dataframes of frequencies and other summary statistics, grouped and sorted however we like. For example, let’s say we want the mean of some of our variables across the whole dataframe: angry.moods %&gt;% summarise( mean.anger.out=mean(Anger.Out), sd.anger.out=sd(Anger.Out) ) ## # A tibble: 1 x 2 ## mean.anger.out sd.anger.out ## &lt;dbl&gt; &lt;dbl&gt; ## 1 16.07692 4.21737 The summarise function has returned a dataframe containing the statistics we need, although in this instance the dataframe only has one row. What if we want the numbers for men and women separately? Utility functions like describeBy have options to do this (you would specify grouping variables in that). But there’s a more general pattern at work — we want to: Split our data (into men and women, or some other categorisation) Apply some function to them (e.g. calculate the mean) and then Combine it into a single table again (for more processing or analysis) It’s helpful to think of this split \\(\\rightarrow\\) apply \\(\\rightarrow\\) combine pattern whenever we are processing data because it makes explicit what we want to do. Split: breaking the data into groups The first task is to organise our dataframe into the relevant groups. To do this we use group_by(): angry.moods %&gt;% group_by(Gender) %&gt;% head ## # A tibble: 6 x 7 ## # Groups: Gender [2] ## Gender Sports Anger.Out Anger.In Control.Out Control.In Anger.Expression ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1 18 13 23 20 36 ## 2 2 1 14 17 25 24 30 ## 3 2 1 13 14 28 28 19 ## 4 2 1 17 24 23 23 43 ## 5 1 1 16 17 26 28 27 ## 6 1 1 16 22 25 23 38 Weirdly, this doesn’t seem to have done anything. The data aren’t sorted by Gender, and there is no visible sign of the grouping, but stick with it… Apply and combine Continuing the example above, once we have grouped our data we can then apply a function to it — for exmaple, summarise: angry.moods %&gt;% group_by(Gender) %&gt;% summarise( mean.anger.out=mean(Anger.Out) ) ## # A tibble: 2 x 2 ## Gender mean.anger.out ## &lt;int&gt; &lt;dbl&gt; ## 1 1 16.56667 ## 2 2 15.77083 And R and dplyr have done as we asked: split the data by Gender, using group_by() apply the summarise() function combine the results into a new data frame A ‘real’ example In the previous section on datasets, we saw some found some raw data from a study which had measured depression with the PHQ-9. Patients were measured on numerous occasions (month is recorded) and were split into treatment and control groups: phq9.df &lt;- readr::read_csv(&quot;phq.csv&quot;) ## Parsed with column specification: ## cols( ## patient = col_integer(), ## phq9_01 = col_integer(), ## phq9_02 = col_integer(), ## phq9_03 = col_integer(), ## phq9_04 = col_integer(), ## phq9_05 = col_integer(), ## phq9_06 = col_integer(), ## phq9_07 = col_integer(), ## phq9_08 = col_integer(), ## phq9_09 = col_integer(), ## month = col_integer(), ## group = col_integer() ## ) glimpse(phq9.df) ## Observations: 2,429 ## Variables: 12 ## $ patient &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ... ## $ phq9_01 &lt;int&gt; 3, 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 3, ... ## $ phq9_02 &lt;int&gt; 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_03 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, ... ## $ phq9_04 &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, ... ## $ phq9_05 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, ... ## $ phq9_06 &lt;int&gt; 3, 2, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_07 &lt;int&gt; 3, 3, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 1, 1, ... ## $ phq9_08 &lt;int&gt; 0, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, ... ## $ phq9_09 &lt;int&gt; 2, 2, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, ... ## $ month &lt;int&gt; 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 0, 1,... ## $ group &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ... If this were our data we might want to: Calculate the sum of the PHQ-9 variables (the PHQ-9 score) Calculate the average PHQ-9 score at each month, and in each group Show these means by group for months 0, 7 and 12 Using only the commands above9 we can write: phq9.summary.df &lt;- phq9.df %&gt;% mutate(phq9 = phq9_01 + phq9_02 + phq9_03 + phq9_04 + phq9_05 + phq9_06 + phq9_07 + phq9_08 + phq9_09) %&gt;% select(patient, group, month, phq9) %&gt;% # remove rows with missing values filter(!is.na(phq9)) %&gt;% # split group_by(month, group) %&gt;% # apply and combine summarise(phq.mean = mean(phq9)) phq9.summary.df %&gt;% filter(month %in% c(0, 7, 12)) %&gt;% pander::pandoc.table() ## ## -------------------------- ## month group phq.mean ## ------- ------- ---------- ## 0 0 19.76 ## ## 0 1 18.97 ## ## 7 0 16.62 ## ## 7 1 13.42 ## ## 12 0 16.15 ## ## 12 1 12.54 ## -------------------------- A ‘neater way’ You might have thought that typing out each variable in the above example (phq9_01 + phq9_02...) seemed a bit repetitive. In general, if you find yourself typing something repetitive in R then there will a better way of doing it, and this is true here. Stepping back, what we want is the row mean of all the variables starting with phq9_0. We can write this more concisely like so: phq9.df %&gt;% mutate(phq9 = rowMeans( select(phq9.df, starts_with(&quot;phq9_0&quot;)) ) ) ## # A tibble: 2,429 x 13 ## patient phq9_01 phq9_02 phq9_03 phq9_04 phq9_05 phq9_06 phq9_07 phq9_08 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 3 3 3 3 3 3 3 0 ## 2 2 1 2 3 3 3 2 3 2 ## 3 2 1 2 3 2 3 2 1 2 ## 4 2 2 2 3 3 3 2 1 1 ## 5 2 2 3 3 3 3 3 2 1 ## 6 2 2 3 3 3 3 3 1 1 ## 7 2 3 3 3 3 3 2 2 1 ## 8 2 2 2 3 3 3 1 2 2 ## 9 2 3 3 3 3 3 2 1 1 ## 10 2 3 3 3 3 3 3 3 3 ## # ... with 2,419 more rows, and 4 more variables: phq9_09 &lt;int&gt;, ## # month &lt;int&gt;, group &lt;int&gt;, phq9 &lt;dbl&gt; I’ve broken the code into multiple lines to make it clearer to read. In English, this code means: Take the the dataframe phq9.df Add (using mutate()) a new variable called phq by Calculating the rowmeans of selected columns in phq9.df which Start with the letters: `phq9_0 Ignore this if you are found the last section confusing, but if you find pipes useful then note that I explicitly passed the phq9.df directly to the select() function. There are other tricks with pipes where you can pass the intermediate result of a series of pipes to a function by putting a . (a full stop or period) as the argument to the function. This can be very useful, so see the package documentation for details. Fancy reshaping As noted above, it’s common to combine the process of reshaping and aggregating or summarising in the same step. For example here we have multiple rows per person, 3 trial at time 1, and 3 more trials at time 2: expt.data %&gt;% arrange(person, time, trial) %&gt;% head %&gt;% pander Condition trial time person RT 1 1 1 1 284.5 1 2 1 1 309.3 1 3 1 1 346.7 1 1 2 1 368 1 2 2 1 263.7 1 3 2 1 220.4 We can reshape and aggregate this in a single step using dcast. Here we request the mean for each person at each time, with observations for each time split across two columns: library(reshape2) expt.data %&gt;% dcast(person~paste0(&#39;time&#39;,time), fun.aggregate=mean) %&gt;% head %&gt;% pander ## Using RT as value column: use value.var to override. person time1 time2 1 313.5 284 2 252.2 263.3 3 263.1 290.4 4 271.2 249.4 5 274.3 329.9 6 280.3 231.7 Here dcast has correctly guessed that RT is the value we want to aggregate (you can specify explicitly with the value.var= parameter). dcast knows to aggregate using the mean because we set this with the agg.function parameter; this just stands for ‘aggregation function’. We don’t have to request the mean though: any function will do. Here we request the SD instead: expt.data %&gt;% dcast(person~time, fun.aggregate=sd) %&gt;% head %&gt;% pander ## Using RT as value column: use value.var to override. person 1 2 1 31.27 75.85 2 15.07 33.98 3 88.71 104 4 40.42 3.913 5 61.34 52.7 6 35.82 17.3 You might have noticed I sneaked something new in here: the call to pander(). This is from the pander:: package, which contains is a useful set of functions function for when writing RMarkdown documents. They convert many R objects into more readable output: here it makes a nice table for us in the compiled document. We cover more tips and tricks for formatting RMarkdown documents in the chapter on sharing and publishing your data. You might also want to check this page on missing values to explain the filter which uses !is.na(), but you could equally leave this for later.↩ "],
["graphics.html", "5 Graphics", " 5 Graphics Graphics are the best thing about R. The base system alone provides lots of useful plotting functions, but the ggplot2 package is exceptional in the consistent and powerful approach it takes to visualising data. This chapter focusses mostly on ggplot, but does include some pointers to other useful plotting functions. It’s also worth pointing out here that the O’Reilly R Graphics cookbook is available as a pdf download and is a much more comprehensive source than this page. The examples below are more selective and show plots likely to be of particular use in reporting your studies. The emphaisis on showing you how to make good plots that help you explore data and communicate your findings, rather than simply reproduce the output of SPSS or Excel. "],
["graphics-benefits.html", "Benefits of visualising data", " Benefits of visualising data Scientists attempt to understanding natural processes, predict events in the observable world, and communicate this understanding to others. In each case, graphics are a powerful tool in their armoury. The doctrine of null hypothesis testing which has infected psychology and many other fields has made some researchers nervous about the power of graphics to explore and find patterns in data: perhaps concerned with the accusation of undertaking a ‘fishing trip’ or capitalising on chance in ther analyses, too many scientists have eschewed graphical statistics and focussed instead on point estimates and p values from complex statistical models. But as the complexity of theories group, the role of graphics becomes ever more important because the right graphical presentations of our data are in many cases the only reliable way of checking the appropriateness of our models, and of communicating our findings efficiently. David McCandless recently spoke at a TedX event about his work on data journalism, and makes a persuasive case for paying more attention to visualisations: For a fascinating history and exploration of the good, the bad and the ugly in data visualisations you should also (at least) skim Edward Tufte’s book [@edward2001visual]. "],
["graphics-approaches.html", "Which tool to use?", " Which tool to use? Typically when setting out to plot data in R it pays to ask yourself whether you need: A quick way to visualise something specific – for example to check some feature of your data before you continue your analysis – without worrying too much about the detail of the graphic. A plot that is specifically designed to communicate your data effectively, and where you do care about the details of the final output. For the first case, where you already know what you want — for example to visualise a distribution of a single variable or to check diagnostics from a linear model — there are many useful built-in functions in base-R. For the second case — for example where you want to visualise the main outcomes of your study, or draw attention to specific aspects of your data — there is ggplot2. We’ll deal with the second case first, because using ggplot highlights many important aspects of plotting in general. "],
["layered-graphics.html", "Layered graphics with ggplot", " Layered graphics with ggplot If you’ve never given much thought to data visualisation before, you might be surprised at the sheer variety of graphs types available. One way to cut through the multitude of options is to determine what the purpose of your plot is. Although not a complete list, it’s likely your plot will show at least one of: Relationships Distributions Comparison Composition The ggplot library makes it easy to produce high quality graphics which serve these ends, and to layer them to produce information-dense plots which are really effective forms of communication. Figure 5.1: Examples of charts showing comaprisons, relationships, distribution and composition. The comparison, distribution and composition plots show 2 variables, but the relationship plot includes 3, increasing the density of the information displayed. A thought on ‘chart chooser’ guides There are various simple chart selection guides available online, of which these are quite nice examples: Chart selection guide (pdf)] ‘Show me the numbers’ chart guide (pdf) However, guides which attempt to be comprehensive and show you a full range of plot types are perhaps not as useful as those which reflect our knowledge of which plots are the most effective forms of communication. For example, almost all guides to plotting, and especially R textbooks, will show you how to plot a simple bar graph. But bar graphs have numerous disadvantages over other plots which can show the same information. Specifically, they: are low in information density (and so inefficient in use of space) make comparisons between multiple data series very difficult (for example in interaction plots), and perhaps most importantly, even when they include error bars, readers consistently misinterpret the quantitative information in bar graphs (specifically, when bar graphs are used to display estimates which contain error, readers assume points above the bar are less likely than points within the bar, even though this is typically not the case). You should be guided in choosing plots not by mechanical rules based on the number or type of variables you want to display. Instead, you should be guided by the evidence from basic studies of human perception, and applied data on how different types of infromation displays are really used by readers. This guide is restricted to examples likely to be useful and effective for experiemental and applied psychologists. Thinking like ggplot When using ggplot it helps to think of five separate steps to making a plot (2 are optional, but commonly used): Choose the data you want to plot. Map variables to axes or other features of the plot (e.g. sizes or colours). (Optionally) use ggplot functions to summarise your data before the plot is drawn (e.g. to calulate means and standard errors for point-range plots). Add visual display layers. (Optionally) Split the plot up across multiple panels using groupings in the data. You can then customise the plot labels and title, and tweak other presentation parameters, although this often isn’t necessary unless sending a graphic for publication. You can also export graphics in multiple high quality formats. The simplest way to demonstrate these steps is with an example, and we begin with a plot showing the relationship betwen variables: ‘Relationships’ Problem to be solved: You want to check/show whether variables are related in a linear fashion, e.g. before running linear regression Step 1: Select data to plot Step 1 is to select our data. As is typical in R, ggplot works best with long-format data. In the examples below we will use the mtcars dataset for convenience, so our first line of code is to use the dplyr pipe symbol (operator) to send the mtcars dataset to the next line of code: mtcars %&gt;% ... Step 2: Map variables to axes, colours, and other features Step 2 is to map the variables we want to axes or other features of the plot (e.g. the colours of points, or the linetypes used in line plots). The specify these mappings we use the aes() function, which is slightly cryptic, but short for ‘aesthetics mapping’. Depending on the plot type you will specify different aesthetics, and they can also have different effects depending on the plot type, but you will commonly specify: x the variable to use as the x axis y the variable to use as the y axis colour: the variable to use to colour points or lines Here we tell ggplot to use disp (engine size) on the x axis, and mpg on the y axis. We also tell it to colour the points differently depending on the value of hp (engine horsepower). At this point ggplot will create and label the axes and plot area, but doesn’t yet display any of our data. For this we need to add visual display layers (in the next step). mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) Other aesthetics There are many other aesthetics which can be specified. Some of the most useful are: ymin and ymax: for upper and lower bounds, e.g. on error bars group:which tells ggplot to group observations by some variable and, for example, plot a different line per-group) fill: like colour but for areas/shapes alpha and size: control the size and opacity of visual features (useful for de-emphasising some features of a plot to make others stand out) See the ggplot documentation for more details: http://ggplot2.tidyverse.org/reference/#section-aesthetics Step 3 We skip step 3 for this example (asking ggplot to automatically make summaries of our data before plotting), but will cover it below - see the stat_summary() function. Step 4: Display data To display data, we have to add a visual layer to the plot. For example, let’s say we want to make a scatter plot, and so draw points for each row of data: mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point() And we have a pretty slick graph: ggplot has now added points for each pair of disp and mpg values, and coloured them according to the value of hp (see choosing colours below XXX). Use the airquality dataset and create your own scatterplot and try to colour the points using the Month variable. Should Month be used as a factor or a numeric variable when colouring the points? What’s even neater about ggplot though is how easy it is to layer different visualisations of the same data. These visual layers are called geom’s and the functions which add them are all prefixed with geom_, so geom_point() for scatter plots, or geom_line() for line plots, or geom_smooth() for a smoothed line plot. We can add this to the scatter plot like so: mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point(size=2) + geom_smooth(se=F, colour=&quot;grey&quot;) In the example above, I have also customised the smoothed line, making it grey to avoid over-intrusion into our perception of the points. Often less is more when plotting graphs: not everything can be emphasised at once, and it’s important to make decisions about what should be given visual priority. Step 5: ‘Splitting up’ or repeating the plot. Very often, you will have drawn plot and think things like I wonder what that would look like if I drew it for men and women separately?. In ggplot this is called facetting, and is easy to achieve, provided your data are in a long format. Using the same mtcars example, let’s say we wanted separate panels for American vs. Foreign cars (information held in the am variable). We simply add the facet_wrap(), and specify the &quot;am&quot; variable: mtcars %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point(size=2) + geom_smooth(se=F, colour=&quot;grey&quot;) + facet_wrap(&quot;am&quot;) One trick is to make sure factors are labelled nicely, because these labels appear on the final plot. Here the mutate() call relabels the factor which makes the plot easier to read: mtcars %&gt;% mutate(american = factor(am, labels=c(&quot;American&quot;, &quot;Foreign&quot;))) %&gt;% ggplot(aes(x = disp, y = mpg, colour=hp)) + geom_point(size=2) + geom_smooth(se=F, colour=&quot;grey&quot;) + facet_wrap(&quot;american&quot;) See the ggplot documentation on facetting for more details. ‘Distributions’ lme4::sleepstudy %&gt;% ggplot(aes(Reaction)) + geom_density() Imagine we wanted to compare distributions for individuals. Simply overlaying the lines is confusing: lme4::sleepstudy %&gt;% ggplot(aes(Reaction, group=Subject)) + geom_density() Facetting produces a nicer result: lme4::sleepstudy %&gt;% ggplot(aes(Reaction)) + geom_density() + facet_wrap(&quot;Subject&quot;) But we could present the same information more compactly, and with better facility to compare between subjects, if we use a bottleplot: lme4::sleepstudy %&gt;% ggplot(aes(Subject, Reaction)) + geom_violin() We might want to plot our Subjects in order of their mean RT: mean.ranked.sleep &lt;- lme4::sleepstudy %&gt;% group_by(Subject) %&gt;% # calculate mean RT mutate(RTm = mean(Reaction)) %&gt;% # sort by mean RT arrange(RTm, Days) %&gt;% ungroup() %&gt;% # create a rank score but conert to factor right away mutate(SubjectRank = factor(dense_rank(RTm))) mean.ranked.sleep %&gt;% ggplot(aes(SubjectRank, Reaction)) + geom_violin() + theme(aspect.ratio = .33) # change the aspect ratio to make long and wide Or we might want to compare individuals against the combined distribution: # duplicate all the data, assigning one-replication to a single subject, &quot;All&quot; sleep.repeat &lt;- bind_rows(lme4::sleepstudy, lme4::sleepstudy %&gt;% mutate(Subject=&quot;All&quot;)) ## Warning in bind_rows_(x, .id): binding factor and character vector, ## coercing into character vector ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector sleep.repeat %&gt;% mutate(all = Subject==&quot;All&quot;) %&gt;% ggplot(aes(Subject, Reaction, color=all)) + geom_violin() + guides(colour=FALSE) + # turn off he legend because we don&#39;t really need it theme(aspect.ratio = .25) # change the aspect ratio to make long and wide Boxplots can also work well to show distributions, and have the advantage of showing the median explicitly: mean.ranked.sleep %&gt;% ggplot(aes(SubjectRank, Reaction)) + geom_boxplot() If we plot the same data by-day, we can clearly see the effect of sleep deprivation, and the increase in variability between subjects as sime goes on: the lack of sleeps seems to be affecting some subjects more than others lme4::sleepstudy %&gt;% ggplot(aes(factor(Days), Reaction)) + geom_boxplot() ‘Comparisons’ Imagine we have rainfall and temperature data for various regions, across months: DAAG::bomregions %&gt;% psych::describe(fast=T) %&gt;% pander vars n mean sd min max range se Year 1 109 1954 31.61 1900 2008 108 3.028 eastAVt 2 99 20.43 0.4585 19.35 21.61 2.255 0.04608 seAVt 3 99 14.59 0.4547 13.62 15.94 2.32 0.0457 southAVt 4 99 18.48 0.4584 17.43 19.53 2.1 0.04607 swAVt 5 99 16.18 0.4734 15.08 17.05 1.97 0.04758 westAVt 6 99 22.33 0.4548 21.22 23.39 2.165 0.04571 northAVt 7 99 24.6 0.5042 23.57 25.94 2.365 0.05067 mdbAVt 8 99 17.59 0.4958 16.36 18.79 2.425 0.04983 auAVt 9 99 21.71 0.4429 20.67 22.87 2.195 0.04452 eastRain 10 109 601.6 123.8 315.3 1030 715 11.85 seRain 11 109 598.1 104.6 354.9 900.6 545.7 10.02 southRain 12 109 381.7 68.63 236 618.2 382.1 6.574 swRain 13 109 657.8 103 420.5 988.8 568.4 9.861 westRain 14 109 352.2 84.55 173.5 646.5 473 8.098 northRain 15 109 520.7 110.2 312.8 946.9 634 10.55 mdbRain 16 109 476 110.9 255.8 821 565.2 10.62 auRain 17 109 457.1 82.55 317.2 785.3 468.1 7.906 SOI 18 109 -0.002676 6.845 -20.01 20.79 40.8 0.6556 co2mlo 19 50 345.6 21.01 316 385.4 69.47 2.972 co2law 20 79 310.4 9.586 295.8 333.7 37.9 1.078 CO2 21 109 324.3 24.59 296.3 385.4 89.19 2.355 sunspot 22 109 60.08 47.69 1.4 190.2 188.8 4.568 Because these data are in wide format (multiple columns have contain the same type of data) we need to first convert to long format. This process has become known as tidying. The code below selects the columns related to rainfall and temperature and then ‘melts’ the data to long format: one row per observation. We then extract the region and the type of measurement from the variable column which is created by using a regular expression: weather.data.long &lt;- DAAG::bomregions %&gt;% select(Year, ends_with(&#39;Rain&#39;), ends_with(&#39;AVt&#39;)) %&gt;% reshape2::melt(id.var=&quot;Year&quot;) %&gt;% extract(variable, into=c(&quot;Region&quot;, &quot;variable&quot;), regex=&quot;(east|north|se|south|west|sw|au)(\\\\w+)&quot;) %&gt;% filter(!is.na(variable)) weather.data.long %&gt;% head ## Year Region variable value ## 1 1900 east Rain 429.98 ## 2 1901 east Rain 500.12 ## 3 1902 east Rain 315.33 ## 4 1903 east Rain 694.09 ## 5 1904 east Rain 564.86 ## 6 1905 east Rain 443.11 5.0.0.0.1 Try stepping through the code above line by line and see what is produced at each step. With our data in long form we can use ggplot to plot our data over time. In the plot below, we use filter() to select only the rainfall measurements: rain &lt;- weather.data.long %&gt;% filter(variable==&#39;Rain&#39;) rain %&gt;% ggplot(aes(Year, value)) + geom_point() + ylab(&#39;Rainfall (mm)&#39;) There are many ways to extract structure from these data, and make comparisons over time. One is to use colour: rain %&gt;% ggplot(aes(Year, value, color=Region)) + geom_point() + ylab(&#39;Rainfall (mm)&#39;) It’s now easy to see the crude differences between the regions (west appears the driest, sw the wettest), but it’s still hard to compare between regions, or over time. For this we can use various forms of summaries, for example a smoothed line plot (the shaded area is the standard error), which clearly shows the relative changes in rainfall in each region over time: rain %&gt;% ggplot(aes(Year, value, color=Region)) + geom_smooth() + ylab(&#39;Rainfall (mm)&#39;) It can sometimes be desireable to include points to preserve the relationship between the summary and the raw data: rain %&gt;% filter(Region %in% c(&#39;west&#39;, &#39;se&#39;)) %&gt;% ggplot(aes(Year, value, color=Region)) + geom_point(alpha=.5, size=.5) + geom_smooth(se=F) + ylab(&#39;Rainfall (mm)&#39;) If we weren’t interested in the time series and just wanted to focus on the most recent year, we might take a different approach. Here a bar plot is used to compare between regions, with the national average (au) highlighted in blue: rain %&gt;% filter(Year == max(Year)) %&gt;% ggplot(aes(Region, value, fill=Region==&quot;au&quot;)) + stat_summary(geom=&quot;bar&quot;) + ylab(&#39;Rainfall (mm)&#39;) + guides(fill=F) Finally, we shouldn’t forget that this dataset included both rainfall and temperature data, and we can display these in different ways, depending on what our research question was. In this case we can use facetting to display temperature and rainfall data side by side. Because AVt and Rain are on such different scales we need to allow the y axis to vary between variables: weather.data.long %&gt;% ggplot(aes(Year, value, color=Region)) + geom_smooth() + facet_wrap(~variable, scales=&quot;free&quot;) ## Warning: Removed 70 rows containing non-finite values (stat_smooth). ‘Composition’ Waffle plots or ‘pictograms’ Waffle plots are neat way of showing the relative frequency of different categories. In applied settings it’s well known that when considering the risks or benefits of interventions clinicans, patients and researchers benefit from statements made using ‘natural frequencies’ [@gigerenzer2003simple], and pictographs or ‘waffle plots’ have been shown to provide patients and their families with a better understanding of the risks of treatments [@tait2010presenting]. Waffle plots can be implemented in R via the waffle:: package: outcomes &lt;- c(&quot;gained weight&quot;=13, &quot;no change&quot;=27, &quot;lost 2kg or more&quot; = 44, &quot;lost 5kg or more&quot; = 16) waffle::waffle(outcomes) Calculating these summary figures is left as an exercise to for the reader, but see the section on summarising data with dplyr. This is one of those occasions where the default ggplot colours could probably be improved, and we can do this by passing the names of the colours we would like to use: weight.loss.colours &lt;- c(&#39;firebrick2&#39;, &#39;darkgoldenrod1&#39;, &#39;palegreen3&#39;, &#39;palegreen4&#39;) waffle::waffle(outcomes, colors = weight.loss.colours) Selecting colours by hand isn’t always the best way though: the colourbrewer library provides some nice shortcuts for using palletes from the excellent ColourBrewer website. Stacked bars The reshape2 package includes data on tipping habits in restaurants for male and female bill-payers, and where the party was fo various sizes: reshape2::tips %&gt;% head %&gt;% pander total_bill tip sex smoker day time size 16.99 1.01 Female No Sun Dinner 2 10.34 1.66 Male No Sun Dinner 3 21.01 3.5 Male No Sun Dinner 3 23.68 3.31 Male No Sun Dinner 2 24.59 3.61 Female No Sun Dinner 4 25.29 4.71 Male No Sun Dinner 4 To begin, we might like to plot tips by time of day: reshape2::tips %&gt;% ggplot(aes(time, tip)) + stat_summary(geom=&quot;bar&quot;) And to facilitate the comparison between men and women we could colour portions of the bars using position_stack(): reshape2::tips %&gt;% ggplot(aes(time, tip, fill=sex)) + stat_summary(geom=&quot;bar&quot;, position=position_stack()) + xlab(&quot;&quot;) + ylab(&quot;Tip ($)&quot;) Or to reverse the comparisons: reshape2::tips %&gt;% ggplot(aes(sex, tip, fill=time)) + stat_summary(geom=&quot;bar&quot;, position=position_stack()) + xlab(&quot;&quot;) + ylab(&quot;Tip ($)&quot;) "],
["utility-plotting-functions.html", "‘Quick and dirty’ (utility) plots", " ‘Quick and dirty’ (utility) plots When exploring a dataset, often useful to use built in functions or helpers from other libraries. These help you quickly visualise relationships, but aren’t always exactly what you need and can be hard to customise. 5.0.1 Distributions hist(mtcars$mpg) plot(density(mtcars$mpg)) boxplot(mpg~cyl, data=mtcars) Hmisc::hist.data.frame(mtcars) Even for simple plots, ggplot has some useful helper functions though: qplot(mpg, data=mtcars, geom=&quot;density&quot;) + xlab(&quot;Miles per gallon&quot;) qplot(x=factor(cyl), y=mpg, data=mtcars, geom=&quot;boxplot&quot;) 5.0.2 Relationships with(mtcars, plot(mpg, wt)) pairs(select(mtcars, wt, disp, mpg)) Again, for quick plots ggplot also has useful shortcut functions: qplot(mpg, wt, color=factor(cyl), data = mtcars) 5.0.3 Quantities I don’t think the base R plots are that convenient here. ggplot2:: and the stat_summary() function makes life much simpler: ggplot(mtcars, aes(factor(cyl), mpg)) + stat_summary(geom=&quot;bar&quot;) And if you are plotting quantities, as disussed above, showing a range is sensible (a boxplot would also fill both definitions): ggplot(mtcars, aes(factor(cyl), mpg)) + stat_summary(geom=&quot;pointrange&quot;) "],
["ggplot-details.html", "Tricks with ggplot", " Tricks with ggplot More ways to facet a plot Facets are ways to repeat a plot for each level of another variable. ggplot has two ways of defining and displaying facets: As a list of plots, using facet_wrap. As a grid or matrix of plots, using facet_grid(). Examples of both are shown below, using the following plot as a starting point: base.plot &lt;- ggplot(mtcars, aes(mpg, wt)) + geom_point() base.plot facet_wrap If we want one facet we just type the tilde (~) symbol and then the name of the variable. This is like typing the right hand side of a formula for a regression model: base.plot + facet_wrap(~cyl) If we want two facets we extend the formula, using the + sign: base.plot + facet_wrap(~cyl+am) Note, the order of variables in the formula makes a difference: base.plot + facet_wrap(~am+cyl) facet_grid With one variable facet_grid produces similar output. Note the . (period) on the left hand side of the formula now to make explicit we only have one variable, and we want it on the x axis: base.plot + facet_grid(.~cyl) We can flip the facets around by putting the cyl variable on the left hand side of the ~: base.plot + facet_grid(cyl~.) And facet_grid can also create facets for two or more variables: base.plot + facet_grid(am~cyl) Here the labelling and the arrangement of plots is perhaps nicer because it is clearer that plots for cyl are arrange left to right, and for am they are top to bottom. Combining separate plots in a grid Note that combining separate plots in a grid is different from facetting, and it may be you want that instead. If you really want to combine several plots, the gridExtra and cowplot packages can be helpful. This is the code from the example in the graphics section, which may be a useful starting point: comparison &lt;- ggplot(mtcars, aes(factor(cyl), mpg)) + geom_boxplot() + ggtitle(&quot;Comparison&quot;) relationships &lt;- ggplot(mtcars, aes(wt, mpg, color=factor(gear))) + geom_point() + ggtitle(&quot;Relationship&quot;) distributions &lt;- ggplot(mtcars, aes(mpg, color=factor(gear))) + geom_density() + ggtitle(&quot;Distribution&quot;) composition &lt;- ggplot(mtcars, aes(factor(cyl), fill = factor(gear))) + geom_bar() + ggtitle(&quot;Composition&quot;) mm &lt;- theme(plot.margin=unit(rep(1.5,4), &quot;line&quot;)) gridExtra::grid.arrange(relationships+mm, distributions+mm, comparison+mm, composition+mm, ncol=2) "],
["exporting-graphics.html", "Exporting for print", " Exporting for print To export ggplot graphics you can use the ggsave() function: ggplot(mtcars, aes(wt, mpg)) + geom_point() ggsave(filename = &quot;myplot.pdf&quot;) See the ggplot docs on exporting or page 323 of the R Graphics Cookbook for lots more detail. "],
["common-inferential-stats.html", "6 Common inferential statistics", " 6 Common inferential statistics R has simple functions for common inferential statistics like Chi2, t-tests, correlations and many more. This section is by no means exhaustive, but covers statistics for crosstabulations, differences in means, and linear correlation. For more on non-parametric statistics this page on the statmethods site is a useful guide. The coin:: package implements many resampling tests, which can also be useful when assumptions of parametric tests are not valid. See this intro to resampling statistics. "],
["crosstabs.html", "Crosstabulations and \\(\\chi^2\\)", " Crosstabulations and \\(\\chi^2\\) We saw in a previous section how to create a frequency table of one or more variables. Using that previous example, assume we already have a crosstabulation of age and prefers lego.table ## prefers ## age duplo lego ## 4 years 38 20 ## 6 years 12 30 We can easily run the inferential \\(\\chi^2\\) (sometimes spelled “chi”, but pronounced “kai”-squared) test on this table: lego.test &lt;- chisq.test(lego.table) lego.test ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: lego.table ## X-squared = 11.864, df = 1, p-value = 0.0005724 Note that we can access each number in this output individually because the chisq.test function returns a list. We do this by using the $ syntax: # access the chi2 value alone lego.test$statistic ## X-squared ## 11.86371 Even nicer, you can use an R package to write up your results for you in APA format! library(apa) apa(lego.test, print_n=T) ## [1] &quot;$\\\\chi^2$(1, n = 100) = 11.86, *p* &lt; .001&quot; See more on automatically displaying statistics in APA format Three-way tables You can also use table() or xtabs() to get 3-way tables of frequencies (xtabs is probably better for this than table). For example, using the mtcars dataset we create a 3-way table, and then convert the result to a dataframe. This means we can print the table nicely in RMarkdown using the pander.table() function, or process it further (e.g. by sorting or reshaping it). xtabs(~am+gear+cyl, mtcars) %&gt;% as_data_frame() %&gt;% pander() am gear cyl n 0 3 4 1 1 3 4 0 0 4 4 2 1 4 4 6 0 5 4 0 1 5 4 2 0 3 6 2 1 3 6 0 0 4 6 2 1 4 6 2 0 5 6 0 1 5 6 1 0 3 8 12 1 3 8 0 0 4 8 0 1 4 8 0 0 5 8 0 1 5 8 2 Often, you will want to present a table in a wider format than this, to aid comparisons between categories. For example, we might want our table to make it easy to compare between US and non-US cars for each different number of cylinders: xtabs(~am+gear+cyl, mtcars) %&gt;% as_data_frame() %&gt;% reshape2::dcast(am+gear~paste(cyl, &quot;Cylinders&quot;)) %&gt;% pander() ## Using n as value column: use value.var to override. am gear 4 Cylinders 6 Cylinders 8 Cylinders 0 3 1 2 12 0 4 2 2 0 0 5 0 0 0 1 3 0 0 0 1 4 6 2 0 1 5 2 1 2 Or our primary question might be related to the effect of am, in which case we might prefer to incude separate columns for US and non-US cars: xtabs(~am+gear+cyl, mtcars) %&gt;% as_data_frame() %&gt;% reshape2::dcast(gear+cyl~paste0(&quot;US=&quot;, am)) %&gt;% pander() ## Using n as value column: use value.var to override. gear cyl US=0 US=1 3 4 1 0 3 6 2 0 3 8 12 0 4 4 2 6 4 6 2 2 4 8 0 0 5 4 0 2 5 6 0 1 5 8 0 2 "],
["correlations.html", "Correlations", " Correlations The base R cor() function provides a simple way to get Pearson correlations, but to get a correlation matrix as you might expect from SPSS or Stata it’s best to use the corr.test() function in the psych package. Before you start though, plotting the correlations might be the best way of getting to grips with the patterns of relationship in your data. A pairs plot is a nice way of doing this: airquality %&gt;% select(-Month, -Day) %&gt;% pairs If we were satisfied the relationships were (reasonably) linear, we could also visualise correlations themselves with a ‘corrgram’, using the corrgram library: library(&quot;corrgram&quot;) airquality %&gt;% select(-Month, -Day) %&gt;% corrgram(lower.panel=corrgram::panel.ellipse, upper.panel=panel.cor, diag.panel=panel.density) Figure 5.1: A corrgram, showing pearson correlations (above the diagonal), variable distributions (on the diagonal) and ellipses and smoothed lines of best fit (below the diagnonal). Long, narrow ellipses denote large correlations; circular ellipses indicate small correlations. The ggpairs function from the GGally package is also a nice way of plotting relationships between a combination of categorical and continuous data - it packs a lot of information into a limited space: mtcars %&gt;% mutate(cyl = factor(cyl)) %&gt;% select(mpg, wt, drat, cyl) %&gt;% GGally::ggpairs() Creating a correlation matrix The psych::corr.test() function is a quick way to obtain a pairwise correlation matrix for an entire dataset, along with p values and confidence intervals which the base R cor() function will not provide: mycorrelations &lt;- psych::corr.test(airquality) mycorrelations ## Call:psych::corr.test(x = airquality) ## Correlation matrix ## Ozone Solar.R Wind Temp Month Day ## Ozone 1.00 0.35 -0.60 0.70 0.16 -0.01 ## Solar.R 0.35 1.00 -0.06 0.28 -0.08 -0.15 ## Wind -0.60 -0.06 1.00 -0.46 -0.18 0.03 ## Temp 0.70 0.28 -0.46 1.00 0.42 -0.13 ## Month 0.16 -0.08 -0.18 0.42 1.00 -0.01 ## Day -0.01 -0.15 0.03 -0.13 -0.01 1.00 ## Sample Size ## Ozone Solar.R Wind Temp Month Day ## Ozone 116 111 116 116 116 116 ## Solar.R 111 146 146 146 146 146 ## Wind 116 146 153 153 153 153 ## Temp 116 146 153 153 153 153 ## Month 116 146 153 153 153 153 ## Day 116 146 153 153 153 153 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Ozone Solar.R Wind Temp Month Day ## Ozone 0.00 0.00 0.00 0.00 0.56 1.00 ## Solar.R 0.00 0.00 1.00 0.01 1.00 0.56 ## Wind 0.00 0.50 0.00 0.00 0.25 1.00 ## Temp 0.00 0.00 0.00 0.00 0.00 0.65 ## Month 0.08 0.37 0.03 0.00 0.00 1.00 ## Day 0.89 0.07 0.74 0.11 0.92 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option One thing to be aware of is that by default corr.test() produces p values that are adjusted for multiple comparisons in the top right hand triangle (i.e. above the diagonal). If you want the uncorrected values use the values below the diagonal (or pass adjust=FALSE when calling the function). Working with correlation matrices It’s important to realise that, as with all R objects, we can work with correlation matrices to continue our data ananalyses. For example, as part of exploring your data, you might want to know whether correlations you observe in one sample are similar to those from another sample, when using the same questions. For example, let’s say we ran a survey measuring variables from the theory of planned behaviour first in students, and later in older adults: We could run correlations for each sample separately: corr.students &lt;- cor(students) corr.public &lt;- cor(public) And we could ‘eyeball’ both of these correlation matrices and try and spot patterns or differences between them, but this is quite hard: corr.students %&gt;% pander() behaviour intention control social.norm attitude behaviour 1.000 0.55 0.648 0.072 0.132 intention 0.551 1.00 0.394 0.298 0.438 control 0.648 0.39 1.000 0.017 0.014 social.norm 0.072 0.30 0.017 1.000 -0.011 attitude 0.132 0.44 0.014 -0.011 1.000 corr.public %&gt;% pander behaviour intention social.norm attitude control behaviour 1.00 0.54 0.282 0.231 0.141 intention 0.54 1.00 0.356 0.376 0.290 social.norm 0.28 0.36 1.000 -0.081 0.010 attitude 0.23 0.38 -0.081 1.000 0.049 control 0.14 0.29 0.010 0.049 1.000 But we could also simply subtract one matrix from the other to show the difference directly: (corr.students - corr.public) %&gt;% pander() behaviour intention control social.norm attitude behaviour 0.000 0.013 0.366 -0.159 -0.009 intention 0.013 0.000 0.038 -0.078 0.148 control 0.366 0.038 0.000 0.097 0.004 social.norm -0.159 -0.078 0.097 0.000 -0.059 attitude -0.009 0.148 0.004 -0.059 0.000 Now it’s much more obvious that the behaviour/control correlation differs between the samples (it’s higher in the students). The point here is not that this is an analysis you are likely to actually report — although you might find it useful when exploring the data and interpreting your findings. But rather this show that a correlation matrix, in common with the results of all the statistical tests we run, are themselves just data points. We can do whatever we like with our results — storing them in data frames to display later, or process as we need. In reality, if you wanted to test the difference in correlations (slopes) in two groups for one outcome variable you probably want to use multiple regression, and if you wanted to test a complex model like the theory of planned behaviour, you might consider CFA and/or SEM). Tables for publication Using apaTables If you want to produce nice correlation tables for publication the apaTables package might be useful. This block saves an APA formatted correlation table to an external Word document like this. Note though, that the APA table format does encourage ‘star gazing’ to some degree. Try to avoid interpreting correlation tables solely based on the significance (or not) of the r values. The pairs or corrgram plots shown above are a much better summary of the data, and are can be just as compact. library(apaTables) apa.cor.table(airquality, filename=&quot;Table1_APA.doc&quot;, show.conf.interval=F) ## ## ## Means, standard deviations, and correlations ## ## ## Variable M SD 1 2 3 4 5 ## 1. Ozone 42.13 32.99 ## ## 2. Solar.R 185.93 90.06 .35** ## ## 3. Wind 9.96 3.52 -.60** -.06 ## ## 4. Temp 77.88 9.47 .70** .28** -.46** ## ## 5. Month 6.99 1.42 .16 -.08 -.18* .42** ## ## 6. Day 15.80 8.86 -.01 -.15 .03 -.13 -.01 ## ## ## Note. * indicates p &lt; .05; ** indicates p &lt; .01. ## M and SD are used to represent mean and standard deviation, respectively. ## By hand If you’re not bothered about strict APA format, you might still want to extract r and p values as dataframes which can then be saved to a csv and opened in Excel, or converted to a table some other way. You can do this by storing the corr.test output in a variable, and the accessing the $r and $p values within it. First, we create the corr.test object: mycorrelations &lt;- psych::corr.test(airquality) Then extract the r values as a table: mycorrelations$r %&gt;% pander() Ozone Solar.R Wind Temp Month Day Ozone 1.000 0.348 -0.602 0.70 0.165 -0.013 Solar.R 0.348 1.000 -0.057 0.28 -0.075 -0.150 Wind -0.602 -0.057 1.000 -0.46 -0.178 0.027 Temp 0.698 0.276 -0.458 1.00 0.421 -0.131 Month 0.165 -0.075 -0.178 0.42 1.000 -0.008 Day -0.013 -0.150 0.027 -0.13 -0.008 1.000 And we can also extract p values: mycorrelations$p %&gt;% pander() Ozone Solar.R Wind Temp Month Day Ozone 0.000 0.002 0.000 0.000 0.56 1.00 Solar.R 0.000 0.000 1.000 0.008 1.00 0.56 Wind 0.000 0.496 0.000 0.000 0.25 1.00 Temp 0.000 0.001 0.000 0.000 0.00 0.65 Month 0.078 0.366 0.027 0.000 0.00 1.00 Day 0.888 0.070 0.739 0.108 0.92 0.00 Saving as a .csv is the same as for other dataframes: write.csv(mycorrelations$r, file=&quot;airquality-r-values.csv&quot;) And can also access the CI for each pariwise correlation as a table: mycorrelations$ci %&gt;% head() %&gt;% pander(caption=&quot;First 6 rows of the table of CI&#39;s for the correlation matrix.&quot;) First 6 rows of the table of CI’s for the correlation matrix. lower r upper p Ozone-Slr.R 0.173 0.348 0.50 0.000 Ozone-Wind -0.706 -0.602 -0.47 0.000 Ozone-Temp 0.591 0.698 0.78 0.000 Ozone-Month -0.018 0.165 0.34 0.078 Ozone-Day -0.195 -0.013 0.17 0.888 Slr.R-Wind -0.217 -0.057 0.11 0.496 Other methods for correlation By default corr.test produces Pearson correlations, but You can pass the method argument psych::corr.test(): psych::corr.test(airquality, method=&quot;spearman&quot;) psych::corr.test(airquality, method=&quot;kendall&quot;) "],
["t-tests.html", "t-tests", " t-tests Visualising your data first Before you run any tests it’s worth plotting your data. Assuming you have a continuous outcome and categorical (binary) predictor (here we use a subset of the built in chickwts data), a boxplot can work well: chicks.eating.beans &lt;- chickwts %&gt;% filter(feed %in% c(&quot;horsebean&quot;, &quot;soybean&quot;)) chicks.eating.beans %&gt;% ggplot(aes(feed, weight)) + geom_boxplot() Figure 6.1: The box in a boxplot indictes the IQR; the whisker indicates the min/max values or 1.5 imes the IQR, whichever is the smaller. If there are outliers beyond 1.5 imes the IQR then they are shown as points. Or a violin or bottle plot, which shows the distributions within each group and makes it relatively easy to check some of the main assumptions of the test: chicks.eating.beans %&gt;% ggplot(aes(feed, weight)) + geom_violin() Layering boxes and bottles can work well too because it combines information about the distribution with key statistics like the median and IQR, and also because it scales reasonably well to multiple categories: chickwts %&gt;% ggplot(aes(feed, weight)) + geom_violin() + geom_boxplot(width=.1) Running a t-test Assuming you really do still want to run a null hypothesis test on one or two means, the t.test() function performs most common variants, illustrated below. 2 independent groups Assuming your data are in long format: t.test(weight ~ feed, data=chicks.eating.beans) ## ## Welch Two Sample t-test ## ## data: weight by feed ## t = -4.5543, df = 21.995, p-value = 0.0001559 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -125.49476 -46.96238 ## sample estimates: ## mean in group horsebean mean in group soybean ## 160.2000 246.4286 Or equivalently, if your data are untidy and each group has it’s own column (e.g. chicks eating soybeans in one column and those eating horsebeans in another): with(untidy.chicks, t.test(horsebean, soybean)) ## ## Welch Two Sample t-test ## ## data: horsebean and soybean ## t = -4.5543, df = 21.995, p-value = 0.0001559 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -125.49476 -46.96238 ## sample estimates: ## mean of x mean of y ## 160.2000 246.4286 Equal or unequal variances? By default R assumes your groups have unequal variances and applies an appropriate correction (you will notice the output labelled ‘Welch Two Sample t-test’). You can turn this correction off (for example, if you’re trying to replcate an analysis done using the default settings in SPSS) but you probably do want to assume unequal variances [see @ruxton2006unequal]. Paired samples If you have repeated measures on a sample you need a paired samples test. # simulate paired samples in pre-post design set.seed(1234) baseline &lt;- rnorm(50, 2.5, 1) followup = baseline + rnorm(50, .5, 1) # run paired samples test t.test(baseline, followup, paired=TRUE) ## ## Paired t-test ## ## data: baseline and followup ## t = -4.36, df = 49, p-value = 6.661e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.9342988 -0.3447602 ## sample estimates: ## mean of the differences ## -0.6395295 Note that we could also ‘melt’ the data into long format and use the paired=TRUE argument with a formula: long.form.data &lt;- data_frame(baseline=baseline, follow=followup) %&gt;% reshape2::melt() ## No id variables; using all as measure variables with(long.form.data, t.test(value~variable, paired=TRUE)) ## ## Paired t-test ## ## data: value by variable ## t = -4.36, df = 49, p-value = 6.661e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.9342988 -0.3447602 ## sample estimates: ## mean of the differences ## -0.6395295 One-sample test Sometimes you might want to compare a sample mean with a specific value: # test if mean of `outcome` variable is different from 2 set.seed(1234) test.scores &lt;- rnorm(50, 2.5, 1) t.test(test.scores, mu=2) ## ## One Sample t-test ## ## data: test.scores ## t = 0.37508, df = 49, p-value = 0.7092 ## alternative hypothesis: true mean is not equal to 2 ## 95 percent confidence interval: ## 1.795420 2.298474 ## sample estimates: ## mean of x ## 2.046947 "],
["linear-models-simple.html", "7 Regression", " 7 Regression This section assumes most readers will have done an introductory statistics course and had some practice running multiple regression and or Anova in SPSS or a similar package. Describing statistical models using formulae R requires that you are explicit about the statistical model you want to run but provides a neat, concise way of describing models, called a formula. For multiple regression and simple Anova, the formulas we write map closely onto the underlying linear model. The formula syntax provides shortcuts to quickly describe all the models you are likely to need. Formulas have two parts: the left hand side and the right hand side, which are separated by the tilde symbol: ~. Here, the tilde just means ‘is predicted by’. For example, this formula specifies a regression model where height is the outcome, and age and gender are the predictor variables.10 height ~ age + gender There are lots more useful tricks to learn when writing formulas, which are covered below. But in the interests of instant gratification let’s work through a simple example first: Running a linear model Linear models (including Anova and multiple regression) are run using the lm(...) function, short for ‘linear model’. We will use the mtcars dataset, which is built into R, for our first example. First, we have a quick look at the data. The pairs plot suggests that mpg might be related to a number of the other variables including disp (engine size) and wt (car weight): mtcars %&gt;% select(mpg, disp, wt) %&gt;% pairs Before running any model, we should ask outselves: “what question we are trying to answer?” In this instance, we can see that both weight (wt) and engine size (disp) are related to mpg, but they are also correlated with one another. We might want to know, then, “are weight and engine size independent predictors of mpg?” That is, if we know a car’s weight, do we gain additional information about it’s mpg by measuring engine size? To answer this, we could use multiple regression, including both wt and disp as predictors of mpg. The formula for this model would be mpg ~ wt + disp. The command below runs the model: lm(mpg ~ wt + disp, data=mtcars) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Coefficients: ## (Intercept) wt disp ## 34.96055 -3.35083 -0.01772 For readers used to wading through reams of SPSS output R might seem concise to the point of rudeness. By default, the lm commands displays very little, only repeating the formula and listing the coefficients for each predictor in the model. So what next? Unlike SPSS, we must be explicit and tell R exactly what we want. The most convenient way to do this is to first store the results of the lm() function: m.1 &lt;- lm(mpg ~ wt + disp, data=mtcars) This stores the results of the lm() function in a variable named m.1. As an aside, this is a pretty terrible variable name — try to give descriptive names to your variables because this will prevent errors and make your code easier to read. We can then use other functions to get more information about the model. For example: summary(m.1) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## wt -3.35082 1.16413 -2.878 0.00743 ** ## disp -0.01773 0.00919 -1.929 0.06362 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Although still compact, the summary function provides some familiar output, including the estimate, SE, and p value for each parameter. Take a moment to find the following statistics in the output above: The coefficients and p values for each predictor The R2 for the overall model. What % of variance in mpg is explained? Answer the original question: ‘accounting for weight (wt), does engine size (disp) tell us anything extra about a car’s mpg?’ More on formulas Above we briefly introduced R’s formula syntax. Formulas for linear models have the following structure: left_hand_side ~ right_hand_side For linear models the left side is our outcome, which is must be a continous variable. For categorical or binary outcomes you need to use glm() function, rather than lm(). See the section on generalised linear models) for more details. The right hand side of the formula lists our predictors. In the example above we used the + symbol to separate the predictors wt and disp. This told R to simply add each predictor to the model. However, many times we want to specify relationships between our predictors, as well as between predictors and outcomes. For example, we might have an experiment with 2 categorical predictors, each with 2 levels — that is, a 2x2 between-subjects design. Below, we define and run a linear model with both vs and am as predictors, along with the interaction of vs:am. We save this model as m.2, and use the summary command to print the coefficients. m.2 &lt;- lm(mpg ~ vs + am + vs:am, data=mtcars) summary(m.2) ## ## Call: ## lm(formula = mpg ~ vs + am + vs:am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.971 -1.973 0.300 2.036 6.250 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.050 1.002 15.017 6.34e-15 *** ## vs 5.693 1.651 3.448 0.0018 ** ## am 4.700 1.736 2.708 0.0114 * ## vs:am 2.929 2.541 1.153 0.2589 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.472 on 28 degrees of freedom ## Multiple R-squared: 0.7003, Adjusted R-squared: 0.6682 ## F-statistic: 21.81 on 3 and 28 DF, p-value: 1.735e-07 We’d normally want to see the Anova table for this model, including the F-tests: car::Anova(m.2) ## Anova Table (Type II tests) ## ## Response: mpg ## Sum Sq Df F value Pr(&gt;F) ## vs 367.41 1 30.4836 6.687e-06 *** ## am 276.03 1 22.9021 4.984e-05 *** ## vs:am 16.01 1 1.3283 0.2589 ## Residuals 337.48 28 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 But before you do too much with Anova in R read this section. Other formula shortcuts In addition to the + symbol, we can use other shortcuts to create linear models. As seen above, the colon (:) operator indicates the interaction between two terms. So a:b is equivalent to creating a new variable in the data frame where a is multiplied by b. The * symbol indicates the expansion of other terms in the model. So, a*b is the equivalent of a + b + a:b. Finally, it’s good to know that other functions can be used within R formulas to save work. For example, if you wanted to transform your dependent variable then log(y) ~ x will do what you might expect, and saves creating temporary variables in your dataset. The formula syntax is very powerful, and the above only shows the basics, but you can read the formulae help pages in RStudio for more details. Run the following models using the mtcars dataset: With mpg as the outcome, and with cyl and hp as predictors As above, but adding the interaction of cyl and hp. Repeat the model above, but write the formula a different way (make the formula either more or less explicit, but retaining the same predictors in the model). I avoid the terms dependent/independent variables because they are confusing to many students, and because they are misleading when discussing non-experimental data.↩ "],
["factors-and-variable-codings.html", "Factors and variable codings", " Factors and variable codings If you store categorical data as numbers (e.g. groups 1, 2, 3 …) it’s important to make sure your predictors are entered correctly into your models. In general, R works in a ‘regressiony’ way and will assume variables in a formula are linear predictors. So, a group variable coded 1…4 will be entered as a single parameter where 4 is considered twice as large as 2, etc. See below for example. In the first model cyl is entered as a ‘linear slope’; in the second each value of cyl (4,5, or 6) is treated as a separate category. The predictions from each model could be very different: linear.model &lt;- lm(mpg ~ cyl, data=mtcars) categorical.model &lt;- lm(mpg ~ factor(cyl), data=mtcars) In the case of different experimental groups what you would normally want is for group to be coded and entered as a number of categorical parameters in your model. The most common way of doing this is to use ‘dummy coding’, and this is what R will implement by default for character or factor variables. To make sure your categorical variables are entered into your model as categories (and not a slope) you can either: Convert the variable to a character or factor type in the dataframe or Specify that the variable is a factor when you run the model For example, here we specify cyl is a factor within the model formula: lm(mpg ~ factor(cyl), data=mtcars) ## ## Call: ## lm(formula = mpg ~ factor(cyl), data = mtcars) ## ## Coefficients: ## (Intercept) factor(cyl)6 factor(cyl)8 ## 26.664 -6.921 -11.564 Whereas here we convert to a factor in the original dataset: mtcars$cyl.factor &lt;- factor(mtcars$cyl) lm(mpg ~ cyl.factor, data=mtcars) ## ## Call: ## lm(formula = mpg ~ cyl.factor, data = mtcars) ## ## Coefficients: ## (Intercept) cyl.factor6 cyl.factor8 ## 26.664 -6.921 -11.564 Neither option is universally better, but if you have variables which are definitely factors (i.e. should never be used as slopes) it’s probably better to convert them in the original dataframe, before you start modelling "],
["parameterisation.html", "Model specification", " Model specification It’s helpful to think about regression and other statistical models as if they were machines that do work for us — perhaps looms in a cloth factory. We feed the machines raw materials, and they busy themselves producing the finished cloth. The nature of the finished cloth is dependent on two factors: the raw material we feed it, and the setup and configuration of the machine itself. In regression (and Anova) the same is true: Our finished results are the parameter estimates the model weaves from our raw data. The pattern we see depends on the configuration of the machine, and it’s important to realise the same data can provide very different outputs depending on the setup of the machine. Equivalent models In some cases the ‘setup’ of the machine produces changes which, although they appear very different, are in fact equivalent in some sense. Let’s say our weaving machine produces a lovely set of rugs, shown in the figure below: Rugs made in configuration 1 Now imagine that we flip all the standard settings on the weaving machine. We feed the same raw materials to the loom, but the results look very different: Rugs made after configuration is changed The second set of rugs are inversions of the first, but the patterns remain the same. The same sort of thing happens when we recode variables before entering them in our regression models. For example: coef(lm(mpg ~ wt, data=mtcars)) ## (Intercept) wt ## 37.285126 -5.344472 We can run a completely equivalent model if we ‘flip’ the weight (wt) coefficient by multiplying by -1: mtcars$wt.reversed &lt;- -1 * mtcars$wt coef(lm(mpg ~ wt.reversed, data=mtcars)) ## (Intercept) wt.reversed ## 37.285126 5.344472 These models are equivalent in all the important ways: the test statistics, p values are all the same: only the sign of the coefficient for weight has changed. The same kind of thing happens when we choose a different coding scheme for categorical variables (see section below): although the parameter estimates change when the coding format thcnages, the underlying model is equivalent because it would make the same predictions for new data. Non-equivalent models In the case above we saw models which were equivalent in the sense that they produced identical predictions for new observations. Now we need to stretch our rug analogy a little, but imagine the output of our machine is now an image of the northern lights, as in image A below, but that by changing some settings of the machine, we might instead produce image B: Image credit: https://www.flickr.com/photos/undercrimson/13155461724 You might reasonably ask why we would prefer image B or C to image A? The answer is that, when running statistical models, we must remember that they are always simplifications of reality that are (hopefully) useful to us. For example, our goal might be to use images of the northern lights to track the position of the aurora in the sky. If so, we might find that thresholding the picture in this way makes it easier to see where the centre of mass of the light is. By smoothing out many of the ‘ripples’ and suface level patterning in the light, the overall shape becomes clearer. And in fact this is one of the techniques computer vision systems do use to pre-process image inputs. Likewise, we face similar problems when analysing psychological data. For example, when we measure an outcome repeatedly over a period of seconds, days or months we are probably interested in the overall shape of the change, rather than in the suface-level patterning of these changes. Let’s stretch the images analogy again and simplify the image above by taking a single pixel high ‘slice’ through the centre of each image (A, B and C). In the figure below I’ve stretched these slices vertically so that you can see the banding in the colour, but each of these images is just a single pixel high: Let’s simplify this even further, and convert these images to greyscale: We might think of these 3 images as follows: ‘Raw’ represents our raw data (image A above), and the greys represent the value of our ‘outcome’ (intensity of light). The x-axis in this case is position in the sky, but could just as well be time or some other continuous variable. ‘Threshold’ represents some kind of categorical model for these data (akin to image B above), in which we ‘chunk’ up the x-axis and make predictions for each chunk. ‘Smooth’ (akin to image C above) represents some kind of linear model with terms which represent the gradual changes in the outcome across the range of the x-axis (like slopes or polynomial terms in regression). Because a digital image is just a list of intensity values for each pixel we can read the image into R and plot the raw values like any other: intensity.vec &lt;- as.vector(png::readPNG(&#39;media/aurora-1-px-raw.png&#39;)) aurora.image &lt;- data_frame( Intensity = intensity.vec) %&gt;% mutate(x = row_number()) aurora.image %&gt;% ggplot(aes(x, Intensity)) + geom_point(size=.5) Figure 7.1: Plot of the intensity of light in the single pixel slice from the Aurora image. Intensity of 1 corresponds to white, and 0 to black in the original image. We can fit linear models to these data, just like any other. So we migth start by predicting intensity with a simple slope: aurora.linear &lt;- lm(Intensity ~ x, data=aurora.image) And we could make predictions from this model and plot them against the original: aurora.image %&gt;% mutate(linear.prediction = predict(aurora.linear)) %&gt;% reshape2::melt(id.var=&#39;x&#39;) %&gt;% ggplot(aes(x, value, group=variable, color=variable)) + geom_point(size=.5) ## Warning: attributes are not identical across measure variables; they will ## be dropped As we can see, our predictions are pretty terrible, because the linear model only allows for a simple slope over the range of x. To improve the model, we can go in one of two ways: Fit slopes and curves for x Break x up into chunks Chunks # Create a new chunked x variable (a factor) x.in.chunks &lt;- cut(aurora.image$x, breaks=8) # Run a model with these chunks as a factor aurora.chunks &lt;- lm(Intensity ~ x.in.chunks, data=aurora.image) # Plot the predictions again aurora.image %&gt;% mutate( linear.prediction = predict(aurora.linear), chunked.prediction = predict(aurora.chunks) ) %&gt;% reshape2::melt(id.var=&#39;x&#39;) %&gt;% ggplot(aes(x, value, group=variable, color=variable)) + geom_point(size=.5) ## Warning: attributes are not identical across measure variables; they will ## be dropped That’s somewhat better, although we can still see that the extremes of our observed data are not well predicted by either the linear model (the flat line) or the chunked model. Try cutting the x variable into more chunks. What are the pros and cons of doing this? How many chunks would you need to reproduce the original data faithfully? Slopes and curves An alternative strategy at this point is to try and fit smooth curves through the data. One way of doing this (explained in greater detail in the section on polynomials) is to fit multiple parameters to represent the initial slope, and then changes in slope, across the values of x. In general, we need to fit one parameter for each change in ‘direction’ we want our cuve to take. For example, we can fit a curve with 3 changes of direction by fitting the ‘third degree’ polynomial: aurora.curve &lt;- lm(Intensity ~ poly(x, 3), data=aurora.image) aurora.image %&gt;% mutate( curve.prediction = predict(aurora.curve) ) %&gt;% reshape2::melt(id.var=&#39;x&#39;) %&gt;% ggplot(aes(x, value, group=variable, color=variable)) + geom_point(size=.5) ## Warning: attributes are not identical across measure variables; they will ## be dropped Or we could increase the number of parameters in our curve to allow a tighter fit with the raw data and plot all the models together: aurora.curve &lt;- lm(Intensity ~ poly(x, 7), data=aurora.image) all.predictions &lt;- aurora.image %&gt;% mutate( linear.prediction = predict(aurora.linear), chunked.prediction = predict(aurora.chunks), curve.prediction = predict(aurora.curve) ) %&gt;% reshape2::melt(id.var=&#39;x&#39;) ## Warning: attributes are not identical across measure variables; they will ## be dropped all.predictions %&gt;% ggplot(aes(x, value, group=variable, color=variable)) + geom_point(size=.5) We can see that this curved model is a better approximation to the raw data than our ‘chunked’ model in some places (e.g. x = 100), but worse in others (e.g. x = 625). Overall though, the R2 is much higher for the curves model here: summary(aurora.chunks)$r.squared ## [1] 0.4463338 summary(aurora.curve)$r.squared ## [1] 0.6162421 And this is the case even though our model contains only 8 parameters, and so is just as parsimonious as the chunked model above. # count the number of parameters in the chunked and curved models length(coef(aurora.chunks)) ## [1] 8 length(coef(aurora.curve)) ## [1] 8 Try to plot a curve that fits even more closely to the data. There are 1200 pixels in our original image. How many parameters would you need for the model to fit the image exactly? What happens in practice if you try and fit this model? For fun, we can even plot our data back in image form and see which is closest to matching the original: There is no ‘right answer’ here: each model has pros and cons. You need to think about what the purpose of your model is, how you want to simplify your data, and then set up your models appropriately. Effect/dummy coding and contrasts TODO: Explain this: options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) lm(mpg~factor(cyl), data=mtcars) ## ## Call: ## lm(formula = mpg ~ factor(cyl), data = mtcars) ## ## Coefficients: ## (Intercept) factor(cyl)6 factor(cyl)8 ## 26.664 -6.921 -11.564 options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) lm(mpg~factor(cyl), data=mtcars) ## ## Call: ## lm(formula = mpg ~ factor(cyl), data = mtcars) ## ## Coefficients: ## (Intercept) factor(cyl)1 factor(cyl)2 ## 20.5022 6.1615 -0.7593 Centering (is often helpful) When interpreting regression coefficients, and especially when interactions are present in a model, it’s often overlooked that the regression parameters are the effect on the outcome of a 1-unit change in the predictor, when all the other predictors are zero. This applies to the intercept too: it is the predicted value of the outcome when all of the predictors are zero. This is unhelpful because it makes the intercept mostly meaningless, and the other coefficients harder to interpret. It’s often a good idea to center your predictors so that you can interpret the intercept of the model as the average of your sample. Scaling inputs Interpreting regression coefficients requires that we think in the units of the predictor. For example, if we include ‘age in years’ in our model, then this yob coefficient gives us the change in the outcome for each additional year. However, we’re often not interested in the effect of a single year. If we are dealing with the effect of age in the general population, we’re unlikely to care about the effect of 1 year, and it might be more useful and natural to think about 10-year differences in age. In contrast, if our research is on adolescence, then the changes of the course of a year might be too crude, and we may want to think about changes over months instead. It’s important to realise there is no general or ‘correct’ solution to this problem. Regression models don’t care about the scale of our variables (within limits), but we do need to make choice about how we scale inputs. These choices should aim to make regression coefficients easily interpretable and make results comparable across studies These two goals will not always be 100% aligned, and there will be tradeoffs needed as you select your strategy, which will normally be one of: Putting coefficients on a ‘natural’ scale or relate to meaningful quantities Standardising coefficients. Using a ‘natural’ scale This will often mean just leaving your predictors ‘as-is’, but it might also mean dividing your predictor by some number to put it into more convenient units. For example, dividing age in years by 10 would mean that you can interpret the coefficient as the change over a decade, which might be easier to think about. Standardising Gelman [@gelman2008scaling] recommends standardising coefficients by centering and dividing by two standard deviations. This can be useful because binary variables like sex (male/female) will then be on a similar scale to numeric inputs. However, be cautious when standardising. You will sometimes see people interpret standardised coefficients in terms of ‘relative importance’ of the predictors. For example, they might say that if \\(\\beta^1 = .2\\) and \\(\\beta^2 = .4\\) then \\(\\beta^2\\) is twice as important as \\(\\beta^\\). Although this is appealing, it’s not always valid. The main problem is that you don’t always know whether you have a full range of values of predictors in your sample. For example, imagine a case where the a regression coefficient for age was linear, and = .5 in a sample from the general population. We can plot these data to show the effect of age, and gender: ggplot(incomes, aes(age, income, group=gender, color=gender)) + geom_point() + geom_smooth(se=F, method=&quot;lm&quot;) Older people earn more than younger people, and men earn slighly more than women (in this simulated dataset), but this gender gap doesn’t change with age. We can model this and print the effects of age and gender: m1 &lt;- lm(income~age+gender, data=incomes) coef(m1) ## (Intercept) age gender1 ## 30494.75486 30.75124 -486.30867 And we can standardize these effects using the stadardize function: coef(arm::standardize(m1)) ## (Intercept) z.age c.gender ## 31727.1240 920.5955 972.6173 Based on these standardised coefficients we migth say that age and gender are of roughly equal importance in predicting income. If we re-fit the model on a subset of the data, for example only individuals under 40, the regression coefficients won’t change much because the effect was constant across the range of ages: younger.incomes &lt;- incomes %&gt;% filter(age&lt;40) m2 &lt;- lm(income~age+gender, data=younger.incomes) coef(m2) ## (Intercept) age gender1 ## 30522.7491 29.6859 -525.4932 But, the standardised coefficients do change, because we have restricted the range of ages in the sample: coef(arm::standardize(m2)) ## (Intercept) z.age c.gender ## 31372.5885 542.2628 1050.9863 The standardised effect of age is now roughly half that of gender. The take home message here is that standardisation can be useful to put predictors on a similar scale, but it’s not a panacea and can’t be interpreted as a simple measure of ‘importance’. You still need to think! Alternatives to rescaling A nice alternative to scaling the inputs of your regression is to set aside the raw coefficients and instead make predictions for values of the predictors that are of theoretical or practical interest. The section on predictions and marginal effects has lots more detail on this. What next It is strongly recommended that you read the section on Anova before doing anything else. As noted above, R has a number of important differences in it’s default settings, as compared with packages like Stata or SPSS. These can make important differences to the way you interpret the output of linear models, especially Anova-type models with categorical predictors. "],
["anova.html", "8 Anova", " 8 Anova Be sure to read the section on linear models in R before you read this section, and specifically the parts on specifying models with formulae. This section attempts to cover in a high level way how to specify anova models in R and some of the issues in interpreting the model output. If you need to revise the basic idea of an Anova, the Howell textbook [@howell2016fundamental]. For a very quick reminder, this interactive/animated explanation of Anova is helpful. If you just want the ‘answers’ — i.e. the syntax to specify common Anova models – you could skip to the next section: Anova cookbook There are 4 rules for doing Anova in R and not wanting to cry: Keep your data in ‘long’ format. Know the differences between character, factor and numeric variables Do not use the aov() or anova() functions to get an Anova table unless you know what you are doing. Learn about the types of sums of squares and always remember to specify type=3, unless you know better. Rules for using Anova in R Rule 1: Use long format data In R, data are almost always most useful a long format where: each row of the dataframe corresponds to a single measurement occasion each column corresponds to a variable which is measured For example, in R we will have data like this: df %&gt;% head %&gt;% pander person time predictor outcome 1 1 2 7 1 2 2 17 1 3 2 9 2 1 5 16 2 2 5 8 2 3 5 7 Whereas in SPSS we might have the same data structured like this: df.wide %&gt;% head %&gt;% pander person predictor Time 1 Time 2 Time 3 1 2 7 17 9 2 5 16 8 7 3 6 7 10 7 4 4 8 6 9 5 2 9 8 12 6 3 12 9 13 R always uses long form data when running an Anova, but one downside is that it therefore has no automatic to know which rows belong to which person (assuming individual people are the unit of error in your model). This means that for repeated measures designs you need to make explicit which measures are repeated when specifying the model (see the section on repeated designs below). Rule 2: Know your variables See the section on dataframes and on the different column types and be sure you can distinguish: Numeric variables Factors Character strings. In Anova: Outcomes will be numeric variables Predictors will be factors or (preferably) character strings If you want to run Ancova models, you can also add numeric predictors. Rule 3: Don’t use aov() or anova() This is the most important rule of all. The aov and anova functions have been around in R a long time. For various historical reasons the defaults for these functions won’t do what you expect if you are used to SPSS, Stata, SAS, and most other stats packages. These differences are important and will be confusing and give you misleading results unless you understand them. The recommendation here is: If you have a factorial experiment define your model using lm() and then use car::Anova() to calculate F tests. If you have repeated measures, your data are perfectly balanced, and you have no missing values then use afex::car_aov(). If you think you want a repeated measures Anova but your data are not balanced, or you have missing data, use linear mixed models instead via the lme4:: package. Rule 4: Use type 3 sums of squares (and learn why) You may be aware, but there are at least 3 different ways of calculating the sums of squares for each factor and interaction in an Anova. In short, SPSS and most other packages use type 3 sums of squares. aov and anova use type 1. By default, car::Anova and ez::ezANOVA use type 2, but can use type 3 if you ask. This means you must: Make sure you use type 3 sums of squares unless you have a reason not to. Always pass type=3 as an argument when running an Anova. A longer explanation of why you probably want type 3 sums of squares is given in this online discussion on stats.stackechange.com and practical implications are shown in this worked example. An even longer answer, including a much deeper exploration of the philosophical questions involved is given by @venables1998exegeses. Recommendations for doing Anova Make sure to Plot your raw data first Where you have interactions, be cautious in interpreting the main effects in your model, and always plot the model predictions. If you find yourself aggregating (averaging) data before running your model, think about using a mixed or multilevel model instead. If you are using repeated measures Anova, check if you should should be using a mixed model instead. If you have an unbalanced design or any missing data, you probably should use a mixed model. "],
["anova-cookbook.html", "Anova ‘Cookbook’", " Anova ‘Cookbook’ This section is intended as a shortcut to running Anova for a variety of common types of model. If you want to understand more about what you are doing, read the section on principles of Anova in R first, or consult an introductory text on Anova which covers Anova [e.g. @howell2012statistical]. Between-subjects Anova Oneway Anova (&gt; 2 groups) If your design has more than 2 groups then you should use oneway Anova. Let’s say we asked people to taste 1 of 4 fruit juices, and rate how tasty it was on a scale from 0 to 10: We can run a oneway Anova with type 3 sums of squares using the Anova function from the car:: package: juice.lm &lt;- lm(tastiness ~ juice, data=tasty.juice) juice.anova &lt;- car::Anova(juice.lm, type=3) juice.anova ## Anova Table (Type III tests) ## ## Response: tastiness ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 615.04 1 114.4793 &lt; 2.2e-16 *** ## juice 128.83 3 7.9932 8.231e-05 *** ## Residuals 515.76 96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 And we could compute the contasts for each fruit against the others (the grand mean): juice.lsm &lt;- lsmeans::lsmeans(juice.lm, pairwise~juice, adjust=&quot;fdr&quot;) juice.contrasts &lt;- summary(lsmeans::contrast(juice.lsm, &quot;eff&quot;)) juice.contrasts ## contrast estimate SE df t.ratio p.value ## Mango effect 0.25 0.4014661 96 0.623 0.5349 ## Apple effect 1.01 0.4014661 96 2.516 0.0271 ## Orange effect 0.65 0.4014661 96 1.619 0.1450 ## Durian effect -1.91 0.4014661 96 -4.758 &lt;.0001 ## ## P value adjustment: fdr method for 4 tests We found a significant main effect of juice, F(3, 96) = 7.99, p &lt; .001. Followup tests (adjusted for false discovery rate) indicated that only Durian differed from the other juices, and was rated a significantly less tasty Mango, Apple, and Orange juice, B = -1.91 (0.40), t = -4.76, p &lt; .001 Factorial Anova We are using a dataset from Howell [@howell2012statistical, chapter 13]: an experiment which recorded Recall among young v.s. older adults (Age) for each of 5 conditions. These data would commonly be plotted something like this: eysenck &lt;- readRDS(&quot;data/eysenck.Rdata&quot;) eysenck %&gt;% ggplot(aes(Condition, Recall, group=Age, color=Age)) + stat_summary(geom=&quot;pointrange&quot;, fun.data = mean_cl_boot) + ylab(&quot;Recall (95% CI)&quot;) + xlab(&quot;&quot;) Visual inspection of the data (see Figure X) suggested that older adults recalled more words than younger adults, and that this difference was greatest for the intention, imagery, and adjective conditions. Recall peformance was worst in the counting and rhyming conditions. Or alternatively if we wanted to provde a better summary of the distribution of the raw data we could use a boxplot: eysenck %&gt;% ggplot(aes(Age, Recall)) + geom_boxplot(width=.33) + facet_grid(~Condition) + ylab(&quot;Recall (95% CI)&quot;) + xlab(&quot;&quot;) Figure 2.2: Boxplot for recall in older and young adults, by condition. We can run a linear model including the effect of Age and Condition and the interaction of these variables, and calculate the Anova: eysenck.model &lt;- lm(Recall~Age*Condition, data=eysenck) car::Anova(eysenck.model, type=3) ## Anova Table (Type III tests) ## ## Response: Recall ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 490.00 1 61.0550 9.85e-12 *** ## Age 1.25 1 0.1558 0.6940313 ## Condition 351.52 4 10.9500 2.80e-07 *** ## Age:Condition 190.30 4 5.9279 0.0002793 *** ## Residuals 722.30 90 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Repeated measures or ‘split plot’ designs It might be controversial to say so, but the tools to run traditional repeat measures Anova in R are a bit of a pain to use. Although there are numerous packages simplify the process a little, their syntax can be obtuse or confusing. To make matters worse, various textbooks, online guides and the R help files themselves show many ways to achieve the same ends, and it can be difficult to follow the differences between the underlying models that are run. At this point, given the many other advantages of linear mixed models over traditional repeated measures Anova, and given that many researchers abuse traditional Anova in practice (e.g. using it for unbalanced data, or where some data are missing), the recommendation here is to simply give up and learn how to run linear mixed models. These can (very closely) replicate traditional Anova approaches, but also: Handle missing data or unbalanced designs gracefully and efficiently. Be expanded to include multiple levels of nesting. For example, allowing pupils to be nested within classes, within schools. Alternatively multiple measurements of individual patients might be clustered by hospital or therapist. Allow time to be treated as a continuous variable. For example, time can be modelled as a slope or some kind of curve, rather than a fixed set of observation-points. This can be more parsimonious, and more flexible when dealing with real-world data (e.g. from clinical trials). It would be best at this point to jump straight to the main section multilevel or mixed-effects models, but to give one brief example of mixed models in use: The sleepstudy dataset in the lme4 package provides reaction time data recorded from participants over a period of 10 days, during which time they were deprived of sleep. lme4::sleepstudy %&gt;% head(12) %&gt;% pander Reaction Days Subject 249.6 0 308 258.7 1 308 250.8 2 308 321.4 3 308 356.9 4 308 414.7 5 308 382.2 6 308 290.1 7 308 430.6 8 308 466.4 9 308 222.7 0 309 205.3 1 309 We can plot these data to show the increase in RT as sleep deprivation continues: lme4::sleepstudy %&gt;% ggplot(aes(factor(Days), Reaction)) + geom_boxplot() + xlab(&quot;Days&quot;) + ylab(&quot;RT (ms)&quot;) + geom_label(aes(y=400, x=2, label=&quot;you start to\\nfeel bad here&quot;), color=&quot;red&quot;) + geom_label(aes(y=450, x=9, label=&quot;imagine how bad\\nyou feel by this point&quot;), color=&quot;red&quot;) If we want to test whether there are significant differences in RTs between Days, we could fit something very similar to a traditional repeat measures Anova using the lme4::lmer() function, and obtain an Anova table for the model using the special lmerTest::anova() function: sleep.model &lt;- lmer(Reaction ~ factor(Days) + (1 | Subject), data=lme4::sleepstudy) lmerTest::anova(sleep.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## factor(Days) 166235 18471 9 153 18.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Traditional repeated measures Anova If you really need to fit the traditional repeated measures Anova (e.g. your supervisor/reviewer has asked you to) then you should use either the afex:: or ez:: packages. Let’s say we have an experiment where we record reaction 25 times (Trial) before and after (Time = {1, 2}) one of 4 experimental manipulations (Condition = {1,2,3,4}). You have 12 participants in each condition and no missing data: expt.data %&gt;% ggplot(aes(Condition, RT)) + geom_boxplot() + facet_wrap(~paste(&quot;Time&quot;, time)) We want to use our repeated measurements before and after the experimental interventions to increase the precision of our estimate of the between-condition differences. Our first step is to aggregate RTs for the multiple trials, taking the mean across all trials at a particular time: expt.data.agg &lt;- expt.data %&gt;% group_by(Condition, person, time) %&gt;% summarise(RT=mean(RT)) head(expt.data.agg) ## Source: local data frame [6 x 4] ## Groups: Condition, person [3] ## ## Condition person time RT ## &lt;fctr&gt; &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 1 1 1 260.8600 ## 2 1 1 2 265.6524 ## 3 1 2 1 279.1190 ## 4 1 2 2 267.0095 ## 5 1 3 1 254.3760 ## 6 1 3 2 256.4459 Because our data are still in long form (we have two rows per person), we have to explicitly tell R that time is a within subject factor. Using the afex:: package we would write: expt.afex &lt;- afex::aov_car(RT ~ Condition * time + Error(person/time), data=expt.data.agg) expt.afex$anova_table %&gt;% pander(caption=&quot;`afex::aov_car` output.&quot;) afex::aov_car output. num Df den Df MSE F ges Pr(&gt;F) Condition 3 44 158.6 159.4 0.8504 1.193e-23 time 1 44 144.6 11.87 0.114 0.001266 Condition:time 3 44 144.6 37.79 0.5513 3.073e-12 Using the ez:: package we would write: expt.ez &lt;- ez::ezANOVA(data=expt.data.agg, dv = RT, wid = person, within = time, between = Condition) expt.ez$ANOVA %&gt;% pander(caption=&quot;`ez::ezANOVA` output.&quot;) ez::ezANOVA output. Effect DFn DFd F p p&lt;.05 ges 2 Condition 3 44 159.4 1.193e-23 * 0.8504 3 time 1 44 11.87 0.001266 * 0.114 4 Condition:time 3 44 37.79 3.073e-12 * 0.5513 These are the same models: any differences in the output are simply due to rounding. You should use whichever of ez:: and afex:: you find easiest to understand The ges column is the generalised eta squared effect-size measure, which is preferable to the partial eta-squared reported by SPSS [@bakeman2005recommended]. But what about [insert favourite R package for Anova]? Lots of people like ez::ezANOVA and other similar packages. My problem with ezANOVA is that it doesn’t use formulae to define the model and for this reason encourages students to think of Anova as something magical and separate from linear models and regression in general. This guide is called ‘just enough R’, so I’ve mostly chosen to show only car::Anova because I find this the most coherent method to explain. Using formulae to specify the model reinforces a technique which is useful in many other contexts. I’ve make an exception for repeated because many people find specifying the error structure explicitly confusing and hard to get right, and so ez:: may be the best option in these cases. Comparison with a multilevel model For reference, a broadly equivalent (although not identical) multilevel model would be: expt.mlm &lt;- lmer(RT ~ Condition * time + (1|person), data=expt.data.agg) anova(expt.mlm) %&gt;% pander() Analysis of Variance Table of type III with Satterthwaite Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) Condition 69154 23051 3 44 159.4 0 time 1716 1716 1 44 11.87 0.001266 Condition:time 16394 5465 3 44 37.79 3.073e-12 Although with a linear mixed model it would also be posible to analyse the trial-by-trial data. Let’s hypothesise, for example, that subjects in Conditions 2 and 4 experienced a ‘practice effect’, such that their RTs reduced over multiple trials. If we plot the data, we can see this suspicion may be supported (how conveninent!): ggplot(expt.data, aes(trial, RT)) + geom_smooth() + facet_wrap(~paste(&quot;Condition&quot;, Condition)) If we wanted to replicate the aggregated RM Anova models shown above we could write: options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) expt.mlm2 &lt;- lmer(RT ~ Condition * time + (time|person), data=expt.data) anova(expt.mlm2) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Condition 1391631 463877 3 64.082 129.803 &lt; 2.2e-16 *** ## time 32791 32791 1 71.596 9.176 0.00341 ** ## Condition:time 313212 104404 3 71.596 29.215 1.911e-12 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 But we can now add a continuous predictor for trial: expt.mlm.bytrial &lt;- lmer(RT ~ Condition * time * trial + (time|person), data=expt.data) anova(expt.mlm.bytrial) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Condition 181589 60530 3 591.06 17.3653 8.114e-11 *** ## time 7744 7744 1 670.24 2.2218 0.13655 ## trial 81015 81015 1 2339.99 23.2424 1.520e-06 *** ## Condition:time 94032 31344 3 670.24 8.9923 7.612e-06 *** ## Condition:trial 126361 42120 3 2339.99 12.0839 7.557e-08 *** ## time:trial 109 109 1 2339.99 0.0314 0.85943 ## Condition:time:trial 27086 9029 3 2339.99 2.5902 0.05125 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The significant Condition:trial term indicates that there was a difference in the practice effects between the experimental conditions. We found a significant interaction between condition and the linear term for trial number, F(3, 2340.18) = 10.83, p &lt; .001. We explored this effect by plotting model-estimated reaction times for each group for trials 1 through 25 (see Figure X): participants in condition 2 and 4 exprienced a greater reduction in RTs across trial, suggesting a larger practice effect for these conditions. See the multilevel models section for more details, including analyses which allow the effects of interventions to vary between participants (i.e., relaxing the assumption that an intervention will be equally effective for all participants). RM Anova v.s. multilevel models The RM Anova is perhaps more familiar, and may be conventional in your field which can make peer review easier (although in other fields mixed models are now expected where the design warrants it). RM Anova requires complete data: any participant with any missing data will be dropped from the analysis. This is problematic where data are expensive to collect, and where data re unlikely to be missing at random, for example in a clinical trial. In these cases RM Anova may be less efficient and more biased than an equivalent multilevel model. There is no simple way of calculating effect size measures like eta2 from the lmer model. This may or may not be a bad thing. @baguley2009standardized, for example, recommends reporting simple (rather than standardised) effect size measures, and is easily done by making predictions from the model. "],
["checking-assumptions.html", "Checking assumptions", " Checking assumptions The text below continues on from this example of factorial Anova. If we want to check that the assumptions of our Anova models are met, these tables and plots would be a reasonable place to start. First running Levene’s test: car::leveneTest(eysenck.model) %&gt;% pander() Levene’s Test for Homogeneity of Variance (center = median) Df F value Pr(&gt;F) group 9 1.031 0.4217 90 NA NA Then a QQ-plot of the model residuals to assess normality: car::qqPlot(eysenck.model) Figure 8.1: QQ plot to assess normality of model residuals And finally a residual-vs-fitted plot: data_frame( fitted = predict(eysenck.model), residual = residuals(eysenck.model)) %&gt;% # and then plot points and a smoothed line ggplot(aes(fitted, residual)) + geom_point() + geom_smooth(se=F) Figure 8.2: Residual vs fitted (spread vs. level) plot to check homogeneity of variance. For more on assumptions checks after linear models or Anova see: http://www.statmethods.net/stats/anovaAssumptions.html "],
["followup-tests.html", "Followup tests", " Followup tests The text below continues on from this example of factorial Anova. If we want to look at post-hoc pairwise tests we can use the the lsmeans() function from the lsmeans:: package. By default Tukey correction is applied for multiple comparisons, which is a reasonable default: lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition) ## $lsmeans ## Age Condition lsmean SE df lower.CL upper.CL ## Young Counting 7.0 0.8958547 90 5.220228 8.779772 ## Older Counting 6.5 0.8958547 90 4.720228 8.279772 ## Young Rhyming 6.9 0.8958547 90 5.120228 8.679772 ## Older Rhyming 7.6 0.8958547 90 5.820228 9.379772 ## Young Adjective 11.0 0.8958547 90 9.220228 12.779772 ## Older Adjective 14.8 0.8958547 90 13.020228 16.579772 ## Young Imagery 13.4 0.8958547 90 11.620228 15.179772 ## Older Imagery 17.6 0.8958547 90 15.820228 19.379772 ## Young Intention 12.0 0.8958547 90 10.220228 13.779772 ## Older Intention 19.3 0.8958547 90 17.520228 21.079772 ## ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## Young,Counting - Older,Counting 0.5 1.26693 90 0.395 1.0000 ## Young,Counting - Young,Rhyming 0.1 1.26693 90 0.079 1.0000 ## Young,Counting - Older,Rhyming -0.6 1.26693 90 -0.474 1.0000 ## Young,Counting - Young,Adjective -4.0 1.26693 90 -3.157 0.0633 ## Young,Counting - Older,Adjective -7.8 1.26693 90 -6.157 &lt;.0001 ## Young,Counting - Young,Imagery -6.4 1.26693 90 -5.052 0.0001 ## Young,Counting - Older,Imagery -10.6 1.26693 90 -8.367 &lt;.0001 ## Young,Counting - Young,Intention -5.0 1.26693 90 -3.947 0.0058 ## Young,Counting - Older,Intention -12.3 1.26693 90 -9.709 &lt;.0001 ## Older,Counting - Young,Rhyming -0.4 1.26693 90 -0.316 1.0000 ## Older,Counting - Older,Rhyming -1.1 1.26693 90 -0.868 0.9970 ## Older,Counting - Young,Adjective -4.5 1.26693 90 -3.552 0.0205 ## Older,Counting - Older,Adjective -8.3 1.26693 90 -6.551 &lt;.0001 ## Older,Counting - Young,Imagery -6.9 1.26693 90 -5.446 &lt;.0001 ## Older,Counting - Older,Imagery -11.1 1.26693 90 -8.761 &lt;.0001 ## Older,Counting - Young,Intention -5.5 1.26693 90 -4.341 0.0015 ## Older,Counting - Older,Intention -12.8 1.26693 90 -10.103 &lt;.0001 ## Young,Rhyming - Older,Rhyming -0.7 1.26693 90 -0.553 0.9999 ## Young,Rhyming - Young,Adjective -4.1 1.26693 90 -3.236 0.0511 ## Young,Rhyming - Older,Adjective -7.9 1.26693 90 -6.236 &lt;.0001 ## Young,Rhyming - Young,Imagery -6.5 1.26693 90 -5.131 0.0001 ## Young,Rhyming - Older,Imagery -10.7 1.26693 90 -8.446 &lt;.0001 ## Young,Rhyming - Young,Intention -5.1 1.26693 90 -4.025 0.0044 ## Young,Rhyming - Older,Intention -12.4 1.26693 90 -9.787 &lt;.0001 ## Older,Rhyming - Young,Adjective -3.4 1.26693 90 -2.684 0.1963 ## Older,Rhyming - Older,Adjective -7.2 1.26693 90 -5.683 &lt;.0001 ## Older,Rhyming - Young,Imagery -5.8 1.26693 90 -4.578 0.0006 ## Older,Rhyming - Older,Imagery -10.0 1.26693 90 -7.893 &lt;.0001 ## Older,Rhyming - Young,Intention -4.4 1.26693 90 -3.473 0.0260 ## Older,Rhyming - Older,Intention -11.7 1.26693 90 -9.235 &lt;.0001 ## Young,Adjective - Older,Adjective -3.8 1.26693 90 -2.999 0.0950 ## Young,Adjective - Young,Imagery -2.4 1.26693 90 -1.894 0.6728 ## Young,Adjective - Older,Imagery -6.6 1.26693 90 -5.209 0.0001 ## Young,Adjective - Young,Intention -1.0 1.26693 90 -0.789 0.9986 ## Young,Adjective - Older,Intention -8.3 1.26693 90 -6.551 &lt;.0001 ## Older,Adjective - Young,Imagery 1.4 1.26693 90 1.105 0.9830 ## Older,Adjective - Older,Imagery -2.8 1.26693 90 -2.210 0.4578 ## Older,Adjective - Young,Intention 2.8 1.26693 90 2.210 0.4578 ## Older,Adjective - Older,Intention -4.5 1.26693 90 -3.552 0.0205 ## Young,Imagery - Older,Imagery -4.2 1.26693 90 -3.315 0.0411 ## Young,Imagery - Young,Intention 1.4 1.26693 90 1.105 0.9830 ## Young,Imagery - Older,Intention -5.9 1.26693 90 -4.657 0.0005 ## Older,Imagery - Young,Intention 5.6 1.26693 90 4.420 0.0011 ## Older,Imagery - Older,Intention -1.7 1.26693 90 -1.342 0.9409 ## Young,Intention - Older,Intention -7.3 1.26693 90 -5.762 &lt;.0001 ## ## P value adjustment: tukey method for comparing a family of 10 estimates Both cell means and pairwise contrasts are shown here. There is much more detail on computing pairwise comparisons and other types of contrasts in the section on multiple comparisons, including ways to extract and present your comparisons in APA format. "],
["general-linear-models.html", "9 General linear models", " 9 General linear models Linear regression is suitable for outcomes which are continuous numerical scores. In practice this requirement is often relaxed slightly, for example for data which are slightly skewed, or where scores are somewhat censored (e.g. questionnaire scores which have a minium or maximum). However, for some types of outcomes standard linear models are unsuitable. Examples here include binary (zero or one) or count data (i.e. positive integers representing frequencies), or proportions (e.g. proportion of product failures per batch). This section is primarily concerned with binary outcomes, but many of the same principles apply to these other types of outcome. Logistic regression In R we fit logistic regression with the glm() function which is built into R, or if we have a multilevel model with a binary outcome we use glmer() from the lme4:: package. Fitting the model is very similar to linear regression, except we need to specify the family=&quot;binomial&quot; parameter to let R know what type of data we are using. Here we use the titanic dataset (you can download this from Kaggle, although you need to sign up for an account). Before we start fitting models, it’s best to plot the data to give us a feel for what is happening. Figure 1 reveals that, across all fare categories, women were more likely to survive the disaster than men. Ticket class also appears to be related to outcome: those with third class tickets were less likely to survive than those with first or second class tickets. However, differences in survival rates for men and women differed across ticket classes: women with third class tickets appear to have been less advantaged (compared to men) than women with first or second class tickets. titanic &lt;- read.csv(&#39;data/titanic.csv&#39;) titanic %&gt;% ggplot(aes(factor(Pclass), Survived, group=Sex, color=Sex)) + stat_summary() + stat_summary(geom=&quot;line&quot;) + xlab(&quot;Ticket class&quot;) Figure 9.1: Survival probabilities by Sex and ticket class. Given the plot above, it seems reasonable to predict survival from Sex and Pclass, and also to include the interaction between these variables. To run a logistic regression we specify the model as we would with lm(), but instead use glm() and specify the family parameter: m &lt;- glm(Survived ~ Sex * factor(Pclass), data=titanic, family = binomial(link=&quot;logit&quot;)) Because it can become repetitive to write out the family parameter in full each time, I usually write a ‘helper function’ called logistic() which simply calls glm with the right settings. For example: # define a helper function for logistic regression the &#39;...&#39; # means &#39;all arguments&#39;, so this function passes all it&#39;s # arguments on to the glm function, but sets the family correctly logistic &lt;- function(...) { glm(..., family = binomial(link=&quot;logit&quot;)) } Which you can use like so: logistic(Survived ~ Sex * factor(Pclass), data=titanic) ## ## Call: glm(formula = ..1, family = binomial(link = &quot;logit&quot;), data = ..2) ## ## Coefficients: ## (Intercept) Sexmale factor(Pclass)2 ## 3.4122 -3.9494 -0.9555 ## factor(Pclass)3 Sexmale:factor(Pclass)2 Sexmale:factor(Pclass)3 ## -3.4122 -0.1850 2.0958 ## ## Degrees of Freedom: 890 Total (i.e. Null); 885 Residual ## Null Deviance: 1187 ## Residual Deviance: 798.1 AIC: 810.1 Tests of parameters As with lm() models, we can use the summary() function to get p values for parameters in glm objects: titanic.model &lt;- logistic(Survived ~ Sex * factor(Pclass), data=titanic) summary(titanic.model) ## ## Call: ## glm(formula = ..1, family = binomial(link = &quot;logit&quot;), data = ..2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.6248 -0.5853 -0.5395 0.4056 1.9996 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 3.4122 0.5868 5.815 6.06e-09 *** ## Sexmale -3.9494 0.6161 -6.411 1.45e-10 *** ## factor(Pclass)2 -0.9555 0.7248 -1.318 0.18737 ## factor(Pclass)3 -3.4122 0.6100 -5.594 2.22e-08 *** ## Sexmale:factor(Pclass)2 -0.1850 0.7939 -0.233 0.81575 ## Sexmale:factor(Pclass)3 2.0958 0.6572 3.189 0.00143 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1186.7 on 890 degrees of freedom ## Residual deviance: 798.1 on 885 degrees of freedom ## AIC: 810.1 ## ## Number of Fisher Scoring iterations: 6 You might have spotted in this table that summary reports z tests rather than t tests for parameters in the glm model. These can be interepreted as you would the t-test in a linear model, however. Tests of categorical predictorss Where there are categorical predictors we can also reuse the car::Anova function to get the equivalent of the F test from a linear model (with type 3 sums of squares; remember not to use the built in anova function unless you want type 1 sums of squares): car::Anova(titanic.model, type=3) ## Analysis of Deviance Table (Type III tests) ## ## Response: Survived ## LR Chisq Df Pr(&gt;Chisq) ## Sex 97.547 1 &lt; 2.2e-16 *** ## factor(Pclass) 90.355 2 &lt; 2.2e-16 *** ## Sex:factor(Pclass) 28.791 2 5.598e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Note that the Anova table for a glm model provides \\(\\chi^2\\) tests in place of F tests. Although they are calculated differently, you can interpret these \\(\\chi^2\\) tests and p values as you would for F tests in a regular Anova. Predictions after glm As with linear models, we can make predictions from glm models for our current or new data. One twist here though is that we have to choose whether to make predictions in units of the response (i.e. probability of survival), or of the transformed response (logit) that is actually the ‘outcome’ in a glm (see the explainer on transformations and links functions). You will almost always want predictions in the units of your response, which means you need to add type=&quot;response&quot; to the predict() function call. Here we predict the chance of survival for a new female passenger with a first class ticket: new.passenger = expand.grid(Pclass=1, Sex=c(&quot;female&quot;)) predict.glm(titanic.model, newdata=new.passenger, type=&quot;response&quot;) ## 1 ## 0.9680851 And we could plot probabilities for each gender and class with a standard error for this prediction if desired: new.passengers = expand.grid(Pclass=1:3, Sex=c(&quot;female&quot;, &quot;male&quot;)) # this creates two vectors: $fit, which contains # predicted probabilities and $se.fit preds &lt;- predict.glm(titanic.model, newdata=new.passengers, type=&quot;response&quot;, se.fit=T) new.passengers %&gt;% mutate(fit = preds$fit, lower=fit - preds$se.fit, upper=fit + preds$se.fit) %&gt;% ggplot(aes(factor(Pclass), fit, ymin=lower, ymax=upper, group=Sex, color=Sex)) + geom_pointrange() + geom_line() + xlab(&quot;Ticket class&quot;) + ylab(&quot;Probability of survival&quot;) Evaluating logistic regression models glm models don’t provide an R2 statistic, but it is possible to evaluate how well the model fits the data in other ways. Although there are various pseudo-R2 statistics available for glm; see https://www.r-bloggers.com/evaluating-logistic-regression-models/ One common technique, however, is to build a model using a ‘training’ dataset (sometimes a subset of your data) and evaluate how well this model predicts new observations in a ‘test’ dataset. See http://r4ds.had.co.nz/model-assess.html for an introduction. "],
["multilevel-models.html", "10 Multilevel models", " 10 Multilevel models Psychological data often contains natural groupings. In intervention research, multiple patients may be treated by individual therapists, or children taught within classes, which are further nested within schools; in experimental research participants may respond on multiple occasions to a variety of stimuli. Although disparate in nature, these groupings share a common characteristic: they induce dependency between the observations we make. That is, our data points are not independently sampled from one another. When data are clustered int his way then multilevel, sometimes called linear mixed models, serve two purposes: They overcome limitations of conventional models which assume that data are independently sampled (read a more detailed explanation of why handling non-independence properly matters) They allow us to answer substantive questions about sources of variation in our data. Repeated measures Anova and beyond RM Anova is another technique which relaxes the assumption of independent sampling, and is widely used in psychology: it is common that participants make repeated responses which can be categorised by various experimental variables (e.g. time, condition). However RM Anova is just a special case of a much wider family of models: linear mixed models, but one which makes a number of restrictions which can be invonvenient, inefficient, or unreasonable. Substantive questions about variation Additionally, rather than simply ‘managing’ the non-independence of observations — treating it is a kind of nuisance to be eliminated — mixed models can allow researchers to focus on the sources of variation in their data directly. It can be of substantive interest to estimate how much variation in the outcome is due to different levels of the nested structure. For example, in a clinical trial researchers might want to know how much influence therapists have on their clients’ outcome: if patients are ‘nested’ within therapists then multilevel models can estimate the variation between therapists (the ‘therapist effect’) and variation ‘within’ therapists (i.e. variation between clients). "],
["fitting-models.html", "Fitting multilevel models in R", " Fitting multilevel models in R Use lmer and glmer Although there are mutiple R packages which can fit mixed-effects regression models, the lmer and glmer functions within the lme4 package are the most frequently used, for good reason, and the examples below all use these two functions. p values in multilevel models For various philosophical and statistical reasons the author of lme4, Doug Bates, has always refused to display p values in the output from lmer (his reasoning is explained here). That notwithstanding, many people have wanted to use the various methods to calculate p values for parameters in mixed models, and calculate F tests for effects and interactions. Various methods have been developed over the years which address at least some of Bates’ concerns, and these techniques have been implemented in R in the lmerTest:: package. In particular, lmerTest implements an anova function for lmer models, which is very helpful. Don’t worry! All you need to do is to load the lmerTest package rather than lme4. This loads updated versions of lmer, glmer, and extra functions for things like calculating F tests and the Anova table. The lmer formula syntax Specifying lmer models is very similar to the syntax for lm. The ‘fixed’ part of the model is exactly the same, with additional parts used to specify random intercepts, random slopes, and control the covariances of these random effects (there’s more on this in the troubleshooting section). Random intercepts The simplest model which allows a ‘random intercept’ for each level in the grouping looks like this: lmer(outcome ~ predictors + (1 | grouping), data=df) Here the outcome and predictors are specified in a formula, just as we did when using lm(). The only difference is that we now add a ‘random part’ to the model, in this case: (1|grouping). The 1 refers to an intercept, and so in English this part of the formula means ‘add a random intercept for each level of grouping’. Random slopes If we want to add a random slope to the model, we could adjust the random part like so: lmer(outcome ~ predictor + (predictor | grouping), data=df) This implicitly adds a random intercept too, so in English this formula says something like: let outcome be predicted by predictor; let variation in outcome to vary between levels of grouping, and also allow the effect of predictor to vary between levels of grouping. The lmer syntax for the random part is very powerful, and allows complex combinations of random intercepts and slopes and control over how these random effects are allowed to correlate with one another. For a detailed guide to fitting two and three level models, with various covariance structures, see: http://rpsychologist.com/r-guide-longitudinal-lme-lmer Are my effects fixed or random? If you’re not sure which part of your model should be ‘fixed’ and which parts should be ‘random’ theres a more detailed explanation in this section. "],
["extending-traditional-rm-anova.html", "Extending traditional RM Anova", " Extending traditional RM Anova As noted in the Anova cookbook section, repeated measures anova can be approximated using linear mixed models. For example, reprising the sleepstudy example, we can approximate a repeated measures Anova in which multiple measurements of Reaction time are taken on multiple Days for each Subject. As we saw before, the traditional RM Anova model is: sleep.rmanova &lt;- afex::aov_car(Reaction ~ Days + Error(Subject/(Days)), data=lme4::sleepstudy) sleep.rmanova ## Anova Table (Type 3 tests) ## ## Response: Reaction ## Effect df MSE F ges p.value ## 1 Days 3.32, 56.46 2676.18 18.70 *** .29 &lt;.0001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG The equivalent lmer model is: library(lmerTest) sleep.lmer &lt;- lmer(Reaction ~ factor(Days) + (1|Subject), data=lme4::sleepstudy) anova(sleep.lmer) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## factor(Days) 166235 18471 9 153 18.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The following sections demonstrate just some of the extensions to RM Anova which are possible with mutlilevel models, Fit a simple slope for Days lme4::sleepstudy %&gt;% ggplot(aes(Days, Reaction)) + geom_point() + geom_jitter() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; slope.model &lt;- lmer(Reaction ~ Days + (1|Subject), data=lme4::sleepstudy) lmerTest::anova(slope.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Days 162703 162703 1 161 169.4 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 slope.model.summary &lt;- summary(slope.model) slope.model.summary$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 251.40510 9.7467163 22.8102 25.79383 0 ## Days 10.46729 0.8042214 161.0036 13.01543 0 Allow the effect of sleep deprivation to vary for different participants If we plot the data, it looks like sleep deprivation hits some participants worse than others: set.seed(1234) lme4::sleepstudy %&gt;% filter(Subject %in% sample(levels(Subject), 10)) %&gt;% ggplot(aes(Days, Reaction, group=Subject, color=Subject)) + geom_smooth(method=&quot;lm&quot;, se=F) + geom_jitter(size=1) + theme_minimal() If we wanted to test whether there was significant variation in the effects of sleep deprivation between subjects, by adding a random slope to the model. The random slope allows the effect of Days to vary between subjects. So we can think of an overall slope (i.e. RT goes up over the days), from which individuals deviate by some amount (e.g. a resiliant person will have a negative deviation or residual from the overall slope). Adding the random slope doesn’t change the F test for Days that much: random.slope.model &lt;- lmer(Reaction ~ Days + (Days|Subject), data=lme4::sleepstudy) lmerTest::anova(random.slope.model) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Days 30031 30031 1 17 45.853 3.264e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Nor the overall slope coefficient: random.slope.model.summary &lt;- summary(random.slope.model) slope.model.summary$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 251.40510 9.7467163 22.8102 25.79383 0 ## Days 10.46729 0.8042214 161.0036 13.01543 0 But we can use the lmerTest::rand() function to show that there is statistically significant variation in slopes between individuals, using the likelihood ratio test: lmerTest::rand(random.slope.model) ## Analysis of Random effects Table: ## Chi.sq Chi.DF p.value ## Days:Subject 42.8 2 5e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Because the random slope for Days is statistically significant, we know it improves the model. One way to see that improvement is to plot residuals (unexplained error for each datapoint) against predicted values. To extract residual and fitted values we use the residuals() and predict() functions. These are then combined in a data_frame, to enable us to use ggplot for the subsequent figures. # create data frames containing residuals and fitted # values for each model we ran above a &lt;- data_frame( model = &quot;random.slope&quot;, fitted = predict(random.slope.model), residual = residuals(random.slope.model)) b &lt;- data_frame( model = &quot;random.intercept&quot;, fitted = predict(slope.model), residual = residuals(slope.model)) # join the two data frames together residual.fitted.data &lt;- bind_rows(a,b) We can see that the residuals from the random slope model are much more evenly distributed across the range of fitted values, which suggests that the assumption of homogeneity of variance is met in the random slope model: # plots residuals against fitted values for each model residual.fitted.data %&gt;% ggplot(aes(fitted, residual)) + geom_point() + geom_smooth(se=F) + facet_wrap(~model) ## `geom_smooth()` using method = &#39;loess&#39; We can plot both of the random effects from this model (intercept and slope) to see how much the model expects individuals to deviate from the overall (mean) slope. # extract the random effects from the model (intercept and slope) ranef(random.slope.model)$Subject %&gt;% # implicitly convert them to a dataframe and add a column with the subject number rownames_to_column(var=&quot;Subject&quot;) %&gt;% # plot the intercept and slobe values with geom_abline() ggplot(aes()) + geom_abline(aes(intercept=`(Intercept)`, slope=Days, color=Subject)) + # add axis label xlab(&quot;Day&quot;) + ylab(&quot;Residual RT&quot;) + # set the scale of the plot to something sensible scale_x_continuous(limits=c(0,10), expand=c(0,0)) + scale_y_continuous(limits=c(-100, 100)) Inspecting this plot, there doesn’t seem to be any strong correlation between the RT value at which an individual starts (their intercept residual) and the slope describing how they change over the days compared with the average slope (their slope residual). That is, we can’t say that knowing whether a person has fast or slow RTs at the start of the study gives us a clue about what will happen to them after they are sleep deprived: some people start slow and get faster; other start fast but suffer and get slower. However we can explicitly check this correlation (between individuals’ intercept and slope residuals) using the VarCorr() function: VarCorr(random.slope.model) ## Groups Name Std.Dev. Corr ## Subject (Intercept) 24.7404 ## Days 5.9221 0.066 ## Residual 25.5918 The correlation between the random intercept and slopes is only 0.066, and so very low. We might, therefore, want to try fitting a model without this correlation. lmer includes the correlation by default, so we need to change the model formula to make it clear we don’t want it: uncorrelated.reffs.model &lt;- lmer( Reaction ~ Days + (1 | Subject) + (0 + Days|Subject), data=lme4::sleepstudy) VarCorr(uncorrelated.reffs.model) ## Groups Name Std.Dev. ## Subject (Intercept) 25.0513 ## Subject.1 Days 5.9882 ## Residual 25.5653 The variance components don’t change much when we constrain the covariance of intercepts and slopes to be zero, and we can explicitly compare these two models using the anova() function, which is somewhat confusingly named because in this instance it is performing a likelihood ratio test to compare the two models: anova(random.slope.model, uncorrelated.reffs.model) ## refitting model(s) with ML (instead of REML) ## Data: lme4::sleepstudy ## Models: ## ..1: Reaction ~ Days + (1 | Subject) + (0 + Days | Subject) ## object: Reaction ~ Days + (Days | Subject) ## Df AIC BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq) ## ..1 5 1762.0 1778.0 -876.00 1752.0 ## object 6 1763.9 1783.1 -875.97 1751.9 0.0639 1 0.8004 Model fit is not significantly worse with the constrained model, so for parsimony’s sake we prefer it to the more complex model. Fitting a curve for the effect of Days In theory, we could also fit additional parameters for the effect of Days, although a combined smoothed line plot/scatterplot indicates that a linear function fits the data reasonably well. lme4::sleepstudy %&gt;% ggplot(aes(Days, Reaction)) + geom_point() + geom_jitter() + geom_smooth() ## `geom_smooth()` using method = &#39;loess&#39; If we insisted on testing a curved (quadratic) function of Days, we could: quad.model &lt;- lmer(Reaction ~ Days + I(Days^2) + (1|Subject), data=lme4::sleepstudy) quad.model.summary &lt;- summary(quad.model) quad.model.summary$coefficients ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 255.4493728 10.4656310 30.04063 24.408406 0.00000000 ## Days 7.4340850 2.9707978 160.00374 2.502387 0.01334034 ## I(Days^2) 0.3370223 0.3177733 160.00374 1.060575 0.29048148 Here, the p value for I(Days^2) is not significant, suggesting (as does the plot) that a simple slope model is sufficient. "],
["icc-and-vpc.html", "Variance partition coefficients and intraclass correlations", " Variance partition coefficients and intraclass correlations The purpose of multilevel models is to partition variance in the outcome between the different groupings in the data. For example, if we make multiple observations on individual participants we partition outcome variance between individuals, and the residual variance. We might then want to know what proportion of the total variance is attributable to variation within-groups, or how much is found between-groups. This statistic is termed the variance partition coefficient VPC, or intraclass correlation. We calculate the VPC woth some simple arithmetic on the variance estimates from the lmer model. We can extract the variance estimates from the VarCorr function: random.intercepts.model &lt;- lmer(Reaction ~ Days + (1|Subject), data=lme4::sleepstudy) VarCorr(random.intercepts.model) ## Groups Name Std.Dev. ## Subject (Intercept) 37.124 ## Residual 30.991 And we can test the variance parameter using the rand() function: rand(random.intercepts.model) ## Analysis of Random effects Table: ## Chi.sq Chi.DF p.value ## Subject 107 1 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Helpfully, if we convert the result of VarCorr to a dataframe, we are provided with the columns vcov which stands for variance or covariance, as well as the sdcor (standard deviation or correlation) which is provided in the printed summary: VarCorr(random.intercepts.model) %&gt;% as_data_frame() ## # A tibble: 2 x 5 ## grp var1 var2 vcov sdcor ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Subject (Intercept) &lt;NA&gt; 1378.1785 37.12383 ## 2 Residual &lt;NA&gt; &lt;NA&gt; 960.4566 30.99123 The variance partition coefficient is simply the variance at a given level of the model, divided by the total variance (the sum of the variance parameters). So we can write: VarCorr(random.intercepts.model) %&gt;% as_data_frame() %&gt;% mutate(icc=vcov/sum(vcov)) %&gt;% select(grp, icc) ## # A tibble: 2 x 2 ## grp icc ## &lt;chr&gt; &lt;dbl&gt; ## 1 Subject 0.5893089 ## 2 Residual 0.4106911 Intraclass correlations were computed from the mixed effects mode. 59% of the variation in outcome was attributable to differences between subjects, \\(\\chi^2(1) = 107\\), p &lt; .001. [It’s not straightforward to put an confidence interval around the VPC estimate from an lmer model. If this is important to you, you should explore re-fitting the same model in a Bayesian framework] "],
["threelevel.html", "3 level models with ‘partially crossed’ random effects", " 3 level models with ‘partially crossed’ random effects The lme4::InstEval dataset records University lecture evaluations by students at ETH Zurich. The variables include: s a factor with levels 1:2972 denoting individual students. d a factor with 1128 levels from 1:2160, denoting individual professors or lecturers. studage an ordered factor with levels 2 &lt; 4 &lt; 6 &lt; 8, denoting student’s “age” measured in the semester number the student has been enrolled. lectage an ordered factor with 6 levels, 1 &lt; 2 &lt; … &lt; 6, measuring how many semesters back the lecture rated had taken place. service a binary factor with levels 0 and 1; a lecture is a “service”, if held for a different department than the lecturer’s main one. dept a factor with 14 levels from 1:15, using a random code for the department of the lecture. y a numeric vector of ratings of lectures by the students, using the discrete scale 1:5, with meanings of ‘poor’ to ‘very good’. For convenience, in this example we take a subsample of the (fairly large) dataset: set.seed(1234) lectures &lt;- sample_n(lme4::InstEval, 10000) We run a model without any predictors, but respecting the clustering in the data, in the example below. This model is a three-level random intercepts model, which splits the variance between lecturers, students, and the residual variance. Because, in some cases, some of the same students provide data on a particular lecturer these data are ‘partially crossed’ (the alternative would be to sample different students for each lecturer). lectures.model &lt;- lmer(y~(1|d)+(1|s), data=lectures) summary(lectures.model) ## summary from lme4 is returned ## some computational error has occurred in lmerTest ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: y ~ (1 | d) + (1 | s) ## Data: lectures ## ## REML criterion at convergence: 33053.3 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.55587 -0.73732 0.05425 0.76974 2.46973 ## ## Random effects: ## Groups Name Variance Std.Dev. ## s (Intercept) 0.08245 0.2871 ## d (Intercept) 0.28066 0.5298 ## Residual 1.38497 1.1768 ## Number of obs: 10000, groups: s, 2692; d, 1076 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 3.23319 0.02373 136.3 As before, we can extract only the variance components from the model, and look at the ICC: VarCorr(lectures.model) %&gt;% as_data_frame() %&gt;% mutate(icc=vcov/sum(vcov)) %&gt;% select(grp, vcov, icc) ## # A tibble: 3 x 3 ## grp vcov icc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s 0.08245431 0.04716828 ## 2 d 0.28066190 0.16055362 ## 3 Residual 1.38497203 0.79227810 And we can add predictors to the model to see if they help explain student ratings: lectures.model.2 &lt;- lmer(y~service*dept+(1|d)+(1|s), data=lectures) anova(lectures.model.2) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## service 10.521 10.5212 1 7351.4 7.6046 0.005836 ** ## dept 15.671 1.2054 13 1153.4 0.8713 0.583610 ## service:dept 25.361 1.9509 13 6399.7 1.4101 0.145724 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Here we can see the service variable does predict evaluations, and we can use the model to estimate the mean and SE for service == 1 or service == 0 (see also the sections on multiple comparisons, followup contrasts, and doing followup contrasts with lmer models for more options here): service.means &lt;- lmerTest::lsmeansLT(lectures.model.2, &quot;service&quot;) service.means$lsmeans.table %&gt;% select(service, Estimate, `Standard Error`) ## service Estimate Standard Error ## service 0 0 3.2846 0.0290 ## service 1 1 3.1694 0.0397 Or change the proportions of variance components at each level (they don’t, much, in this instance): VarCorr(lectures.model.2) %&gt;% as_data_frame() %&gt;% mutate(icc=vcov/sum(vcov)) %&gt;% select(grp, vcov, icc) ## # A tibble: 3 x 3 ## grp vcov icc ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 s 0.08133698 0.04677067 ## 2 d 0.27419184 0.15766674 ## 3 Residual 1.38353063 0.79556258 "],
["contrasts-lmer.html", "Contrasts and followup tests using lmer", " Contrasts and followup tests using lmer Many of the contrasts possible after lm and Anova models are also possible using lmer for multilevel models. Instead of the lsmeans package we can use the lsmeansLT function in the lmerTest package. Let’s say we repeat one of the models used in a previous section, looking at the effect of Days of sleep deprivation on reaction times: sleep &lt;- lme4::sleepstudy %&gt;% mutate(Days=factor(Days)) m &lt;- lmer(Reaction~Days+(1|Subject), data=sleep) anova(m) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## Days 166235 18471 9 153 18.703 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 A quirk of lsmeansLT and factors [One quirk of lsmeansLT is that you can’t tell R that a variable is a factor within a formula. So the model lmer(Reaction~factor(Days)+(1|Subject), data=lme4::sleepstudy) would run perfectly well, and we could even use it with lsmeansLT to get cell means, but would fail if we later tried to calculate contrasts. This is why in the example we transform Days to be a factor in in a copy of the dataframe, and then run the model.]{.tip} We can see a significant effect of Days in the Anova table, and want to compute followup tests. To first estimate cell means and create an lsmeans object, you can use the lsmeans() function in the lsmeans:: package: m.lsm &lt;- lsmeans::lsmeans(m, &quot;Days&quot;) m.lsm ## Days lsmean SE df lower.CL upper.CL ## 0 256.6518 11.45778 41.98 233.5288 279.7748 ## 1 264.4958 11.45778 41.98 241.3727 287.6188 ## 2 265.3619 11.45778 41.98 242.2389 288.4849 ## 3 282.9920 11.45778 41.98 259.8690 306.1150 ## 4 288.6494 11.45778 41.98 265.5264 311.7724 ## 5 308.5185 11.45778 41.98 285.3954 331.6415 ## 6 312.1783 11.45778 41.98 289.0552 335.3013 ## 7 318.7506 11.45778 41.98 295.6276 341.8736 ## 8 336.6295 11.45778 41.98 313.5065 359.7525 ## 9 350.8512 11.45778 41.98 327.7282 373.9742 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 It might be nice to extract these estimates and plot them: m.lsm.df &lt;- m.lsm %&gt;% broom::tidy() m.lsm.df %&gt;% ggplot(aes(Days, estimate, ymin=conf.low, ymax=conf.high)) + geom_pointrange() + ylab(&quot;RT&quot;) If we wanted to compare each day against every other day (i.e. all the pairwise comparisons) we can use contrast(): # results not shown to save space lsmeans::contrast(m.lsm, &#39;tukey&#39;) Or we might want to see if there was a significant change between any consecutive days (it doesn’t look like there was): # results not shown to save space lsmeans::contrast(m.lsm, &#39;consec&#39;, method=&quot;fdr&quot;) Perhaps more interesting in this example is to check the polynomial contrasts, to see if there was a linear or quadratic change in RT over days: # results not shown to save space lsmeans::contrast(m.lsm, &#39;poly&#39;) %&gt;% broom::tidy() %&gt;% head(3) %&gt;% pander(caption=&quot;The first three polynomial contrasts. Note you&#39;d have to have quite a fancy theory to warrant looking at any of the higher level polynomial terms.&quot;) "],
["troubleshooting-multilevel-models.html", "Troubleshooting", " Troubleshooting Convergence problems and simplifying the random effects structure It’s common, when variances and covariances are close to zero, that lmer has trouble fitting your model. The solution is to simplify complex models, removing of constraining some random effects. For example, in an experiment where you have multiple stimuli and different experimental conditions, with many repeated trials, you might end up with data like this: df %&gt;% head() ## # A tibble: 6 x 5 ## trial condition block subject RT ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 1 1 1 1 300.8379 ## 2 2 1 1 1 299.2759 ## 3 3 1 1 1 300.6742 ## 4 4 1 1 1 299.5173 ## 5 5 1 1 1 299.7643 ## 6 6 1 1 1 300.3312 Which you could model with lmer like this: m1 &lt;- lmer(RT ~ block * trial * condition + (block+condition|subject), data=df) You can list the random effects from the model using the VarCorr function: VarCorr(m1) ## Groups Name Std.Dev. Corr ## subject (Intercept) 0.9923367 ## block 0.0071373 -0.312 ## condition 0.0160946 -0.403 -0.744 ## Residual 0.9973866 As VarCorr shows, this model estimates: - random intercepts for subject, - random slopes for trial and condition, and - three covariances between these random effects. If these covariances are very close to zero though, as is often the case, this can cause convergence issues, especially if insufficient data are available. If this occurs, you might want to simplify the model. For example, to remove all the covariances between random effects you might rewrite the model this way: m2 &lt;- lmer(RT ~ block * trial * condition + (1|subject) + (0+block|subject) + (0+condition|subject), data=df) VarCorr(m2) To remove only covariances with the intercept: m3 &lt;- lmer(RT ~ block * trial * condition + (1|subject) + (0+block+condition|subject), data=df) VarCorr(m3) In general, the recommendation is to try and fit a full random effects structure, and simplify it by removing the least theoretically plausible parameters. See: This tutorial on mixed models in linguistics: http://www.bodowinter.com/tutorial/bw_LME_tutorial2.pdf @barr2013random, which recommends you ‘keep it maximal’, meaning that you should keep all random effects terms, including covariances, where this is possible. See this page for lots more examples of more complex mixed models "],
["multilevel-bayes-reasons.html", "Bayesian multilevel models", " Bayesian multilevel models Complex models with many random effects it can be challenging to fit using standard software [see eager2017mixed and @gelman2014bayesian]. Many authors have noted that a Bayesian approach to model fitting can be advantageous for multilevel models. A brief example of fitting multilevel models via MCMC is given in this section: Bayes via MCMC "],
["mediation-and-covariance-modelling.html", "11 Mediation and covariance modelling ", " 11 Mediation and covariance modelling "],
["mediation.html", "Mediation", " Mediation XXX TODO EXPAND This chapter will assume: You know what mediation analyses are and why you want to run them You a basic understanding of the Baron and Kenny approach, but an inkling that there are more up to date ways of testing mediation hypotheses. "],
["covariance-modelling.html", "Covariance modelling", " Covariance modelling The CFA examples were adapted from a guide originally produced by Jon May This section covers path analysis (path models), confirmatory factor analysis (CFA) and structural equation modelling (SEM). You are encouraged to work through the path models and CFA sections, and especially the material on assessing model fit, before tacking SEM. Before you start this either section make sure you have the lavaan package installed (see installing packages). install.packages(lavaan) And we load the package to make all the functions available with minimal typing: library(lavaan) "],
["path-models.html", "Path models", " Path models Path models are an extension of linear regression, but where multiple observed variables can be considered as ‘outcomes’. Because the terminology of outcomes v.s. predictors breaks down when variables can be both outcomes and predictors at the same time, it’s normal to distinguish instead between: Exogenous variables: those which are not predicted by any other Endogenous variables: variables which do have predictors, and may or may not predict other variales Defining a model To define a path model, lavaan requires that you specify the relationships between variables in a text format. A full guide to this lavaan model syntax is available on the project website. For path models the format is very simple, and resembles a series of linear models, written over several lines, but in text rather than as a model formula: # define the model over multiple lines for clarity mediation.model &lt;- &quot; y ~ x + m m ~ x &quot; In this case the ~ symbols just means ‘regressed on’ or ‘is predicted by’. The model in the example above defines that our outcome y is predicted by both x and m, and that x also predicts m. You might recognise this as a mediation model. Make sure you include the closing quote symbol, and also be careful when running the code which defines the model. RStdudio can sometimes get confused and only run some of the lines, leading to errors. The simplest solution is to select the entire block explicitly and run that. To fit the model we pass the model specification and the data to the sem() function: mediation.fit &lt;- sem(mediation.model, data=mediation.df) As we did for linear regression models, we have saved the model fit object into a variable, here named mediation.fit. To display the model results we can use summary(). The key section of the output to check is the table listed ‘Regressions’, which lists the regression parameters for the predictors for each of the endogenous variables. summary(mediation.fit) ## lavaan (0.5-23.1097) converged normally after 12 iterations ## ## Number of observations 200 ## ## Estimator ML ## Minimum Function Test Statistic 0.000 ## Degrees of freedom 0 ## Minimum Function Value 0.0000000000000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.166 0.075 2.198 0.028 ## m 0.190 0.070 2.721 0.007 ## m ~ ## x 0.530 0.067 7.958 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.967 0.097 10.000 0.000 ## .m 0.993 0.099 10.000 0.000 From this table we can see that both x and m are significant predictors of y, and that x also predicts m. This implie that mediation is taking place, but see the mediation chapter for details of testing indirect effects in lavaan. Where’s the intercept? Path analysis is part of the set of techniques often termed ‘covariance modelling’. As the name implies the primary focus here is the relationships between variables, and less so the mean-structure of the variables. In fact, by default the software first creates the covariance matrix of all the variables in the model, and the fit is based only on these values, plus the sample sizes (in early SEM software you typically had to provide the covariance matrix directly, rather than working with the raw data). Nonetheless, because path analysis is an extension of regression techniques it is possible to request that intercepts are included in the model, and means estimated, by adding meanstructure=TRUE to the sem() function (see the lavaan manual for details). In the output below we now also see a table labelled ‘Intercepts’ which gives the mean values of each variable when it’s predictors are zero (just like in linear regression): mediation.fit.means &lt;- sem(mediation.model, meanstructure=T, data=mediation.df) summary(mediation.fit.means) ## lavaan (0.5-23.1097) converged normally after 16 iterations ## ## Number of observations 200 ## ## Estimator ML ## Minimum Function Test Statistic 0.000 ## Degrees of freedom 0 ## Minimum Function Value 0.0000000000000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Regressions: ## Estimate Std.Err z-value P(&gt;|z|) ## y ~ ## x 0.166 0.075 2.198 0.028 ## m 0.190 0.070 2.721 0.007 ## m ~ ## x 0.530 0.067 7.958 0.000 ## ## Intercepts: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 10.629 0.362 29.323 0.000 ## .m 5.097 0.070 72.298 0.000 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .y 0.967 0.097 10.000 0.000 ## .m 0.993 0.099 10.000 0.000 Tables of model coefficients If you want to present results from these models in table format, the parameterEstimates() function is useful to extract the relevant numbers as a dataframe. We can then manipulate and present this table as we would any other dataframe. In the example below we extract the parameter estimates, select only the regression parameters (~) and remove some of the columns to make the final output easier to read: parameterEstimates(mediation.fit.means) %&gt;% as_data_frame() %&gt;% rownames_to_column() %&gt;% mutate(term=paste(lhs, op, rhs)) %&gt;% rename(estimate=est, std.error = se, p.value=pvalue, statistic = z, conf.low=ci.lower, conf.hi=ci.upper) %&gt;% select(term, op, everything(), -lhs, -rhs) ## # A tibble: 9 x 9 ## term op rowname estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 y ~ x ~ 1 0.16570565 0.07539776 2.197753 2.796671e-02 ## 2 y ~ m ~ 2 0.18989637 0.06979726 2.720685 6.514674e-03 ## 3 m ~ x ~ 3 0.52976119 0.06656841 7.958147 1.776357e-15 ## 4 y ~~ y ~~ 4 0.96727307 0.09672731 10.000000 0.000000e+00 ## 5 m ~~ m ~~ 5 0.99275572 0.09927557 10.000000 0.000000e+00 ## 6 x ~~ x ~~ 6 1.12014957 0.00000000 NA NA ## 7 y ~1 ~1 7 10.62875295 0.36247576 29.322658 0.000000e+00 ## 8 m ~1 ~1 8 5.09667420 0.07049575 72.297607 0.000000e+00 ## 9 x ~1 ~1 9 -0.03640346 0.00000000 NA NA ## # ... with 2 more variables: conf.low &lt;dbl&gt;, conf.hi &lt;dbl&gt; lm(mpg~wt, data=mtcars) %&gt;% tidy(conf.int=T) %&gt;% names ## [1] &quot;term&quot; &quot;estimate&quot; &quot;std.error&quot; &quot;statistic&quot; &quot;p.value&quot; &quot;conf.low&quot; ## [7] &quot;conf.high&quot; parameterEstimates(mediation.fit.means) %&gt;% filter(op == &quot;~&quot;) %&gt;% mutate(term = paste(lhs, op, rhs)) %&gt;% select(term, everything(), -se, -lhs, -rhs, -op) %&gt;% pander(caption=&quot;Regression parameters from `mediation.fit`&quot;) Regression parameters from mediation.fit term est z pvalue ci.lower ci.upper y ~ x 0.1657 2.198 0.02797 0.01793 0.3135 y ~ m 0.1899 2.721 0.006515 0.0531 0.3267 m ~ x 0.5298 7.958 1.776e-15 0.3993 0.6602 Diagrams Because describing path, CFA and SEM models in words can be tedious and difficult for readers to follow it is conventional to include a diagram of (at least) your final model, and perhaps also initial or alternative models. The semPlot:: package makes this relatively easy: passing a fitted lavaan model to the semPaths() function produces a line drawing, and gives the option to overlap raw or standardised coefficients over this drawing: # unfortunately semPaths plots very small by default, so we set # some extra parameters to increase the size to make it readable semPlot::semPaths(mediation.fit, &quot;par&quot;, sizeMan = 15, sizeInt = 15, sizeLat = 15, edge.label.cex=1.5, fade=FALSE) "],
["cfa.html", "Confirmatory factor analysis (CFA)", " Confirmatory factor analysis (CFA) Open some data and check that all looks well: hz &lt;- lavaan::HolzingerSwineford1939 hz %&gt;% glimpse() ## Observations: 301 ## Variables: 15 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, ... ## $ sex &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2... ## $ ageyr &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12,... ## $ agemo &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5,... ## $ school &lt;fctr&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, ... ## $ grade &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7... ## $ x1 &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.333... ## $ x2 &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5... ## $ x3 &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875,... ## $ x4 &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.000... ## $ x5 &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5... ## $ x6 &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286, ... ## $ x7 &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.347... ## $ x8 &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4... ## $ x9 &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.500... Defining the model As noted above, to define models in lavaan you must specify the relationships between variables in a text format. A full guide to this lavaan model syntax is available on the project website. For CFA models, like path models, the format is fairly simple, and resembles a series of linear models, written over several lines. In the model below there are three latent variables, visual, writing and maths. The latent variable names are followed by =~ which means ‘is manifested by’, and then the observed variables, our measures for the latent variable, are listed, separated by the + symbol. hz.model &lt;- &#39; visual =~ x1 + x2 + x3 writing =~ x4 + x5 + x6 maths =~ x7 + x8 + x9&#39; Note that we have saved our model specification/syntax in a variable named hz.model. The other special symbols in the lavaan syntax which can be used for CFA models are: a ~~ b, which represents a covariance. a ~~ a, which is a variance (you can think of this as the covariance of a variable with itself) To run the analysis we again pass the model specification and the data to the cfa() function: hz.fit &lt;- cfa(hz.model, data=hz) summary(hz.fit, standardized=TRUE) ## lavaan (0.5-23.1097) converged normally after 35 iterations ## ## Number of observations 301 ## ## Estimator ML ## Minimum Function Test Statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual =~ ## x1 1.000 0.900 0.772 ## x2 0.554 0.100 5.554 0.000 0.498 0.424 ## x3 0.729 0.109 6.685 0.000 0.656 0.581 ## writing =~ ## x4 1.000 0.990 0.852 ## x5 1.113 0.065 17.014 0.000 1.102 0.855 ## x6 0.926 0.055 16.703 0.000 0.917 0.838 ## maths =~ ## x7 1.000 0.619 0.570 ## x8 1.180 0.165 7.152 0.000 0.731 0.723 ## x9 1.082 0.151 7.155 0.000 0.670 0.665 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual ~~ ## writing 0.408 0.074 5.552 0.000 0.459 0.459 ## maths 0.262 0.056 4.660 0.000 0.471 0.471 ## writing ~~ ## maths 0.173 0.049 3.518 0.000 0.283 0.283 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.549 0.114 4.833 0.000 0.549 0.404 ## .x2 1.134 0.102 11.146 0.000 1.134 0.821 ## .x3 0.844 0.091 9.317 0.000 0.844 0.662 ## .x4 0.371 0.048 7.779 0.000 0.371 0.275 ## .x5 0.446 0.058 7.642 0.000 0.446 0.269 ## .x6 0.356 0.043 8.277 0.000 0.356 0.298 ## .x7 0.799 0.081 9.823 0.000 0.799 0.676 ## .x8 0.488 0.074 6.573 0.000 0.488 0.477 ## .x9 0.566 0.071 8.003 0.000 0.566 0.558 ## visual 0.809 0.145 5.564 0.000 1.000 1.000 ## writing 0.979 0.112 8.737 0.000 1.000 1.000 ## maths 0.384 0.086 4.451 0.000 1.000 1.000 Model output The output has three parts: Parameter estimates. The values in the first column are the standardised weights from the observed variables to the latent factors. Factor covariances. The values in the first column are the covariances between the latent factors. Error variances. The values in the first column are the estimates of each observed variable’s error variance. Plotting models As before, we can use the semPaths() function to visualise the model. This is an important step because it helps explain the model to others, and also gives you an opportunity to check you have specified your model correctly. semPlot::semPaths(hz.fit) And for ‘final’ models we might want to overplot model parameter estimates (in this case, standardised): # std refers to standardised estimates. &quot;par&quot; would plot # the unstandardised estimates semPlot::semPaths(hz.fit, &quot;std&quot;) "],
["model-fit.html", "Model fit", " Model fit To examine the model fit we use fitmeasures() and pass a list of the names of the fit indices we would like calculated: library(lavaan) fitmeasures(hz.fit, c(&#39;cfi&#39;, &#39;rmsea&#39;, &#39;rmsea.ci.upper&#39;, &#39;bic&#39;)) ## cfi rmsea rmsea.ci.upper bic ## 0.931 0.092 0.114 7595.339 This looks OK, but the fit indices indicate the model could be improved. In particular the RMSEA figure is above 0.05. See the notes on goodness of fit statistics for more detail. "],
["modification-indices.html", "Modification indices", " Modification indices To examine the modification indices we type: modificationindices(hz.fit) But because this function produces a very long table of output, it can be helpful to sort and filter the rows to show only those model modifications which might be of interes to us. The command below converts the output of modificationindices() to a dataframe. It then: Sorts the rows by the mi column, which represents the change in model \\(\\chi^2\\) we see if the path was included (see sorting) Filters the results to show only those with \\(\\chi^2\\) change &gt; 5 Selects only the lhs, op, rhs, mi, and epc columns. modificationindices(hz.fit) %&gt;% as_data_frame() %&gt;% arrange(-mi) %&gt;% filter(mi &gt; 5) %&gt;% select(lhs, op, rhs, mi, epc) %&gt;% pander(caption=&quot;Largest MI values for hz.fit&quot;) Largest MI values for hz.fit lhs op rhs mi epc visual =~ x9 36.41 0.577 x7 ~~ x8 34.15 0.5364 visual =~ x7 18.63 -0.4219 x8 ~~ x9 14.95 -0.4231 writing =~ x3 9.151 -0.2716 x2 ~~ x7 8.918 -0.1827 writing =~ x1 8.903 0.3503 x2 ~~ x3 8.532 0.2182 x3 ~~ x5 7.858 -0.1301 visual =~ x5 7.441 -0.2099 x1 ~~ x9 7.335 0.1379 x4 ~~ x6 6.22 -0.2348 x4 ~~ x7 5.92 0.09818 x1 ~~ x7 5.42 -0.1291 x7 ~~ x9 5.183 -0.1867 The lhs (left hand side), rhs (right hans side) and op (operation) columns specify what modification should be made. Latent factor to variable links have =~ in the ‘op’ column. Error covariances for observed variables have ~~ as the op. These symbols match the symbols used to describe a path in the lavaan model syntax. If we add the largest MI path to our model it will look like this: # same model, but with x9 now loading on visual hz.model.2 &lt;- &quot; visual =~ x1 + x2 + x3 + x9 writing =~ x4 + x5 + x6 maths =~ x7 + x8 + x9&quot; hz.fit.2 &lt;- cfa(hz.model.2, data=hz) fitmeasures(hz.fit.2, c(&#39;cfi&#39;, &#39;rmsea&#39;, &#39;rmsea.ci.upper&#39;, &#39;bic&#39;)) ## cfi rmsea rmsea.ci.upper bic ## 0.967 0.065 0.089 7568.123 RMSEA has improved somewhat, but we’d probably want to investigate this model further, and make additional improvements to it (although see the notes on model improvements) "],
["model-improvement.html", "Model modification and improvement", " Model modification and improvement Modification indices are a way of improving your model by identifying parameters which, if included, would improve model fit (or constraints removed). However, remember that: Use of modification indices should be informed by theory MI may suggest paths which don’t make substantive sense It’s very important to avoid adding paths in a completely data-driven way because this is almost certain to lead to over-fitting. It’s also important to work one step at a time, because the table of modification indices may change as you add additional paths. For example, the path second largest MI value may change once you add the path with the largest MI to the model. The basic steps to follow are: Run a simple, theoretically-derived model Notice it fits badly Add any additional paths which make theoretical sense Check GOF; If it still fits badly then, Run MI and identify the largest value If this parameter makes theoretical sense, relax the constraint Re-run the model and return to step 4 "],
["sem.html", "Structural eqution modelling (SEM)", " Structural eqution modelling (SEM) Combining Path models and CFA to create structural equation models (SEM) allows researchers to combine allow for measurment imperfection whilst also (attempting to) infer information about causation. SEM involves adding paths to CFA models which are, like predictors in standard regression models, are assumed to be causal in nature; i.e. rather than variables \\(x\\) and \\(y\\) simply covarying with one another, we are prepared to make the assumption that \\(x\\) causes \\(y\\). It’s worth pointing out though, right from the offset, that causal relationships drawn from SEM models always dependent on assumptions we are prepared to make when setting up our model. There is nothing magical in the technique that makes allows us to infer causality from non-experimental data (although note SEM can be used for some experimental analyses). It is only be our substantive knowledge of the domain that makes any kind of causal inference reasonable, and when using SEM the onus is always on us to check our assumptions, provide sensitivity analyses which test alternative causal models, and interpret observational data cautiously. [Note, there are techniques which use SEM as a means to make stronger kinds of causal statements, for example instrumental variable analysis, but even here, inferring causality still requires that we make strong assumptions about the process which generated our data. Nonetheless, with these caveats in mind, SEM can be a useful technique to quantify relationships been observed variables where we have measurement error, and especially where we have a theoretical model linking these observations. Steps to running an SEM Identify and test the fit of a measurement model. This is a CFA model which includes all of your observed variables, arranged in relation to the latent variables you think generated the data, and where covariances between all these latent variables are included. This step many include many rounds of model fitting and modification. Ensure your measurement model fits the data adequately before continuing. Test alternative or simplified measurements models and report where these perform well (e.g. are close in fit to your desired model). SEM models that are based on a poorly fitting measurment model will produce parameter estimates that are imprecise, unstable or both, and you should not proceed unless an adequately fitting measrement model is founds (see this nice discussion, which includes relevant references). Convert your measurement model by removing covariances between latent variables, and including new structural paths. Test model fit, and interpret the paths of interest. Avoid making changes to the measurement part of the model at this stage. Where the model is complex consider adjusting p values to allow for multuple comparisons (if using NHST). Test alternative models (e.g. with paths removed or reversed). Report where alternatives also fit the data. In writing up, provide sufficient detail for other researchers to replicate your analyses, and to follow the logic of the ammendments you make. Ideally share your raw data, but at a minimum share the covariance matrix. Report GOF statistics, and follow published reporting guidelines for SEM. Always include a diagram of your final model (at the least). A worked example: Building from a measurement model to SEM Imagine we have some data from a study that aimed to test the theory of planned behaviour. Researcher measured exercise and intentions, along with multiple measures of attitudes, social norms and percieved behavioural control. tpb.df %&gt;% psych::describe(fast=T) ## vars n mean sd min max range se ## a1 1 469 0.09 1.59 -4.88 5.09 9.97 0.07 ## a2 2 459 -0.05 1.21 -3.85 3.15 7.00 0.06 ## a3 3 487 0.02 1.10 -3.30 3.57 6.88 0.05 ## sn1 4 487 0.02 1.54 -4.86 4.47 9.33 0.07 ## sn2 5 487 0.04 1.20 -3.14 3.12 6.26 0.05 ## sn3 6 487 0.02 1.13 -2.96 4.03 6.99 0.05 ## sn4 7 471 -0.03 1.10 -3.42 2.79 6.20 0.05 ## pc1 8 487 -0.13 1.45 -4.71 4.13 8.84 0.07 ## pc2 9 487 0.02 1.25 -3.99 3.81 7.80 0.06 ## pc3 10 487 -0.08 1.27 -3.30 3.63 6.93 0.06 ## pc4 11 487 0.01 1.04 -3.85 3.34 7.19 0.05 ## pc5 12 487 -0.03 1.19 -3.39 4.19 7.58 0.05 ## intention 13 469 9.97 2.47 3.51 19.61 16.10 0.11 ## exercise 14 487 79.87 18.07 20.00 147.00 127.00 0.82 There were some missing data, but nothing to suggest a systematic pattern. For the moment we continue with standard methods: mice::md.pattern(tpb.df) ## a3 sn1 sn2 sn3 pc1 pc2 pc3 pc4 pc5 exercise sn4 a1 intention a2 ## 416 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 ## 16 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 ## 23 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 ## 11 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 ## 13 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 ## 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 2 ## 2 1 1 1 1 1 1 1 1 1 1 0 1 1 0 2 ## 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1 2 ## 2 1 1 1 1 1 1 1 1 1 1 1 1 0 0 2 ## 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 2 ## 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 3 ## 0 0 0 0 0 0 0 0 0 0 16 18 18 28 80 We start by fitting a measurement model. The model sytax includes lines with =~ separatatig left and right hand side (to define the latents) ~~ to specify latent covariances We are not including exercise and intention yet because these are observed variables only (we don’t have multiple measurements for them) and so they don’t need to be in the measurement model: mes.mod &lt;- &#39; # the &quot;measurement&quot; part, defining the latent variables AT =~ a1 + a2 + a3 + sn1 SN =~ sn1 + sn2 + sn3 + sn4 PBC =~ pc1 + pc2 + pc3 + pc4 + pc5 # note that lavaan automatically includes latent covariances # but we can add here anyway to be explicit AT ~~ SN SN ~~ PBC AT ~~ PBC &#39; We can fit this model to the data like so: mes.mod.fit &lt;- cfa(mes.mod, data=tpb.df) summary(mes.mod.fit) ## lavaan (0.5-23.1097) converged normally after 54 iterations ## ## Used Total ## Number of observations 429 487 ## ## Estimator ML ## Minimum Function Test Statistic 58.845 ## Degrees of freedom 50 ## P-value (Chi-square) 0.183 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) ## AT =~ ## a1 1.000 ## a2 0.408 0.050 8.081 0.000 ## a3 0.312 0.044 7.078 0.000 ## sn1 -0.076 0.205 -0.371 0.711 ## SN =~ ## sn1 1.000 ## sn2 0.509 0.156 3.254 0.001 ## sn3 0.403 0.125 3.219 0.001 ## sn4 0.369 0.115 3.199 0.001 ## PBC =~ ## pc1 1.000 ## pc2 0.732 0.079 9.239 0.000 ## pc3 0.777 0.083 9.366 0.000 ## pc4 0.381 0.061 6.258 0.000 ## pc5 0.674 0.076 8.884 0.000 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) ## AT ~~ ## SN 1.384 0.445 3.108 0.002 ## SN ~~ ## PBC 0.145 0.090 1.606 0.108 ## AT ~~ ## PBC 0.112 0.092 1.223 0.221 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) ## .a1 0.433 0.195 2.214 0.027 ## .a2 1.138 0.085 13.336 0.000 ## .a3 1.040 0.075 13.932 0.000 ## .sn1 1.046 0.214 4.876 0.000 ## .sn2 1.060 0.088 11.975 0.000 ## .sn3 1.025 0.078 13.131 0.000 ## .sn4 0.965 0.072 13.341 0.000 ## .pc1 1.125 0.118 9.570 0.000 ## .pc2 0.959 0.084 11.463 0.000 ## .pc3 1.002 0.089 11.202 0.000 ## .pc4 0.933 0.068 13.788 0.000 ## .pc5 0.976 0.081 12.018 0.000 ## AT 2.155 0.260 8.283 0.000 ## SN 1.503 0.758 1.984 0.047 ## PBC 1.029 0.154 6.692 0.000 And we can assess model fit using fitmeasures. Here we select a subset of the possible fit indices to keep the output manageable. useful.fit.measures &lt;- c(&#39;chisq&#39;, &#39;rmsea&#39;, &#39;cfi&#39;, &#39;aic&#39;) fitmeasures(mes.mod.fit, useful.fit.measures) ## chisq rmsea cfi aic ## 58.845 0.020 0.989 16095.222 This model looks pretty good (see the guide to fit indices), but still check modification indices to identify improvements. If they made theoretical sense we might choose to add paths: modificationindices(mes.mod.fit) %&gt;% as_data_frame() %&gt;% filter(mi&gt;4) %&gt;% arrange(-mi) %&gt;% pander(caption=&quot;Modification indices for the measurement model&quot;) Modification indices for the measurement model lhs op rhs mi epc sepc.lv sepc.all sepc.nox AT =~ sn2 18.38 0.4607 0.6763 0.5619 0.5619 sn1 ~~ sn2 10.16 -0.318 -0.318 -0.1723 -0.1723 AT =~ sn4 9.234 -0.2651 -0.3892 -0.3598 -0.3598 a1 ~~ sn2 4.938 0.1793 0.1793 0.09262 0.09262 a2 ~~ sn2 4.555 0.125 0.125 0.08495 0.08495 However, in this case unless we had substantive reasons to add the paths, it would probably be reasonable to continue with the original model. Measurement model fits, so proceed to SEM Our SEM model adapts the CFA (measurement model), including additional observed variables (e.g. intention and exercise) and any relevant structural paths: sem.mod &lt;- &#39; # this section identical to measurement model AT =~ a1 + a2 + a3 + sn1 SN =~ sn1 + sn2 + sn3 + sn4 PBC =~ pc1 + pc2 + pc3 + pc4 + pc5 # additional structural paths intention ~ AT + SN + PBC exercise ~ intention &#39; We can fit it as before, but now using the sem() function rather than the cfa() function: sem.mod.fit &lt;- cfa(sem.mod, data=tpb.df) The first thing we do is check the model fit: fitmeasures(sem.mod.fit, useful.fit.measures) ## chisq rmsea cfi aic ## 185.866 0.062 0.921 20651.995 RMSEA is slightly higher than we like, so we can check the modification indices: sem.mi &lt;- modificationindices(sem.mod.fit) %&gt;% as_data_frame() %&gt;% arrange(-mi) sem.mi %&gt;% head(6) %&gt;% pander(caption=&quot;Top 6 modification indices for the SEM model&quot;) Top 6 modification indices for the SEM model lhs op rhs mi epc sepc.lv sepc.all sepc.nox PBC ~ exercise 163.5 0.04515 0.04491 0.8218 0.8218 PBC =~ intention 97.11 1.136 1.142 0.4581 0.4581 SN =~ intention 96.11 0.8495 1.104 0.4427 0.4427 exercise ~ PBC 84.5 5.944 5.976 0.3265 0.3265 AT =~ intention 84.29 0.6897 0.994 0.3986 0.3986 PBC ~ intention 78.83 0.252 0.2506 0.6249 0.6249 Interestingly, this model suggests two additional paths involving exercise and the PBC latent: sem.mi %&gt;% filter(lhs %in% c(&#39;exercise&#39;, &#39;PBC&#39;) &amp; rhs %in% c(&#39;exercise&#39;, &#39;PBC&#39;)) %&gt;% pander() lhs op rhs mi epc sepc.lv sepc.all sepc.nox PBC ~ exercise 163.5 0.04515 0.04491 0.8218 0.8218 exercise ~ PBC 84.5 5.944 5.976 0.3265 0.3265 Of these suggested paths, the largest MI is for the one which says PBC is predicted by exercise. However, the model would also be improved by allowing PBC to predict exercise. Which should we add? The answer will depend on both previous theory and knowledge of the data. If it were the case that exercise was measured at a later time point than PBC. In this case the decision is reasonably clear, because the temporal sequencing of observations would determine the most likely path. These data were collected contemporaneously, however, and so we can’t use our design to differentiate the causal possibilities. Another consideration would be that, by adding a path from exercise to PBC we would make the model non-recursive, and likely non-identified. A theorist might also argue that because previous studies, and the theory of planned behaviour itself, predict that PBC may exert a direct influence on behaviour, we should add the path with the smaller MI (so allow PBC to predict exercise). In this case, the best course of action would probably be to report the theoretically implied model, but also test alternative models in which causal relations between the variables are reversed or otherwise altered (along with measures of fit and key parameter estimates). The discussion of your paper would then make the case for your preferred account, but make clear that the data were (most likely) unable to provide a persuasive case either way, and that alternative explanations cannot be ruled out. Interpreting and presenting key parameters One of the best ways to present estimates from your final model is in a diagram, because this is intutive and provides a simple way for readers to comprehend the paths implied by your model. We can automatically generate a plot from a fitted model using semPaths(). Here, the what='std' is requesting standardised parameter estimates be shown. Adding residuals=F hides variances of observed and latent variables, which are not of interest here. The line thicknesses are scaled to represent the size parameter itself: semPlot::semPaths(sem.mod.fit, what=&#39;std&#39;, residuals=F) For more information on reporting SEM however, see @schreiber2006reporting. "],
["identification.html", "‘Identification’ in CFA and SEM", " ‘Identification’ in CFA and SEM Identification refers to the idea that a model is ‘estimable’, or more specifically whether there is a single best solution for the parameters specified in the model. An analogy would be the ‘line of best fit’ in regression - if we could draw two lines that fit the data equally well then our method doesn’t enable us to choose between these possibilities, and is essentially meaningless (or uninterpretable, anyway). This is a complex topic, but David Kenny has an excellent page here which covers identification in lots of detail: http://davidakenny.net/cm/identify.htm. Some of the key ideas to takeaway are: Feedback loops and other non-recursive models are likely to cause problems without special attention. Latent variables need a scale. To do this either fix their variance, or fix a factor loading to 1. You need ‘enough data’. Normally this will be at least 3 measured variables per latent. Sometimes 2 is enough, provided the errors of these variables are uncorrelated, but you may struggle to fit models because of ‘empirical under-identification’11 If a model is non-identified, it may either i) fail to run or, worse, ii) produce spurious results. Rule B For structural models, ‘Rule B’ also applies when deciding when a model is identified: No more than one of the following statements should be true about variables or latents in your model: X directly causes Y Y directly causes X X and Y have a correlated disturbance X and Y are correlated exogenous variables But see http://davidakenny.net/cm/identify_formal.htm#RuleB for a proper explanation. Note, indicators themselves should be correlated with one another in a bivariate correlation matrix. It’s only the errors which should be uncorrelated.↩ "],
["cfa-sem-missing-data.html", "Missing data", " Missing data If you have missing data you can use the missing = &quot;ML&quot; argument to ask lavaan to estimate the ‘full information maximum likelihood’ (see http://lavaan.ugent.be/tutorial/est.html). # fit ML model including mean structure to make comparable with FIML fit below # (means are always included with FIML model fits) sem.mod.fit &lt;- sem(sem.mod, data=tpb.df, meanstructure=TRUE) # fit again including missing data also sem.mod.fit.fiml &lt;- sem(sem.mod, data=tpb.df, missing=&quot;ML&quot;) It doesn’t look like the parameter estimates change much. To compare them explicitly we can extract the relevant coefficients from each (they don’t look all that different): bind_cols(parameterestimates(sem.mod.fit) %&gt;% select(lhs, op, rhs, est, pvalue) %&gt;% rename(ml=est, ml.p = pvalue), parameterestimates(sem.mod.fit.fiml) %&gt;% transmute(fiml=est, fiml.p = pvalue)) %&gt;% # select only the regression paths filter(op==&quot;~&quot;) %&gt;% as_huxtable() %&gt;% set_caption(&quot;Comparison of ML and MLM parameter estimates.&quot;) %&gt;% print_md() ## ---------------------------------------------------------------------------- ## intention ~ AT 0.28 0.11 0.32 0.05 ## ---------- ---------- ---------- ---------- ---------- ---------- ---------- ## intention ~ SN 0.45 0.04 0.48 0.02 ## ## intention ~ PBC 1.01 0.00 1.04 0.00 ## ## exercise ~ intention 5.76 0.00 5.74 0.00 ## ## ---------------------------------------------------------------------------- ## Table: Comparison of ML and MLM parameter estimates. "],
["gof.html", "Goodness of fit statistics in CFA", " Goodness of fit statistics in CFA It’s worth noting that many ‘goodness of fit’ statistics are misnamed and are in fact indexing ‘badness of fit’. This applies to RMSEA, \\(\\chi^2\\), BIC, AIC and others. However all of these indices are trying to solve similar problems in subtly different ways. The problem is that we would like a model which: Fits the data we have and also Predicts new data You might think that these goals would be aligned and that a model which fits the data we have would also be good ad predicting new data, but this isn’t the case. In fact, if we overfit our current data we won’t be able to predict new observations very accurately. How fit indices work There is a tradeoff involved to avoid over-fitting the data, and most fit indices attempt to: Quantify how well the model fits the current data but Penalise models which use many parameters (i.e. those in danger of overfitting) Each formula for a goodness of fit statistic represents a different tradeoff between these goals. Model fit statistics are useful but can be misleading and misused. See David Kenny’s page on model fit for more details: http://davidakenny.net/cm/fit.htm Below are some of the most useful and commonly reported GOF statistics for CFA and SEM models: Root Mean Square Error of Approximation (RMSEA) MacCallum, Browne and Sugawara (1996) have used 0.01, 0.05, and 0.08 to indicate excellent, good, and mediocre fit, respectively. RMSEA &lt; .05 often used as a cutoff for a reasonably fitting model, athough others suggest .1. RMSEA is also used to calculate the ‘probability of a close fit’ or pclose statistic — this is the probability that the RMSEA is under 0.05. Comparative fit index (CFI) CFI (and the related TLI) assesses the relative improvement in fit of your model compared with the baseline model. CFI ranges between 0 and 1. The conventional (rule of thumb) threshold for a good fitting model is for CFI to be &gt; .9 Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) The AIC and BIC are measures of comparative fit, so can be used when models are non-nested (and therefore otherwise not easily comparable). AIC is particularly attractive because it corresponds to a measure of predictive accuracy. That is, selecting the model with the smallest AIC is one way of asking: “which model is most likely to accurately predict new data?” Factors which can influence fit statistics All of the following can influence or bias fit statistics: Number of variables (although note RMSEA trends to reduce with more parameters included, but other fit statistics will increase). Model complexity (different statistics reward parsimony to different degrees). Sample size (varies by statistic: some increase and others decrease with sample size). Non-normality of outcome data will (tend to) worsen fit. Which statistics should you report? When reporting absolute model fit, RMSEA and CFI are the most widely reported, and are probably sufficient. However, you should almost never just report a single model, and so: When comparing nested models you should report the \\(\\chi^2\\) lrtest. When comparing non-nested models you should also report differences in BIC and AIC. Further reading This set of slides on model fit provides all fo the formulae and an explanation for many different fit indices: http://www.psych.umass.edu/uploads/people/79/Fit_Indices.pdf "],
["bayes-mcmc.html", "12 Baysian model fitting", " 12 Baysian model fitting Baysian fitting of linear models via MCMC methods This is a minimal guide to fitting and interpreting regression and multilevel models via MCMC. For much more detail, and a much more comprehensive introduction to modern Bayesian analysis see Jon Kruschke’s Doing Bayesian Data Analysis. Let’s revisit our previous example which investigated the effect of familiar and liked music on pain perception: painmusic &lt;- readRDS(&#39;data/painmusic.RDS&#39;) painmusic %&gt;% ggplot(aes(liked, with.music - no.music, group=familiar, color=familiar)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_se) + stat_summary(geom=&quot;line&quot;, fun.data=mean_se) + ylab(&quot;Pain (VAS) with.music - no.music&quot;) + scale_color_discrete(name=&quot;&quot;) + xlab(&quot;&quot;) # set sum contrasts options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) pain.model &lt;- lm(with.music ~ no.music + familiar * liked, data=painmusic) summary(pain.model) ## ## Call: ## lm(formula = with.music ~ no.music + familiar * liked, data = painmusic) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.5397 -1.0123 -0.0048 0.9673 4.8882 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.55899 0.40126 3.885 0.000177 *** ## no.music 0.73588 0.07345 10.019 &lt; 2e-16 *** ## familiar1 0.20536 0.13895 1.478 0.142354 ## liked1 0.30879 0.13900 2.222 0.028423 * ## familiar1:liked1 -0.18447 0.13983 -1.319 0.189909 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.47 on 107 degrees of freedom ## Multiple R-squared: 0.5043, Adjusted R-squared: 0.4858 ## F-statistic: 27.22 on 4 and 107 DF, p-value: 1.378e-15 Do the same thing again, but with with MCMC using Stan: library(rstanarm) options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) pain.model.mcmc &lt;- stan_lm(with.music ~ no.music + familiar * liked, data=painmusic, prior=NULL) summary(pain.model.mcmc) ## ## Model Info: ## ## function: stan_lm ## family: gaussian [identity] ## formula: with.music ~ no.music + familiar * liked ## algorithm: sampling ## priors: see help(&#39;prior_summary&#39;) ## sample: 4000 (posterior sample size) ## num obs: 112 ## ## Estimates: ## mean sd 2.5% 25% 50% 75% 97.5% ## (Intercept) 1.7 0.4 0.9 1.4 1.7 2.0 2.5 ## no.music 0.7 0.1 0.6 0.7 0.7 0.8 0.9 ## familiar1 0.2 0.1 -0.1 0.1 0.2 0.3 0.5 ## liked1 0.3 0.1 0.0 0.2 0.3 0.4 0.6 ## familiar1:liked1 -0.2 0.1 -0.5 -0.3 -0.2 -0.1 0.1 ## sigma 1.5 0.1 1.3 1.4 1.5 1.5 1.7 ## log-fit_ratio 0.0 0.1 -0.1 0.0 0.0 0.0 0.1 ## R2 0.5 0.1 0.4 0.4 0.5 0.5 0.6 ## mean_PPD 5.3 0.2 4.9 5.2 5.3 5.5 5.7 ## log-posterior -205.9 2.3 -211.3 -207.3 -205.6 -204.3 -202.6 ## ## Diagnostics: ## mcse Rhat n_eff ## (Intercept) 0.0 1.0 1176 ## no.music 0.0 1.0 1156 ## familiar1 0.0 1.0 4000 ## liked1 0.0 1.0 4000 ## familiar1:liked1 0.0 1.0 3583 ## sigma 0.0 1.0 2892 ## log-fit_ratio 0.0 1.0 1718 ## R2 0.0 1.0 1443 ## mean_PPD 0.0 1.0 4000 ## log-posterior 0.1 1.0 1025 ## ## For each parameter, mcse is Monte Carlo standard error, n_eff is a crude measure of effective sample size, and Rhat is the potential scale reduction factor on split chains (at convergence Rhat=1). Posterior probabilities for parameters library(bayesplot) mcmc_areas(as.matrix(pain.model.mcmc), regex_pars = &#39;familiar|liked&#39;, prob = .9) mcmc_intervals(as.matrix(pain.model.mcmc), regex_pars = &#39;familiar|liked&#39;, prob_outer = .9) Credible intervals Credible intervals are distinct from confidence intervals TODO EXPAND mHPDI &lt;- function(l){ # median and HPDI # this utility function used to return a dataframe, which is required when using # dplyr::do() below ci = rethinking::HPDI(l, prob=.95) data_frame(median=median(l), lower=ci[1], upper=ci[2]) } params.of.interest &lt;- pain.model.mcmc %&gt;% as_tibble %&gt;% reshape2::melt() %&gt;% filter(stringr::str_detect(variable, &quot;famil|liked&quot;)) %&gt;% group_by(variable) params.of.interest %&gt;% do(., mHPDI(.$value)) %&gt;% pander::pandoc.table(caption=&quot;Estimates and 95% credible intervals for the parameters of interest&quot;) ## ## ------------------------------------------------- ## variable median lower upper ## ------------------ --------- ---------- --------- ## familiar1 0.1942 -0.07615 0.4441 ## ## liked1 0.299 0.02158 0.5685 ## ## familiar1:liked1 -0.1821 -0.4562 0.09123 ## ------------------------------------------------- ## ## Table: Estimates and 95% credible intervals for the parameters of interest Bayesian ‘p values’ for parameters We can do simple arithmetic with the posterior draws to calculate the probability a parameter is greater than (or less than) zero: params.of.interest %&gt;% summarise(estimate=mean(value), `p (x&lt;0)` = mean(value &lt; 0), `p (x&gt;0)` = mean(value &gt; 0)) ## # A tibble: 3 x 4 ## variable estimate `p (x&lt;0)` `p (x&gt;0)` ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 familiar1 0.1954239 0.06725 0.93275 ## 2 liked1 0.2970875 0.01975 0.98025 ## 3 familiar1:liked1 -0.1808220 0.90250 0.09750 Or if you’d like the Bayes Factor (evidence ratio) for one hypotheses vs another, for example comparing the hypotheses that a parameter is &gt; vs. &lt;= 0, then you can use the hypothesis function in the brms package: pain.model.mcmc.df &lt;- pain.model.mcmc %&gt;% as_tibble brms::hypothesis(pain.model.mcmc.df, c(&quot;familiar1 &gt; 0&quot;, &quot;liked1 &gt; 0&quot;, &quot;familiar1:liked1 &lt; 0&quot;)) ## Hypothesis Tests for class : ## Estimate Est.Error l-95% CI u-95% CI Evid.Ratio ## (familiar1) &gt; 0 0.20 0.13 -0.02 Inf 13.87 ## (liked1) &gt; 0 0.30 0.14 0.07 Inf 49.63 ## (familiar1:liked1) &lt; 0 -0.18 0.14 -Inf 0.05 9.26 ## Star ## (familiar1) &gt; 0 ## (liked1) &gt; 0 * ## (familiar1:liked1) &lt; 0 ## --- ## &#39;*&#39;: The expected value under the hypothesis lies outside the 95% CI. Here although we only have a ‘significant’ p value for one of the parameters, we can also see there is “very strong” evidence that familiarity also influences pain, and “strong” evidence for the interaction of familiarity and liking, according to conventional rules of thumb when interpreting Bayes Factors. TODO - add a fuller explanation of why multiple comparisons are not an issue for Bayesian analysis [@gelman2012we], because p values do not have the same interpretation in terms of long run frequencies of replication; they are a representation of the weight of the evidence in favour of a hypothesis. TODO: Also reference Zoltan Dienes Bayes paper. "],
["power-analysis.html", "13 Power analysis", " 13 Power analysis For most inferential statistics If you want to do power analysis for a standard statistical test, e.g. t-tests, chi2 or Anova, the pwr:: package is what you need. This guide has a good walkthrough. For multilevel or generalised linear models If you’d like to run power analyses for linear mixed models (multilevel models) then you need the simr:: package. It has some neat features for calculating power by simulating data and results from a model you specify. To take a simple example, let’s fabricate some data where outcomes y are nested within groups g, and where there is a covariate of interest x. That is, we are interested in estimating our power to detect an effect of x. Note though that in this simulated data, we create outcomes (y) with no relationship to x, and no clustering by g at this stage (i.e. y, x and g are uncorrelated). set.seed(1234) simulated.df &lt;- data_frame( x = rep(0:1, 50), g = rep(1:10, 10), y = rnorm(100) ) GGally::ggpairs(simulated.df) To use simr:: we first run our ‘model of interest’. In this case it’s a random-intercepts multilevel model. We can verify x is unrelated to outcome with lmerTest::anova: library(lmerTest) ## ## Attaching package: &#39;lmerTest&#39; ## The following object is masked from &#39;package:lme4&#39;: ## ## lmer ## The following object is masked from &#39;package:stats&#39;: ## ## step model.of.interest &lt;- lmer(y ~ x + (1|g), data=simulated.df) anova(model.of.interest) ## Analysis of Variance Table of type III with Satterthwaite ## approximation for degrees of freedom ## Sum Sq Mean Sq NumDF DenDF F.value Pr(&gt;F) ## x 0.077811 0.077811 1 98 0.07641 0.7828 The next step is to modify our saved model of interest. We tweak the fitted parameters to represent our predicted values for effect sizes, variances, and covariances. First, let’s change the ‘true’ effect of x to be 0.2: fixef(model.of.interest)[&#39;x&#39;] &lt;- .2 We now use the powerSim function to use our tweaked model to: create a dataset using the parameters of our model (i.e. make random draws of y which relate to g and x as specified in the model summary). re-run lmer on this simulated data. repeat this hundreds or thousands of times count how many times (i.e., for what proportion) we get a significant p value power.sim &lt;- powerSim(model.of.interest, nsim=100) ## Simulating: | |Simulating: |= |Simulating: |== |Simulating: |=== |Simulating: |==== |Simulating: |===== |Simulating: |====== |Simulating: |======= |Simulating: |======== |Simulating: |========= |Simulating: |========== |Simulating: |=========== |Simulating: |============ |Simulating: |============= |Simulating: |============== |Simulating: |=============== |Simulating: |================ |Simulating: |================= |Simulating: |================== |Simulating: |=================== |Simulating: |==================== |Simulating: |===================== |Simulating: |====================== |Simulating: |======================= |Simulating: |======================== |Simulating: |========================= |Simulating: |========================== |Simulating: |=========================== |Simulating: |============================ |Simulating: |============================= |Simulating: |============================== |Simulating: |=============================== |Simulating: |================================ |Simulating: |================================= |Simulating: |================================== |Simulating: |=================================== |Simulating: |==================================== |Simulating: |===================================== |Simulating: |====================================== |Simulating: |======================================= |Simulating: |======================================== |Simulating: |========================================= |Simulating: |========================================== |Simulating: |=========================================== |Simulating: |============================================ |Simulating: |============================================= |Simulating: |============================================== |Simulating: |=============================================== |Simulating: |================================================ |Simulating: |================================================= |Simulating: |================================================== |Simulating: |=================================================== |Simulating: |==================================================== |Simulating: |===================================================== |Simulating: |====================================================== |Simulating: |======================================================= |Simulating: |======================================================== |Simulating: |========================================================= |Simulating: |========================================================== |Simulating: |=========================================================== |Simulating: |============================================================ |Simulating: |=============================================================| power.sim ## Power for predictor &#39;x&#39;, (95% confidence interval): ## 11.00% ( 5.62, 18.83) ## ## Test: unknown test ## Effect size for x is 0.20 ## ## Based on 100 simulations, (0 warnings, 0 errors) ## alpha = 0.05, nrow = 100 ## ## Time elapsed: 0 h 0 m 26 s Our observed power (proportion of times we get a significant p value) is very low here, so we might want increase our hypothesised effect of x, for example to see what power we have to detect an effect of x = .8: fixef(model.of.interest)[&#39;x&#39;] &lt;- .8 power.sim &lt;- powerSim(model.of.interest, nsim=100) ## Simulating: | |Simulating: |= |Simulating: |== |Simulating: |=== |Simulating: |==== |Simulating: |===== |Simulating: |====== |Simulating: |======= |Simulating: |======== |Simulating: |========= |Simulating: |========== |Simulating: |=========== |Simulating: |============ |Simulating: |============= |Simulating: |============== |Simulating: |=============== |Simulating: |================ |Simulating: |================= |Simulating: |================== |Simulating: |=================== |Simulating: |==================== |Simulating: |===================== |Simulating: |====================== |Simulating: |======================= |Simulating: |======================== |Simulating: |========================= |Simulating: |========================== |Simulating: |=========================== |Simulating: |============================ |Simulating: |============================= |Simulating: |============================== |Simulating: |=============================== |Simulating: |================================ |Simulating: |================================= |Simulating: |================================== |Simulating: |=================================== |Simulating: |==================================== |Simulating: |===================================== |Simulating: |====================================== |Simulating: |======================================= |Simulating: |======================================== |Simulating: |========================================= |Simulating: |========================================== |Simulating: |=========================================== |Simulating: |============================================ |Simulating: |============================================= |Simulating: |============================================== |Simulating: |=============================================== |Simulating: |================================================ |Simulating: |================================================= |Simulating: |================================================== |Simulating: |=================================================== |Simulating: |==================================================== |Simulating: |===================================================== |Simulating: |====================================================== |Simulating: |======================================================= |Simulating: |======================================================== |Simulating: |========================================================= |Simulating: |========================================================== |Simulating: |=========================================================== |Simulating: |============================================================ |Simulating: |=============================================================| power.sim ## Power for predictor &#39;x&#39;, (95% confidence interval): ## 95.00% (88.72, 98.36) ## ## Test: unknown test ## Effect size for x is 0.80 ## ## Based on 100 simulations, (0 warnings, 0 errors) ## alpha = 0.05, nrow = 100 ## ## Time elapsed: 0 h 0 m 27 s We might also want to set one of the variance parameters of our model to represent clustering within-g. First we can use VarCorr() to check the variance parameters of the model we just ran: VarCorr(model.of.interest) ## Groups Name Std.Dev. ## g (Intercept) 1.1165e-08 ## Residual 1.0091e+00 And we could simulate increasing the variance parameter for g to 0.5: VarCorr(model.of.interest)[&#39;g&#39;] &lt;- .5 power.sim &lt;- powerSim(model.of.interest, nsim=100) ## Simulating: | |Simulating: |= |Simulating: |== |Simulating: |=== |Simulating: |==== |Simulating: |===== |Simulating: |====== |Simulating: |======= |Simulating: |======== |Simulating: |========= |Simulating: |========== |Simulating: |=========== |Simulating: |============ |Simulating: |============= |Simulating: |============== |Simulating: |=============== |Simulating: |================ |Simulating: |================= |Simulating: |================== |Simulating: |=================== |Simulating: |==================== |Simulating: |===================== |Simulating: |====================== |Simulating: |======================= |Simulating: |======================== |Simulating: |========================= |Simulating: |========================== |Simulating: |=========================== |Simulating: |============================ |Simulating: |============================= |Simulating: |============================== |Simulating: |=============================== |Simulating: |================================ |Simulating: |================================= |Simulating: |================================== |Simulating: |=================================== |Simulating: |==================================== |Simulating: |===================================== |Simulating: |====================================== |Simulating: |======================================= |Simulating: |======================================== |Simulating: |========================================= |Simulating: |========================================== |Simulating: |=========================================== |Simulating: |============================================ |Simulating: |============================================= |Simulating: |============================================== |Simulating: |=============================================== |Simulating: |================================================ |Simulating: |================================================= |Simulating: |================================================== |Simulating: |=================================================== |Simulating: |==================================================== |Simulating: |===================================================== |Simulating: |====================================================== |Simulating: |======================================================= |Simulating: |======================================================== |Simulating: |========================================================= |Simulating: |========================================================== |Simulating: |=========================================================== |Simulating: |============================================================ |Simulating: |=============================================================| power.sim ## Power for predictor &#39;x&#39;, (95% confidence interval): ## 33.00% (23.92, 43.12) ## ## Test: unknown test ## Effect size for x is 0.80 ## ## Based on 100 simulations, (0 warnings, 0 errors) ## alpha = 0.05, nrow = 100 ## ## Time elapsed: 0 h 0 m 26 s Because the amount of clustering in our data has increased our statistical power has gone down. This is because, when clustering is present, each new observation (row) in the dataset provides less new information to estimate our treatment effect. Note that in this example we increased the variance associated with g by quite a lot: setting the variance of g to 0.5 equates to an ICC for g of .33 (because .5 / (.5 + 1) = .33; see the section on calculating ICCs and VPCs) For more details of simr see: http://onlinelibrary.wiley.com/doi/10.1111/2041-210X.12504/full Note that for real applications you would want to set nsim to something reasonably large. Certainly at least 1000 simulations, and perhaps up to ~5000. "],
["learning-key-patterns.html", "14 Learning key patterns", " 14 Learning key patterns Statistics courses teach statistics, and software training or programming courses can teach you the fundamentals of a particular package — for example R. However, a large part of learning how to analyse data is actually not about particular statistical techniques, nor the detals of implementing them in a particular package. Instead, the most useful things to take away from a course can be how to think about different types of problem, and general strategies for tackling them. One label applied to these approaches and strategies are ‘patterns’. We’ve already come across one pattern in the section on summarising data: the split, apply, combine method. This section outlines some other patterns or ways of working that may be helpful. "],
["understanding-interactions.html", "15 Unpicking interactions", " 15 Unpicking interactions Objectives of this section: Clarify/recap what an interaction is Appreciate the importance of visualising interactions Compare different methods of plotting interactions in raw data Visualise interactions based on statistical model predictions Deal with cases where predictors are both categorical and continuous (or a mix) What is an interaction? For an interaction to occur we must measure: An outcome: severity of injury in a car crash, for example. At least 2 predictors of that outcome: e.g. age and gender. Let’s think of a scenario where we’ve measured severity of injury after road accidents, along with the age and gender of the drivers involved. Let’s assume12: Women are likely to be more seriously injured than men in a crash (a +10 point increase in severity) Drivers over 60 are more likely to injured than younger drivers (+10 point severity vs &lt;60 years) For an interaction to occur we have to show that, for example: If you ware old and also female then you will be more severely injured This increase in severity of injury is more than we would expect simply by adding the effects for being female (+10 points) and for being over 60 (+10 points). That is, if an interaction occurs the risk of being older and female is &gt; a 20 point increase in severity. Think of some other example of interactions from your own work. Interactions capture the idea that the effect of one predictor changes depending on the value of another predictor. Visualising interactions from raw data In the previous section we established that interactions capture the idea that the effect of one predictor changes depending on the value of another predictor. We can see this illustrated in the traditional bar plot below. In the left panel we see a dummy dataset in which there is no interaction; in the right panel are data which do show evidence of an interaction: Figure 5.1: Bar plot of injury severity by age and gender. Howeber this bar plot might be better if it were re-drawn as a point and line plot: Figure 15.1: Point and line plot of injury severity by age and gender. The reason the point and line plot improves on the bars for a number of reasons: Readers tend to misinterpret bar plots by assuming that values ‘above’ the bar are less likely than values contained ‘within’ the bar, when this is not the case [@newman2012bar]. The main effects are easy to distinguish in the line plot: just ask yourself if the lines are horizontal or not, and whether they are separated vertically. In contrast, reading the interaction from the bar graph requires that we average pairs of bars (sometimes not adjacent to one another) and compare them - a much more difficult mental operation. The interaction is easy to spot: Ask yourself if the lines are parallel. If they are parallel then the difference between men and women is constant for individuals of different ages. A painful example Before setting out to test for an interaction using some kind of statistical model, it’s always a good idea to first visualise the relationships between outcomes and predictors. A student dissertation project investigated the analgesic quality of music during an experimental pain stimulus. Music was selected to be either liked (or disliked) by participants and was either familiar or unfamiliar to them. Pain was rated without music (no.music) and with music (with.music) using a 10cm visual analog scale anchored with the labels “no pain” and “worst pain ever”. painmusic &lt;- readRDS(&#39;data/painmusic.RDS&#39;) painmusic %&gt;% glimpse ## Observations: 112 ## Variables: 4 ## $ liked &lt;fctr&gt; Disliked, Disliked, Liked, Disliked, Liked, Liked,... ## $ familiar &lt;fctr&gt; Familiar, Unfamiliar, Familiar, Familiar, Familiar... ## $ no.music &lt;dbl&gt; 4, 4, 6, 5, 3, 2, 6, 6, 7, 2, 7, 3, 5, 7, 6, 3, 7, ... ## $ with.music &lt;dbl&gt; 7, 8, 7, 3, 3, 1, 6, 8, 9, 8, 7, 5, 7, 8, 4, 5, 4, ... Before running inferential tests, it would be helpful to see if the data are congruent with the study prediction that liked and familiar music would be more effective at reducing pain than disliked or unfamiliar music We can do this in many different ways. The most common (but not the best) choice would be a simple bar plot, which we can create using the stat_summary() function from ggplot2. painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = familiar, y=change.in.pain)) + facet_wrap(~liked) + stat_summary(geom=&quot;bar&quot;) + xlab(&quot;&quot;) This gives a pretty clear indication that something is going on, but we have no idea about the distribution of the underlying data, and so how much confidence we should place in the finding. We are also hiding distributional information that could be useful to check that assumptions of models we run later are also met (for example of equal variances between groups). If we want to preserve more information about the underlying distribution we can use density plots, boxplots, or pointrange plots, among others. Here we use a grouped density plot. The interaction() function is used to automatically create a variable with the 4 possible groupings we can make when combining theliked and familiar variables: painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = change.in.pain, color = interaction(familiar:liked))) + geom_density() + scale_color_discrete(name=&quot;&quot;) And here we use a boxplot to achieve similar ends: painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = interaction(familiar:liked), y = change.in.pain)) + geom_boxplot() + geom_hline(yintercept = 0, linetype=&quot;dotted&quot;) + xlab(&quot;&quot;) The advantage of these last two plots is that they preserve quite a bit of infrmation about the variable of interest. However, they don’t make it easy to read the main effects and interaction as we saw for the point-line plot above. We can combine some benefits of both plots by adding an error bar to the point-line plot: painmusic %&gt;% ggplot(aes(liked, with.music - no.music, group=familiar, color=familiar)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_se) + stat_summary(geom=&quot;line&quot;, fun.data=mean_se) + ylab(&quot;Pain (VAS) with.music - no.music&quot;) + scale_color_discrete(name=&quot;&quot;) + xlab(&quot;&quot;) This plot doesn’t include all of the information about the distribution of effects that the density or boxplots do (for example, we can’t see any asymmetry in the distributions any more), but we still get some information about the variability of the effect of the experimental conditions on pain by plotting the SE of the mean over the top of each point13 At this point, especially if your current data include only categorical predictors, you might want to move on to the section on making predictions from models and visualising these. Continuous predictors XXX TODO User modelr::gather_predictions to plot This example is loosely based on figures reported by @kockelman2002driver↩ We could equally well plot the 95% confidence interval for the mean, or the interquartile range)↩ "],
["predictions-and-margins.html", "16 Making predictions", " 16 Making predictions Objectives of this section: Distingish predicted means (predictions) from predicted effects (‘margins’) Calculate both predictions and marginal effects for a lm() Plot predictions and margins Think about how to plot effects in meaningful ways Predictions vs margins Before we start, let’s consider what we’re trying to achieve in making predictions from our models. We need to make a distinction between: Predicted means Predicted effects or marginal effects Consider the example used in a previous section where we measured injury.severity after road accidents, plus two predictor variables: gender and age. Predicted means ‘Predicted means’ (or predictions) refers to our best estimate for each category of person we’re interested in. For example, if age were categorical (i.e. young vs. older people) then might have 4 predictions to calculate from our model: Age Gender mean Young Male ? Old Male ? Young Female ? Old Female ? And as before, we might plot these data: Figure 5.1: Point and line plot of injury severity by age and gender. This plot uses the raw data, but these points could equally have been estimated from a statistical model which adjusted for other predictors. Effects (margins) Terms like: predicted effects, margins or marginal effects refer, instead, to the effect of one predictor. There may be more than one marginal effect because the effect of one predictor can change across the range of another predictor. Extending the example above, if we take the difference between men and women for each category of age, we can plot these differences. The steps we need to go through are: Reshape the data to be wide, including a separate column for injury scores for men and women Subtract the score for men from that of women, to calculate the effect of being female Plot this difference score margins.plot &lt;- inter.df %&gt;% # reshape the data to a wider format reshape2::dcast(older~female) %&gt;% # calculate the difference between men and women for each age mutate(effect.of.female = Female - Male) %&gt;% # plot the difference ggplot(aes(older, effect.of.female, group=1)) + geom_point() + geom_line() + ylab(&quot;Effect of being female&quot;) + xlab(&quot;&quot;) + geom_hline(yintercept = 0) margins.plot As before, these differences use the raw data, but could have been calculated from a statistical model. In the section below we do this, making predictions for means and marginal effects from a lm(). Continuous predictors In the examples above, our data were all categorical, which mean that it was straightforward to identify categories of people for whom we might want to make a prediction (i.e. young men, young women, older men, older women). However, age is typically measured as a continuous variable, and we would want to use a grouped scatter plot to see this: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(size=1) + scale_color_discrete(name=&quot;&quot;) But to make predictions from this continuous data we need to fit a line through the points (i.e. run a model). We can do this graphically by calling geom_smooth() which attempts to fit a smooth line through the data we observe: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(alpha=.2, size=1) + geom_smooth(se=F)+ scale_color_discrete(name=&quot;&quot;) Figure 16.1: Scatter plot overlaid with smooth best-fit lines And if we are confident that the relationships between predictor and outcome are sufficiently linear, then we can ask ggplot to fit a straight line using linear regression: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(alpha = .1, size = 1) + geom_smooth(se = F, linetype=&quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;, se = F) + scale_color_discrete(name=&quot;&quot;) Figure 16.2: Scatter plot overlaid with smoothed lines (dotted) and linear predictions (coloured) What these plots illustrate is the steps a researcher might take before fitting a regression model. The straight lines in the final plot represent our best guess for a person of a given age and gender, assuming a linear regression. We can read from these lines to make a point prediction for men and women of a specific age, and use the information about our uncertainty in the prediction, captured by the model, to estimate the likely error. To make our findings simpler to communicate, we might want to make estimates at specific ages and plot these. These ages could be: Values with biological or cultural meaning: for example 18 (new driver) v.s. 65 (retirement age) Statistical convention (e.g. median, 25th, and 75th centile, or mean +/- 1 SD) We’ll see examples of both below. "],
["predicted-means-and-margins-using-lm.html", "Predicted means and margins using lm()", " Predicted means and margins using lm() The section above details two types of predictions: predictions for means, and predictions for margins (effects). We can use the figure below as a way of visualising the difference: gridExtra::grid.arrange(means.plot+ggtitle(&quot;Means&quot;), margins.plot+ggtitle(&quot;Margins&quot;), ncol=2) Figure 2.1: Example of predicted means vs. margins. Note, the margin plotted in the second panel is the difference between the coloured lines in the first. A horizontal line is added at zero in panel 2 by convention. Running the model Lets say we want to run a linear model predicts injury severity from gender and a categorical measurement of age (young v.s. old). Our model formula would be: severity.of.injury ~ age.category * gender. Here we fit it an request the Anova table which enables us to test the main effects and interaction14: injurymodel &lt;- lm(severity.of.injury ~ age.category * gender, data=injuries) anova(injurymodel) ## Analysis of Variance Table ## ## Response: severity.of.injury ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age.category 1 4173.3 4173.3 154.573 &lt; 2.2e-16 *** ## gender 1 8488.5 8488.5 314.404 &lt; 2.2e-16 *** ## age.category:gender 1 1141.5 1141.5 42.279 1.25e-10 *** ## Residuals 996 26890.8 27.0 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Having saved the regression model in the variable injurymodel we can use this to make predictions for means and estimate marginal effects: Making predictions for means When making predictions, they key question to bear in mind is ‘predictions for what?’ That is, what values of the predictor variables are we going to use to estimate the outcome? It goes like this: Create a new dataframe which contains the values of the predictors we want to make predictions at Make the predictions using the predict() function. Convert the output of predict() to a dataframe and plot the numbers. Step 1: Make a new dataframe prediction.data &lt;- data_frame( age.category = c(&quot;young&quot;, &quot;older&quot;, &quot;young&quot;, &quot;older&quot;), gender = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;) ) prediction.data ## # A tibble: 4 x 2 ## age.category gender ## &lt;chr&gt; &lt;chr&gt; ## 1 young Male ## 2 older Male ## 3 young Female ## 4 older Female Step 2: Make the predictions The R predict() function has two useful arguments: newdata, which we set to our new data frame containing the predictor values of interest interval which we here set to confidence15 injury.predictions &lt;- predict(injurymodel, newdata=prediction.data, interval=&quot;confidence&quot;) injury.predictions ## fit lwr upr ## 1 57.14239 56.49360 57.79117 ## 2 59.19682 58.56688 59.82676 ## 3 60.79554 60.14278 61.44830 ## 4 67.12521 66.47642 67.77399 Making prdictions for margins (effects of predictors) library(&#39;tidyverse&#39;) m &lt;- lm(mpg~vs+wt, data=mtcars) m.predictions &lt;- predict(m, interval=&#39;confidence&#39;) mtcars.plus.predictions &lt;- bind_cols( mtcars, m.predictions %&gt;% as_data_frame() ) prediction.frame &lt;- expand.grid(vs=0:1, wt=2) %&gt;% as_data_frame() prediction.frame.plus.predictions &lt;- bind_cols( prediction.frame, predict(m, newdata=prediction.frame, interval=&#39;confidence&#39;) %&gt;% as_data_frame() ) mtcars.plus.predictions %&gt;% ggplot(aes(vs, fit, ymin=lwr, ymax=upr)) + stat_summary(geom=&quot;pointrange&quot;) prediction.frame.plus.predictions %&gt;% ggplot(aes(vs, fit, ymin=lwr, ymax=upr)) + geom_pointrange() prediction.frame.plus.predictions ## # A tibble: 2 x 5 ## vs wt fit lwr upr ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 2 24.11860 21.61207 26.62514 ## 2 1 2 27.27297 25.57096 28.97499 mtcars.plus.predictions %&gt;% group_by(vs) %&gt;% summarise_each(funs(mean), fit, lwr, upr) ## # A tibble: 2 x 4 ## vs fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 16.61667 14.93766 18.29568 ## 2 1 24.55714 22.81586 26.29843 Marginal effects What is the effect of being black or female on the chance of you getting diabetes? Two ways of computing, depending on which of these two you hate least: Calculate the effect of being black for someone who is 50% female (marginal effect at the means, MEM) Calculate the effect first pretending someone is black, then pretending they are white, and taking the difference between these estimate (average marginal effect, AME) library(margins) margins(m, at = list(wt = 1:2)) ## at(wt) vs wt ## 1 3.154 -4.443 ## 2 3.154 -4.443 m2 &lt;- lm(mpg~vs*wt, data=mtcars) summary(m2) ## ## Call: ## lm(formula = mpg ~ vs * wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9950 -1.7881 -0.3423 1.2935 5.2061 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.5314 2.6221 11.263 6.55e-12 *** ## vs 11.7667 3.7638 3.126 0.0041 ** ## wt -3.5013 0.6915 -5.063 2.33e-05 *** ## vs:wt -2.9097 1.2157 -2.393 0.0236 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.578 on 28 degrees of freedom ## Multiple R-squared: 0.8348, Adjusted R-squared: 0.8171 ## F-statistic: 47.16 on 3 and 28 DF, p-value: 4.497e-11 m2.margins &lt;- margins(m2, at = list(wt = 1.5:4.5)) summary(m2.margins) ## factor wt AME SE z p lower upper ## vs 1.5 7.4021 2.0902 3.5413 0.0004 3.3054 11.4989 ## vs 2.5 4.4924 1.2376 3.6299 0.0003 2.0667 6.9181 ## vs 3.5 1.5827 1.2848 1.2319 0.2180 -0.9353 4.1008 ## vs 4.5 -1.3270 2.1737 -0.6105 0.5416 -5.5874 2.9334 ## wt 1.5 -4.7743 0.5854 -8.1560 0.0000 -5.9216 -3.6270 ## wt 2.5 -4.7743 0.5854 -8.1556 0.0000 -5.9217 -3.6269 ## wt 3.5 -4.7743 0.5854 -8.1562 0.0000 -5.9216 -3.6270 ## wt 4.5 -4.7743 0.5854 -8.1561 0.0000 -5.9216 -3.6270 summary(m2.margins) %&gt;% as_data_frame() %&gt;% filter(factor==&quot;vs&quot;) %&gt;% ggplot(aes(wt, AME)) + geom_point() + geom_line() Because this is simulated data, the main effects and interactions all have tiny p values.↩ This gives us the confidence interval for the prediction, which is the range within which we would expect the true value to fall, 95% of the time, if we replicated the study. We could ask instead for the prediction interval, which would be the range within which 95% of new observations with the same predictor values would fall. For more on this see the section on confidence v.s. prediction intervals↩ "],
["predictions-with-continuous-covariates.html", "Predictions with continuous covariates", " Predictions with continuous covariates Run 2 x Continuous Anova Predict at different levels of X "],
["visualising-interactions.html", "Visualising interactions", " Visualising interactions Steps this page will work through: Running the the model (first will be a 2x2 between Anova) Using predict(). Creating predictions at specific values Binding predictions and the original data together. Using GGplot to layer points, lines and error bars. "],
["models-are-data-too.html", "17 Models are data", " 17 Models are data You might remember the episode of the Simpsons where Homer designs a car for ‘the average man’. It doesn’t end well. Traditional statistics packages are a bit like Homer’s car. They try to work for everyone, but in the process become bloated and difficult to use. This is particularly true of the output of software like SPSS, which by default produces multiple pages of ‘results’ for even relatively simple statistical models. However, the problem is not just that SPSS is incredibly verbose. The real issue is that SPSS views the results of a model as the end of a process, rather than the beginning. The model SPSS has is something like: Collect data Choose analysis from GUI Select relevant figures from pages of output and publish. This is a problem because in real life it just doesn’t work that way. In reality you will want to do things like: Run the same model for different outcomes Re-run similar models as part of a sensitivity analysis Compare different models and produce summaries of results from multiple models All of this requires an iterative process, in which you may want to compare and visualise the results of multiple models. In a traditional GUI, this quickly becomes overwhelming. However, if we treat modelling as a process which both consumes and produces data, R provides many helpful tools. This is an important insight: in R, the results of analyses are not the end point — instead model results are themselves data, to be processed, visualised, and compared. Storing models in variables This may seem obvious (and we have seen many examples in the sections above), but because R variables can contain anything, we can use them to store the results of our models. This is important, because it means we can keep track of different versions of the models we run, and compare them. Extracting results from models One of the nice things about R is that the summary() function will almost always provide a concise output of whatever model you send it, showing the key features of an model you have run. However, this text output isn’t suitable for publication, and can even be too verbose for communicating with colleagues. Often, when communicating with others, you want to focus in on the important details from analyses and to do this you need to extract results from your models. Thankfully, there is almost always a method to extract results to a dataframe. For example, if we run a linear model: model.fit &lt;- lm(mpg ~ wt + disp, data=mtcars) summary(model.fit) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## wt -3.35082 1.16413 -2.878 0.00743 ** ## disp -0.01773 0.00919 -1.929 0.06362 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 We can extract the parameter table from this model by saving the summary() of it, and then using the $ operator to access the coefficients table (actually a matrix), which is stored within the summary object. model.fit.summary &lt;- summary(model.fit) model.fit.summary$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055404 2.164539504 16.151497 4.910746e-16 ## wt -3.35082533 1.164128079 -2.878399 7.430725e-03 ## disp -0.01772474 0.009190429 -1.928609 6.361981e-02 ‘Poking around’ with $ and @ It’s actually a useful trick to learn how to ‘poke around’ inside R objects using the $ and @ operators (if you want the gory details of how these operators work, start with this guide to object systems in R). In the video below, I use RStudio’s autocomplete feature to find results buried within a lm object: For example, we could write the follwing to extract a table of coefficients, test statistics and p values from an lm() object (this is shown in the video: model.fit.summary &lt;- summary(model.fit) model.fit.summary$coefficients %&gt;% as_data_frame() ## # A tibble: 3 x 4 ## Estimate `Std. Error` `t value` `Pr(&gt;|t|)` ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 34.96055404 2.164539504 16.151497 4.910746e-16 ## 2 -3.35082533 1.164128079 -2.878399 7.430725e-03 ## 3 -0.01772474 0.009190429 -1.928609 6.361981e-02 Save work: use a Broom The broom:: library is worth learning because it makes it really easy to turn model results into dataframes, which is almost always what we want when working with data. It takes a slightly different approach than simply poking around with $ and @, because it providing general methods to ‘clean up’ the output of many older R functions. For example, the lm() or car::Anova functions display results in the console, but don’t make it easy to extract results as a dataframe. broom:: provides a consistent way of extracting the key numbers from most R objects. Let’s say we have a regression model: (model.1 &lt;- lm(mpg ~ factor(cyl) + wt + disp, data=mtcars)) ## ## Call: ## lm(formula = mpg ~ factor(cyl) + wt + disp, data = mtcars) ## ## Coefficients: ## (Intercept) factor(cyl)6 factor(cyl)8 wt disp ## 34.041673 -4.305559 -6.322786 -3.306751 0.001715 We can extract model fit statistics — that is, attributes of the model as a whole — with glance(). This produces a dataframe: glance(model.1) ## r.squared adj.r.squared sigma statistic p.value df logLik ## 1 0.8375299 0.8134602 2.603054 34.7961 2.726351e-10 5 -73.30158 ## AIC BIC deviance df.residual ## 1 158.6032 167.3976 182.949 27 If we want to extract information about the model coefficients we can use tidy: tidy(model.1, conf.int = T) %&gt;% pander term estimate std.error statistic p.value conf.low conf.high (Intercept) 34.04 1.963 17.34 3.662e-16 30.01 38.07 factor(cyl)6 -4.306 1.465 -2.939 0.006662 -7.311 -1.3 factor(cyl)8 -6.323 2.598 -2.433 0.02186 -11.65 -0.9913 wt -3.307 1.105 -2.992 0.005855 -5.574 -1.039 disp 0.001715 0.01348 0.1272 0.8997 -0.02595 0.02938 Which can then be plotted easily (adding the conf.int=T includes 95% confidence intervals for each parameter, which we can pass to ggplot): tidy(model.1, conf.int = T) %&gt;% ggplot(aes(term, estimate, ymin=conf.low, ymax=conf.high)) + geom_pointrange() + geom_hline(yintercept = 0) Finally, we can use the augment function to get information on individual rows in the modelled data: namely the fitted and residual values, plus common diagnostic metrics like Cooks distances: augment(model.1) %&gt;% head() %&gt;% pander(split.tables=Inf) ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead .rownames mpg factor.cyl. wt disp .fitted .se.fit .resid .hat .sigma .cooksd .std.resid Mazda RX4 21 6 2.62 160 21.35 1.058 -0.3468 0.1653 2.652 0.0008423 -0.1458 Mazda RX4 Wag 21 6 2.875 160 20.5 1.009 0.4964 0.1501 2.651 0.001512 0.2069 Datsun 710 22.8 4 2.32 108 26.56 0.7854 -3.755 0.09103 2.538 0.04586 -1.513 Hornet 4 Drive 21.4 6 3.215 258 19.55 1.355 1.853 0.2711 2.618 0.05169 0.8336 Hornet Sportabout 18.7 8 3.44 360 16.96 0.9784 1.739 0.1413 2.627 0.0171 0.7209 Valiant 18.1 6 3.46 225 18.68 1.059 -0.5806 0.1654 2.65 0.002363 -0.2442 Again these can be plotted: augment(model.1) %&gt;% ggplot(aes(x=.fitted, y=.resid)) + geom_point() + geom_smooth() ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## Warning: Deprecated: please use `purrr::possibly()` instead ## `geom_smooth()` using method = &#39;loess&#39; Because broom always returns a dataframe with a consistent set of column names we can also combine model results into tables for comparison. In this plot we see what happens to the regression coefficients in model 1 when we add disp, carb and drat in model 2. We plot the coefficients side by side for ease of comparison, and can see that the estimates for cyl1 and wt both shrink slightly with the addition of these variables: # run a new model with more predictors (model.2 &lt;- lm(mpg ~ factor(cyl) + wt + disp + carb + drat, data=mtcars)) ## ## Call: ## lm(formula = mpg ~ factor(cyl) + wt + disp + carb + drat, data = mtcars) ## ## Coefficients: ## (Intercept) factor(cyl)6 factor(cyl)8 wt disp ## 29.849209 -2.796142 -4.116561 -2.748229 -0.002826 ## carb drat ## -0.587422 1.056532 # make a single dataframe from both models # addin a new `model` column with mutate to # identify which coefficient came from which model combined.results &lt;- bind_rows( tidy(model.1, conf.int = T) %&gt;% mutate(model=&quot;1&quot;), tidy(model.2, conf.int = T) %&gt;% mutate(model=&quot;2&quot;)) combined.results %&gt;% # remove the intercept to make plot scale more sane filter(term != &quot;(Intercept)&quot;) %&gt;% ggplot(aes(term, estimate, ymin=conf.low, ymax=conf.high, color=model)) + geom_pointrange(position=position_dodge(width=.1)) + geom_hline(yintercept = 0) "],
["process-model-results.html", "‘Processing’ results", " ‘Processing’ results XXX TODO e.g.: Calculate VPC/ICC from an lmer models using model %&gt;% summary %&gt;% as_data_frame()$varcor "],
["output-tables.html", "Printing tables", " Printing tables XXX TODO Pander and pandoc Dealing with rounding and string formatting issues Missing values/unequal length columns Point out that arbitrarily complex tables often not worth the candle, longer easier than wider etc. "],
["apa-output.html", "APA formatting for free", " APA formatting for free A neat trick to avoid fat finger errors is to use functions to automatically display results in APA format. Unfortunately, there isn’t a single package which works with all types of model, but it’s not too hard switch between them. Chi2 For basic stats the apa:: package is simple to use. Below we use the apa::chisq_apa() function to properly format the results of our chi2 test ([see the full chi2 example]#crosstabs)): lego.test &lt;- chisq.test(lego.table) lego.test ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: lego.table ## X-squared = 11.864, df = 1, p-value = 0.0005724 And we can format in APA like so: apa::apa(lego.test, print_n=T) ## [1] &quot;$\\\\chi^2$(1, n = 100) = 11.86, *p* &lt; .001&quot; or using apastats:: we also get Cramer’s V, a measure of effect size: apastats::describe.chi(lego.table, addN=T) ## [1] &quot;$\\\\chi^2$(1, _N_ = 100) = 11.86, _p_ &lt; .001, _V_ = .34&quot; 17.0.0.1 Inserting results into your text If you are using RMarkdown, you can drop formatted results into your text without copying and pasting. Just type the following and the chi2 test result is automatically inserted inline in your text: Example of inline call to R functions within the text. This is shown as an image, because it would otherwise be hidden in this output (because the function is evaluated when we knit the document) Age (4 vs 6 years) was significantly associated with preference for duplo v.s. lego, \\(\\chi^2\\)(1, N = 100) = 11.86, p &lt; .001, V = .34 T-test # run the t test cars.test &lt;- t.test(wt~am,data=mtcars, var.equal=T) cars.test ## ## Two Sample t-test ## ## data: wt by am ## t = 5.2576, df = 30, p-value = 1.125e-05 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.8304317 1.8853577 ## sample estimates: ## mean in group 0 mean in group 1 ## 3.768895 2.411000 And then we can format as APA apa::apa(cars.test) ## [1] &quot;*t*(30) = 5.26, *p* &lt; .001, *d* = 1.86&quot; American cars were significantly heavier than foreign cars, mean difference=1358lbs; t(30) = 5.26, p &lt; .001, d = 1.86 Anova mpg.anova &lt;- car::Anova(lm(mpg~am*cyl, data=mtcars)) library(apastats) # extract and format main effect describe.Anova(mpg.anova, term=&quot;am&quot;) ## [1] &quot;_F_(1, 28) = 4.28, _p_ = .048&quot; # and the interaction describe.Anova(mpg.anova, term=&quot;am:cyl&quot;) ## [1] &quot;_F_(1, 28) = 3.41, _p_ = .076&quot; There was no interaction between location of manufacture and number of cylinders, F(1, 28) = 3.41, p = .076, but there was a main effect of location of manufacture, F(1, 28) = 3.41, p = .076, such that US-made cars had significantly higher fuel consumption than European or Japanese brands (see [Figure X or Table X]) Multilevel models If you have loaded the lmerTest package apastats can output either coefficients for single parameters, or F tests: sleep.model &lt;- lmer(Reaction~factor(Days)+(1|Subject), data=lme4::sleepstudy) #a single coefficient (this is a contrast from the reference category) describe.glm(sleep.model, term=&quot;factor(Days)1&quot;) ## [1] &quot;_t_ = 0.75, _p_ = .455&quot; # or describe the F test for the overall effect of Days describe.lmtaov(anova(sleep.model), term=&#39;factor(Days)&#39;) ## [1] &quot;_F_(9, 153.0) = 18.70, _p_ &lt; .001&quot; There were significant differences in reaction times across the 10 days of the study, F(9, 153.0) = 18.70, p &lt; .001 such that reaction latencies tended to increase in duration (see [Figure X]). "],
["code-reuse.html", "Simplifying and re-using", " Simplifying and re-using Complex programming is beyond the scope of this guide, but if you sometimes find yourself repeating the same piece of code over and over then it might be worth writing a simple function. The examples shown in this book are mostly very simple, and are typically not repetitive. However in running your own analyses you may find that you start to repeat chunks of code in a number of places (even across many files). Not only is this sort of copying-and-pasting tedious, it can be hard to read and maintain. For example, if you are repeating the same analysis for multiple variables and discover an error in your calculations you would have to fix this is in multiple places in your code. You can also introduce errors when copying and pasting code in this way, and these can be hard to spot. The sorts of tasks which can end up being repetitive include: Running models Extracting results from models you run Producing tables Specifying output settings for graphs In these cases it might be worth writing your own function to facilitate this repetition, or use some other forms of code re-use (e.g. see the example for ggplot graphics below). Writing helper functions For example, in the section on logistic regression we wrote a helper function called logistic() which was simply a shortcut for glm() but with the correct ‘family’ argument pre-specified. For more information on writing your own R functions I recommend Hadley Wickham’s ‘R for Data Scientists’, chapter 19 and, if necessary, ‘Advanced R’: http://adv-r.had.co.nz. Or you prefer a book, try @grolemund2014hands. Re-using code with ggplot Another common case where we might want to re-use code is when we produce a number of similar plots, and where we might want to re-use many of the same settings. Sometimes repeating plots is best achieved by creating a single long-form dataframe and facetting the plot. However this may not always be possible or desireable. Lets say we have two plots like these: plot.mpg &lt;- mtcars %&gt;% ggplot(aes(factor(cyl), mpg)) + geom_boxplot() plot.wt &lt;- mtcars %&gt;% ggplot(aes(factor(cyl), wt)) + geom_boxplot() And we want to add consistent labels and other settings to them. We can simply type: myplotsettings &lt;- xlab(&quot;Number of cylinders&quot;) And then we simply add (+) these settings to each plot: plot.mpg + myplotsettings And: plot.wt + myplotsettings This reduces typing and makes easier to produce a consistent set of plots. "],
["table1.html", "“Table 1”", " “Table 1” Table 1 in reports of clinical trials and many psychological studies reports characteristics of the sample. Typically, you will want to present information collected at baseline, split by experimental groups, including: Means, standard deviations or other descriptive statistics for continuous variables Frequencies of particular responses for categorical variables Some kind of inferential test for a zero-difference between the groups; this could be a t-test, an F-statistic where there are more than 2 groups, or a chi-squared test for categorical variables. Producing this table is a pain because it requires collating multiple statistics, calculated from different functions. Many researchers resort to performing all the analyses required for each part of the table, and then copying-and-pasting results into Word. It can be automated though! This example combines and extends many of the techniques we have learned using the split-apply-combine method. To begin, let’s simulate some data from a fairly standard 2-arm clinical trial or psychological experiment: Check our data: boring.study %&gt;% glimpse ## Observations: 280 ## Variables: 8 ## $ person &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1... ## $ time &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ condition &lt;fctr&gt; Control, Control, Control, Control, Control, Contro... ## $ yob &lt;dbl&gt; 1984, 1984, 1982, 1973, 1976, 1988, 1984, 1981, 1982... ## $ WM &lt;dbl&gt; 86, 112, 87, 101, 97, 97, 89, 78, 114, 107, 110, 100... ## $ education &lt;chr&gt; NA, &quot;Postgraduate&quot;, &quot;Primary&quot;, &quot;Postgraduate&quot;, &quot;Seco... ## $ ethnicity &lt;chr&gt; &quot;Mixed / multiple ethnic groups&quot;, &quot;White British&quot;, &quot;... ## $ Attitude &lt;dbl&gt; 4, 4, 8, 11, 9, 5, 6, 8, 10, 10, 7, 15, 10, 8, 6, 5,... Start by making a long-form table for the categorical variables: boring.study.categorical.melted &lt;- table1.categorical.Ns &lt;- boring.study %&gt;% select(condition, education, ethnicity) %&gt;% melt(id.var=&#39;condition&#39;) Then calculate the N’s for each response/variable in each group: (table1.categorical.Ns &lt;- boring.study.categorical.melted %&gt;% group_by(condition, variable, value) %&gt;% summarise(N=n()) %&gt;% dcast(variable+value~condition, value.var=&quot;N&quot;)) ## variable value Control ## 1 education Graduate 28 ## 2 education Postgraduate 33 ## 3 education Primary 26 ## 4 education Secondary 30 ## 5 education &lt;NA&gt; 23 ## 6 ethnicity Asian / Asian British 37 ## 7 ethnicity Black / African / Caribbean / Black British 26 ## 8 ethnicity Mixed / multiple ethnic groups 36 ## 9 ethnicity White British 41 ## Intervention ## 1 23 ## 2 30 ## 3 31 ## 4 30 ## 5 26 ## 6 32 ## 7 32 ## 8 35 ## 9 41 Then make a second table containing Chi2 test statistics for each variable: (table1.categorical.tests &lt;- boring.study.categorical.melted %&gt;% group_by(variable) %&gt;% do(., chisq.test(.$value, .$condition) %&gt;% tidy) %&gt;% # this purely to facilitate matching rows up below mutate(firstrowforvar=T)) ## # A tibble: 2 x 6 ## # Groups: variable [2] ## variable statistic p.value parameter method ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;fctr&gt; ## 1 education 1.032863 0.7933008 3 Pearson&#39;s Chi-squared test ## 2 ethnicity 0.997093 0.8019554 3 Pearson&#39;s Chi-squared test ## # ... with 1 more variables: firstrowforvar &lt;lgl&gt; Combine these together: (table1.categorical.both &lt;- table1.categorical.Ns %&gt;% group_by(variable) %&gt;% # we join on firstrowforvar to make sure we don&#39;t duplicate the tests mutate(firstrowforvar=row_number()==1) %&gt;% left_join(., table1.categorical.tests, by=c(&quot;variable&quot;, &quot;firstrowforvar&quot;)) %&gt;% # this is gross, but we don&#39;t want to repeat the variable names in our table ungroup() %&gt;% mutate(variable = ifelse(firstrowforvar==T, as.character(variable), NA)) %&gt;% select(variable, value, Control, Intervention, statistic, parameter, p.value)) ## # A tibble: 9 x 7 ## variable value Control ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 education Graduate 28 ## 2 &lt;NA&gt; Postgraduate 33 ## 3 &lt;NA&gt; Primary 26 ## 4 &lt;NA&gt; Secondary 30 ## 5 &lt;NA&gt; &lt;NA&gt; 23 ## 6 ethnicity Asian / Asian British 37 ## 7 &lt;NA&gt; Black / African / Caribbean / Black British 26 ## 8 &lt;NA&gt; Mixed / multiple ethnic groups 36 ## 9 &lt;NA&gt; White British 41 ## # ... with 4 more variables: Intervention &lt;int&gt;, statistic &lt;dbl&gt;, ## # parameter &lt;int&gt;, p.value &lt;dbl&gt; Now we deal with the continuous variables. First we make a ‘long’ version of the continuous data continuous_variables &lt;- c(&quot;yob&quot;, &quot;WM&quot;) boring.continuous.melted &lt;- boring.study %&gt;% select(condition, continuous_variables) %&gt;% melt() %&gt;% group_by(variable) ## Using condition as id variables boring.continuous.melted %&gt;% head ## # A tibble: 6 x 3 ## # Groups: variable [1] ## condition variable value ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 Control yob 1984 ## 2 Control yob 1984 ## 3 Control yob 1982 ## 4 Control yob 1973 ## 5 Control yob 1976 ## 6 Control yob 1988 Then calculate separate tables of t-tests and means/SD’s: (table.continuous_variables.tests &lt;- boring.continuous.melted %&gt;% # note that we pass the result of t-test to tidy, which returns a dataframe do(., t.test(.$value~.$condition) %&gt;% tidy) %&gt;% select(variable, statistic, parameter, p.value)) ## # A tibble: 2 x 4 ## # Groups: variable [2] ## variable statistic parameter p.value ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 yob -0.02325905 277.2538 0.9814604 ## 2 WM -1.59245520 276.2850 0.1124259 (table.continuous_variables.descriptives &lt;- boring.continuous.melted %&gt;% group_by(variable, condition) %&gt;% # this is not needed here because we have no missing values, but if there # were missing value in this dataset then mean/sd functions would fail below, # so best to remove rows without a response: filter(!is.na(value)) %&gt;% # note, we might also want the median/IQR summarise(Mean=mean(value), SD=sd(value)) %&gt;% group_by(variable, condition) %&gt;% # we format the mean and SD into a single column using sprintf. # we don&#39;t have to do this, but it makes reshaping simpler and we probably want # to round the numbers at some point, and so may as well do this now. transmute(MSD = sprintf(&quot;%.2f (%.2f)&quot;, Mean, SD)) %&gt;% dcast(variable~condition)) ## Adding missing grouping variables: `variable`, `condition` ## Using MSD as value column: use value.var to override. ## variable Control Intervention ## 1 yob 1978.78 (5.00) 1978.79 (5.27) ## 2 WM 98.59 (9.62) 100.49 (10.41) And combine them: (table.continuous_variables.both &lt;- left_join(table.continuous_variables.descriptives, table.continuous_variables.tests)) ## Joining, by = &quot;variable&quot; ## variable Control Intervention statistic parameter p.value ## 1 yob 1978.78 (5.00) 1978.79 (5.27) -0.02325905 277.2538 0.9814604 ## 2 WM 98.59 (9.62) 100.49 (10.41) -1.59245520 276.2850 0.1124259 Finally put the whole thing together: (table1 &lt;- table1.categorical.both %&gt;% # make these variables into character format to be consistent with # the Mean (SD) column for continuus variables mutate_each(funs(format), Control, Intervention) %&gt;% # note the &#39;.&#39; as the first argument, which is the input from the pipe bind_rows(., table.continuous_variables.both) %&gt;% # prettify a few things rename(df = parameter, p=p.value, `Control N/Mean (SD)`= Control, Variable=variable, Response=value, `t/χ2` = statistic)) ## `mutate_each()` is deprecated. ## Use `mutate_all()`, `mutate_at()` or `mutate_if()` instead. ## To map `funs` over a selection of variables, use `mutate_at()` ## Warning in bind_rows_(x, .id): binding character and factor vector, ## coercing into character vector ## # A tibble: 11 x 7 ## Variable Response ## &lt;chr&gt; &lt;chr&gt; ## 1 education Graduate ## 2 &lt;NA&gt; Postgraduate ## 3 &lt;NA&gt; Primary ## 4 &lt;NA&gt; Secondary ## 5 &lt;NA&gt; &lt;NA&gt; ## 6 ethnicity Asian / Asian British ## 7 &lt;NA&gt; Black / African / Caribbean / Black British ## 8 &lt;NA&gt; Mixed / multiple ethnic groups ## 9 &lt;NA&gt; White British ## 10 yob &lt;NA&gt; ## 11 WM &lt;NA&gt; ## # ... with 5 more variables: `Control N/Mean (SD)` &lt;chr&gt;, ## # Intervention &lt;chr&gt;, `t/χ2` &lt;dbl&gt;, df &lt;dbl&gt;, p &lt;dbl&gt; And we can print to markdown format for outputting. This is best done in a separate chunk to avoid warnings/messages appearing in the final document. table1 %&gt;% # split.tables argument needed to avoid the table wrapping pander(split.tables=Inf, missing=&quot;-&quot;, justify=c(&quot;left&quot;, &quot;left&quot;, rep(&quot;center&quot;, 5)), caption=&#39;Table presenting baseline differences between conditions. Categorical variables tested with Pearson χ2, continuous variables with two-sample t-test.&#39;) Table presenting baseline differences between conditions. Categorical variables tested with Pearson χ2, continuous variables with two-sample t-test. Variable Response Control N/Mean (SD) Intervention t/χ2 df p education Graduate 28 23 1.033 3 0.7933 - Postgraduate 33 30 - - - - Primary 26 31 - - - - Secondary 30 30 - - - - - 23 26 - - - ethnicity Asian / Asian British 37 32 0.9971 3 0.802 - Black / African / Caribbean / Black British 26 32 - - - - Mixed / multiple ethnic groups 36 35 - - - - White British 41 41 - - - yob - 1978.78 (5.00) 1978.79 (5.27) -0.02326 277.3 0.9815 WM - 98.59 (9.62) 100.49 (10.41) -1.592 276.3 0.1124 Some exercises to work on/extensions to this code you might need: Add a new continuous variable to the simulated dataset and include it in the final table Create a third experimental group and amend the code to i) include 3 columns for the N/Mean and ii) report the F-test from a one-way Anova as the test statistic. Add the within-group percentage for each response to a categorical variable. "],
["quirks.html", "18 Dealing with quirks of R ", " 18 Dealing with quirks of R "],
["rownames.html", "Rownames are evil", " Rownames are evil Historically ‘row names’ were used on R to label individual rows in a dataframe. It turned out that this is generally a bad idea, because sorting and some summary functions would get very confused and mix up row names and the data itself. It’s now considered best practice to avoid row names for this reason. Consequently, the functions in the dplyr library remove row names when processing dataframes. For example here we see the row names in the mtcars data: mtcars %&gt;% head(3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 But here we don’t because arrange has stripped them: mtcars %&gt;% arrange(mpg) %&gt;% head(3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 ## 2 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4 ## 3 13.3 8 350 245 3.73 3.840 15.41 0 0 3 4 Converting the results of psych::describe() also returns rownames, which can get lots if we sort the data. We see row names here: psych::describe(mtcars) %&gt;% as_data_frame() %&gt;% head(3) ## # A tibble: 3 x 13 ## vars n mean sd median trimmed mad min max ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 32 20.09062 6.026948 19.2 19.696154 5.41149 10.4 33.9 ## 2 2 32 6.18750 1.785922 6.0 6.230769 2.96520 4.0 8.0 ## 3 3 32 230.72188 123.938694 196.3 222.523077 140.47635 71.1 472.0 ## # ... with 4 more variables: range &lt;dbl&gt;, skew &lt;dbl&gt;, kurtosis &lt;dbl&gt;, ## # se &lt;dbl&gt; But not here (just numbers in their place): psych::describe(mtcars) %&gt;% as_data_frame() %&gt;% arrange(mean) %&gt;% head(3) ## # A tibble: 3 x 13 ## vars n mean sd median trimmed mad min max range ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 9 32 0.40625 0.4989909 0 0.3846154 0.0000 0 1 1 ## 2 8 32 0.43750 0.5040161 0 0.4230769 0.0000 0 1 1 ## 3 11 32 2.81250 1.6152000 2 2.6538462 1.4826 1 8 7 ## # ... with 3 more variables: skew &lt;dbl&gt;, kurtosis &lt;dbl&gt;, se &lt;dbl&gt; Preserving row names If you want to preserve row names, it’s best to convert the names to an extra colum in the data. So, the example below does what we probably want: # the var=&#39;car.name&#39; argument is optional, but can be useful mtcars %&gt;% rownames_to_column(var=&quot;car.name&quot;) %&gt;% arrange(mpg) %&gt;% head(3) ## car.name mpg cyl disp hp drat wt qsec vs am gear carb ## 1 Cadillac Fleetwood 10.4 8 472 205 2.93 5.250 17.98 0 0 3 4 ## 2 Lincoln Continental 10.4 8 460 215 3.00 5.424 17.82 0 0 3 4 ## 3 Camaro Z28 13.3 8 350 245 3.73 3.840 15.41 0 0 3 4 lm(mpg~wt, data=mtcars) %&gt;% broom::tidy() %&gt;% pander term estimate std.error statistic p.value (Intercept) 37.29 1.878 19.86 8.242e-19 wt -5.344 0.5591 -9.559 1.294e-10 "],
["string-handling.html", "Working with character strings", " Working with character strings 18.0.1 Searching and replacing If you want to search inside a string there are lots of useful functions in the stringr:: library. These replicate some functionality in base R, but like other packages in the ‘tidyverse’ they tend to be more consistent and easier to use. For example: cheese &lt;- c(&quot;Stilton&quot;, &quot;Brie&quot;, &quot;Cheddar&quot;) stringr::str_detect(cheese, &quot;Br&quot;) ## [1] FALSE TRUE FALSE stringr::str_locate(cheese, &quot;i&quot;) ## start end ## [1,] 3 3 ## [2,] 3 3 ## [3,] NA NA stringr::str_replace(cheese, &quot;Stil&quot;, &quot;Mil&quot;) ## [1] &quot;Milton&quot; &quot;Brie&quot; &quot;Cheddar&quot; Using paste to make labels Paste can combine character strings with other types of variable to produce a new vector: paste(mtcars$cyl, &quot;cylinders&quot;)[1:10] ## [1] &quot;6 cylinders&quot; &quot;6 cylinders&quot; &quot;4 cylinders&quot; &quot;6 cylinders&quot; &quot;8 cylinders&quot; ## [6] &quot;6 cylinders&quot; &quot;8 cylinders&quot; &quot;4 cylinders&quot; &quot;4 cylinders&quot; &quot;6 cylinders&quot; Which can be a useful way to label graphs: mtcars %&gt;% ggplot(aes(paste(mtcars$cyl, &quot;cylinders&quot;), mpg)) + geom_boxplot() + xlab(&quot;&quot;) Fixing up variable after melting In this example melt() creates a new column called variable. sleep.wide %&gt;% melt(id.var=&quot;Subject&quot;) %&gt;% arrange(Subject, variable) %&gt;% head ## Subject variable value ## 1 1 Day.0 249.5600 ## 2 1 Day.1 258.7047 ## 3 1 Day.2 250.8006 ## 4 1 Day.3 321.4398 ## 5 1 Day.4 356.8519 ## 6 1 Day.5 414.6901 However the contents of variable are now a character string (i.e. a list of letters and numbers) rather than numeric values (see column types) but in this instance we know that the values Day.1, Day.2… are not really separate categories but actually form a linear sequence, from 1 to 9. We can use the extract or separate functions to split up variable and create a numeric column for Day: sleep.long %&gt;% separate(variable, c(&quot;variable&quot;, &quot;Day&quot;)) %&gt;% mutate(Day=as.numeric(Day)) %&gt;% arrange(Subject) %&gt;% head %&gt;% pander Subject variable Day value 1 Day 0 249.6 1 Day 1 258.7 1 Day 2 250.8 1 Day 3 321.4 1 Day 4 356.9 1 Day 5 414.7 See the user guide for separate and extract for more details. If you are familiar with regular expressions you will be happy to know that you can use regex to separate variables using extract and separate. See this guide for more details on how separate and extract work "],
["colours.html", "Colours", " Colours Picking colours for plots See https://www.perceptualedge.com/articles/b-eye/choosing_colors.pdf for an interesting discussion on picking colours for data visualisation. Also check the ggplots docs for colour brewer and the Colour Brewer website. Named colours in R print.col &lt;- Vectorize(function(col){ rgb &lt;- grDevices::col2rgb(col) sprintf(&quot;&lt;span style=&#39;padding:1em; background-color: rgb(%s, %s, %s);&#39;&gt;&amp;nbsp;&lt;/span&gt; %s \\n\\n&quot;, rgb[1], rgb[2], rgb[3], col, rgb[1], rgb[2], rgb[3]) }) pandoc.p(print.col(colours())) white aliceblue antiquewhite antiquewhite1 antiquewhite2 antiquewhite3 antiquewhite4 aquamarine aquamarine1 aquamarine2 aquamarine3 aquamarine4 azure azure1 azure2 azure3 azure4 beige bisque bisque1 bisque2 bisque3 bisque4 black blanchedalmond blue blue1 blue2 blue3 blue4 blueviolet brown brown1 brown2 brown3 brown4 burlywood burlywood1 burlywood2 burlywood3 burlywood4 cadetblue cadetblue1 cadetblue2 cadetblue3 cadetblue4 chartreuse chartreuse1 chartreuse2 chartreuse3 chartreuse4 chocolate chocolate1 chocolate2 chocolate3 chocolate4 coral coral1 coral2 coral3 coral4 cornflowerblue cornsilk cornsilk1 cornsilk2 cornsilk3 cornsilk4 cyan cyan1 cyan2 cyan3 cyan4 darkblue darkcyan darkgoldenrod darkgoldenrod1 darkgoldenrod2 darkgoldenrod3 darkgoldenrod4 darkgray darkgreen darkgrey darkkhaki darkmagenta darkolivegreen darkolivegreen1 darkolivegreen2 darkolivegreen3 darkolivegreen4 darkorange darkorange1 darkorange2 darkorange3 darkorange4 darkorchid darkorchid1 darkorchid2 darkorchid3 darkorchid4 darkred darksalmon darkseagreen darkseagreen1 darkseagreen2 darkseagreen3 darkseagreen4 darkslateblue darkslategray darkslategray1 darkslategray2 darkslategray3 darkslategray4 darkslategrey darkturquoise darkviolet deeppink deeppink1 deeppink2 deeppink3 deeppink4 deepskyblue deepskyblue1 deepskyblue2 deepskyblue3 deepskyblue4 dimgray dimgrey dodgerblue dodgerblue1 dodgerblue2 dodgerblue3 dodgerblue4 firebrick firebrick1 firebrick2 firebrick3 firebrick4 floralwhite forestgreen gainsboro ghostwhite gold gold1 gold2 gold3 gold4 goldenrod goldenrod1 goldenrod2 goldenrod3 goldenrod4 gray gray0 gray1 gray2 gray3 gray4 gray5 gray6 gray7 gray8 gray9 gray10 gray11 gray12 gray13 gray14 gray15 gray16 gray17 gray18 gray19 gray20 gray21 gray22 gray23 gray24 gray25 gray26 gray27 gray28 gray29 gray30 gray31 gray32 gray33 gray34 gray35 gray36 gray37 gray38 gray39 gray40 gray41 gray42 gray43 gray44 gray45 gray46 gray47 gray48 gray49 gray50 gray51 gray52 gray53 gray54 gray55 gray56 gray57 gray58 gray59 gray60 gray61 gray62 gray63 gray64 gray65 gray66 gray67 gray68 gray69 gray70 gray71 gray72 gray73 gray74 gray75 gray76 gray77 gray78 gray79 gray80 gray81 gray82 gray83 gray84 gray85 gray86 gray87 gray88 gray89 gray90 gray91 gray92 gray93 gray94 gray95 gray96 gray97 gray98 gray99 gray100 green green1 green2 green3 green4 greenyellow grey grey0 grey1 grey2 grey3 grey4 grey5 grey6 grey7 grey8 grey9 grey10 grey11 grey12 grey13 grey14 grey15 grey16 grey17 grey18 grey19 grey20 grey21 grey22 grey23 grey24 grey25 grey26 grey27 grey28 grey29 grey30 grey31 grey32 grey33 grey34 grey35 grey36 grey37 grey38 grey39 grey40 grey41 grey42 grey43 grey44 grey45 grey46 grey47 grey48 grey49 grey50 grey51 grey52 grey53 grey54 grey55 grey56 grey57 grey58 grey59 grey60 grey61 grey62 grey63 grey64 grey65 grey66 grey67 grey68 grey69 grey70 grey71 grey72 grey73 grey74 grey75 grey76 grey77 grey78 grey79 grey80 grey81 grey82 grey83 grey84 grey85 grey86 grey87 grey88 grey89 grey90 grey91 grey92 grey93 grey94 grey95 grey96 grey97 grey98 grey99 grey100 honeydew honeydew1 honeydew2 honeydew3 honeydew4 hotpink hotpink1 hotpink2 hotpink3 hotpink4 indianred indianred1 indianred2 indianred3 indianred4 ivory ivory1 ivory2 ivory3 ivory4 khaki khaki1 khaki2 khaki3 khaki4 lavender lavenderblush lavenderblush1 lavenderblush2 lavenderblush3 lavenderblush4 lawngreen lemonchiffon lemonchiffon1 lemonchiffon2 lemonchiffon3 lemonchiffon4 lightblue lightblue1 lightblue2 lightblue3 lightblue4 lightcoral lightcyan lightcyan1 lightcyan2 lightcyan3 lightcyan4 lightgoldenrod lightgoldenrod1 lightgoldenrod2 lightgoldenrod3 lightgoldenrod4 lightgoldenrodyellow lightgray lightgreen lightgrey lightpink lightpink1 lightpink2 lightpink3 lightpink4 lightsalmon lightsalmon1 lightsalmon2 lightsalmon3 lightsalmon4 lightseagreen lightskyblue lightskyblue1 lightskyblue2 lightskyblue3 lightskyblue4 lightslateblue lightslategray lightslategrey lightsteelblue lightsteelblue1 lightsteelblue2 lightsteelblue3 lightsteelblue4 lightyellow lightyellow1 lightyellow2 lightyellow3 lightyellow4 limegreen linen magenta magenta1 magenta2 magenta3 magenta4 maroon maroon1 maroon2 maroon3 maroon4 mediumaquamarine mediumblue mediumorchid mediumorchid1 mediumorchid2 mediumorchid3 mediumorchid4 mediumpurple mediumpurple1 mediumpurple2 mediumpurple3 mediumpurple4 mediumseagreen mediumslateblue mediumspringgreen mediumturquoise mediumvioletred midnightblue mintcream mistyrose mistyrose1 mistyrose2 mistyrose3 mistyrose4 moccasin navajowhite navajowhite1 navajowhite2 navajowhite3 navajowhite4 navy navyblue oldlace olivedrab olivedrab1 olivedrab2 olivedrab3 olivedrab4 orange orange1 orange2 orange3 orange4 orangered orangered1 orangered2 orangered3 orangered4 orchid orchid1 orchid2 orchid3 orchid4 palegoldenrod palegreen palegreen1 palegreen2 palegreen3 palegreen4 paleturquoise paleturquoise1 paleturquoise2 paleturquoise3 paleturquoise4 palevioletred palevioletred1 palevioletred2 palevioletred3 palevioletred4 papayawhip peachpuff peachpuff1 peachpuff2 peachpuff3 peachpuff4 peru pink pink1 pink2 pink3 pink4 plum plum1 plum2 plum3 plum4 powderblue purple purple1 purple2 purple3 purple4 red red1 red2 red3 red4 rosybrown rosybrown1 rosybrown2 rosybrown3 rosybrown4 royalblue royalblue1 royalblue2 royalblue3 royalblue4 saddlebrown salmon salmon1 salmon2 salmon3 salmon4 sandybrown seagreen seagreen1 seagreen2 seagreen3 seagreen4 seashell seashell1 seashell2 seashell3 seashell4 sienna sienna1 sienna2 sienna3 sienna4 skyblue skyblue1 skyblue2 skyblue3 skyblue4 slateblue slateblue1 slateblue2 slateblue3 slateblue4 slategray slategray1 slategray2 slategray3 slategray4 slategrey snow snow1 snow2 snow3 snow4 springgreen springgreen1 springgreen2 springgreen3 springgreen4 steelblue steelblue1 steelblue2 steelblue3 steelblue4 tan tan1 tan2 tan3 tan4 thistle thistle1 thistle2 thistle3 thistle4 tomato tomato1 tomato2 tomato3 tomato4 turquoise turquoise1 turquoise2 turquoise3 turquoise4 violet violetred violetred1 violetred2 violetred3 violetred4 wheat wheat1 wheat2 wheat3 wheat4 whitesmoke yellow yellow1 yellow2 yellow3 yellow4 yellowgreen ColourBrewer with ggplot See: http://ggplot2.tidyverse.org/reference/scale_brewer.html "],
["getting-help.html", "19 Getting help", " 19 Getting help If you don’t know or can’t remember what a function does, R provides help files which explain how they work. To access a help file for a function, just type ?command in the console, or run ?command command within an R block. For example, running ?mean would bring up the documentation for the mean function. You can also type CRTL-Shift-H while your cursor is over any R function in the RStudio interface. It’s fair to say R documentation isn’t always written for beginners. However the ‘examples’ sections are usually quite informative: you can normally see this by scrolling right to the end of the help file. Finding the backtick on your keyboard The backtick ``` symbol is unfamiliar to some readers. Here’s where it is: On windows On a Mac "],
["intervals.html", "20 Confidence and Intervals", " 20 Confidence and Intervals Some quick definitions to begin. Let’s say we have made an estimate from a model. To keep things simple, it could just be the sample mean. A Confidence interval is the range within which we would expect the ‘true’ value to fall, 95% of the time, if we replicated the study. A Prediction interval is the range within which we expect 95% of new observations to fall. If we’re considering the prediction interval for a specific point prediction (i.e. where we set predictors to specific values), then this interval woud be for new observations with the same predictor values. A Bayesian Credible interval is the range of values within which we are 95% sure the true value lies, based on our prior knowledge and the data we have collected. The problem with confidence intervals Confidence intervals are helpful when we want to think about how precise our estimate is. For example, in an RCT we will want to estimate the difference between treatment groups, and it’s conceivable we would to want to know, for example, the range within which the true effect would fall 95% of the time if we replicated our study many times (although in reality, this isn’t a question many people would actually ask). If we run a study with small N, intuitively we know that we have less information about the difference between our RCT treatments, and so we’d like the CI to expand accordingly. So — all things being equal — the confidence interval reduces as we collect more data. The problem with confidence intervals comes about because many researchers and clinicians read them incorrectly. Typically, they either: Forget that the CI represents only the precision of the estimate. The CI doesn’t reflect how good our predictions for new observations will be. Misinterpret the CI as the range in which we are 95% sure the true value lies. Forgetting that the CI depends on sample size. By forgetting that the CI contracts as the sample size increases, researchers can become overconfident about their ability to predict new observations. Imagine that we sample data from two populations with the same mean, but different variability: set.seed(1234) df &lt;- expand.grid(v=c(1,3,3,3), i=1:1000) %&gt;% as_data_frame %&gt;% mutate(y = rnorm(length(.$i), 100, v)) %&gt;% mutate(samp = factor(v, labels=c(&quot;Low variability&quot;, &quot;High variability&quot;))) df %&gt;% ggplot(aes(y)) + geom_histogram() + facet_grid(~samp) + scale_color_discrete(&quot;&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we sample 100 individuals from each population the confidence interval around the sample mean would be wider in the high variability group. If we increase our sample size we would become more confident about the location of the mean, and this confidence interval would shrink. But imagine taking a single new sample from either population. These samples would be new grey squares, which we place on the histograms above. It does not matter how much extra data we have collected in group B or how sure what the mean of the group is: We would always be less certain making predictions for new observations in the high variability group. The important insight here is that if our data are noisy and highly variable we can never make firm predictions for new individuals, even if we collect so much data that we are very certain about the location of the mean. "],
["multiple-comparisons.html", "21 Multiple comparisons", " 21 Multiple comparisons People get confused about multiple comparisons and worry about ‘doing things right’. There are many different tests and procedures, and thousands of pages of tutorials and guides each of which recommends a slightly different approach. Textbooks typically describe the tests themselves in detail, and list the assumptions they make, but it sometimes feels like nobody will give a straight answer. The inconsistency arises because different researchers have different priorities. It might help to re-address what a p value is, and what it is for. p values and ‘false discoveries’ In frequentist statistics, p values are defined as *the probability of obtaining a test statistic at least as large as that observed, if the null hypothesis is true. That is, it’s the chance that the data we have collected are atypical and will mislead us into thinking there is a difference, when the true effect is zero. Let’s pretend we are creative researchers and, over the course of our career, we will develop 100 hypotheses, each of which we test in an experiment, represented by squares in the plot below: We are concientious and use sample size calculations for our studies, setting our desired power = .8, and use p = .05 as our criterion for rejecting the null hypotheses. As is common, we report tests which reject the null hypothesis as if our predictions had been supported, and the alternative hypotheses were true (this is a bad idea, but run with it for the moment). Let’s be generous and say that, in reality, 50% of our hypotheses are true (the other 50% are nice ideas, but are not correct). Because we set our power to be .8 in our sample size calculation, this means that over the course of our career we will detect around 40 ‘true’ effects, and publish them. These are shown in blue in the figure below: Because we set our alpha level to 0.05, we will also have some false positives, shown in red: But what that means is that for the effects we publish as supporting our hypotheses (i.e. the blue and red squares) then we will be making false claims 5/40 = 12.5% of the time. This is obviously much higher than the nominal alpha level implies. What’s more, if we’re not hotshot theorists (or work in an area where theories are less rigorously developed) and only 20% of our hypotheses are in fact true then we will make even more false claims: 5/20 = 25%. Multiple tests on the same data Another problem arises when we run multiple statistical tests on the same data. The most common case is that we have multiple experimental conditions and want to test whether any of the differences between them are significant. If we had experimental 3 groups, then there are 3 possible comparisons between them, for example (A-B, A-C, B-C). This can be compounded if we have multiple outcome measurements (for example, measuring depression and anxiety in a clincal trial; see @feise2002multiple). This works like a lucky dip or lottery: if you buy more tickets you have a larger change of winning a prize. In this case, with three comparisons between the groups (tickets) we have a 15% chance of winning a prize, rather than the 5% we intended. Assuming each of our 100 experiments allows for 3 tests, any of which would be ‘interesting’ if significant (and we would therefore publish them), then our plot looks like this: And our ‘false discovery rate’ (at the level of our published papers) is now over one third: 15/40 = 37.5%. False discovery rates for single experiments We can also think about a false discovery rate for findings presented within a particular paper, or set of analyses. Imagine we have a 2x4 factorial design, and so we have 8 experimental groups, and 28 possible pairwise combinations: 8 cells in the 2x4 design Cell.No A B 1 1 0 2 2 0 3 3 0 4 4 0 5 1 1 6 2 1 7 3 1 8 4 1 Pairwise comparisons Comparison Cells 1 1 vs. 2 2 1 vs. 3 3 1 vs. 4 … … … … 26 6 vs. 7 27 6 vs. 8 28 7 vs. 8 Assuming there are ‘real’ differences between only one-third of these pairings, then we have the same problem as we did when considering all the experiments in a researchers’ career: Using p &lt; .05 as our criterion, and power of .8 then we will report around 9 significant differences, but 1 or 2 of these will false discoveries (16%). What to do about it? There are two decisions you need to make: First, which technique do you want to use to mitigate the risks of multiple comparisons? Second, what is the ‘family’ of tests you are going to apply this to? If you find the whole language around null hypothesis testing and p values unhelpful, and the detail of multiple comparison adjustment confusing, there is another way: Multiple comparison problems are largely a non-issue for Bayesian analyses [@gelman2012we], and recent developments in the software make simple models like Anova and regression easy to implement in a Bayesian framework in R. Which technique? Inevitably, it depends! If your worry is ever making a type 1 error then you need to control the familywise error rate. The Bonferroni or Tukey correction are likely the best solutions (see details below). Both have the disadvantage of increasing the type 2 error rate: you will falsely accept the null more frequently, and so neglect findings which are ‘true’ and might be theoretically or clinically relevant. If your main concern is about the number of ‘false discoveries’ you make, and you think it’s import to you to avoid rejecting ‘good’ hypotheses, then you need a technique to control the false discovery rate. The Benjamini and Hochberg method, often abbreviated as BH or FDR correction, is what you need. Principled arguments can be made for both approaches; there is no ‘correct choice’. Which you pick will depend on which concerns are most pressing to you, and also (inevitably) what is conventional in your field. It’s worth noting that in specific fields (for example neuroscience, where huge numbers of comparisons must be made between voxels or regions in brain imaging studies) more sophisticated methods to control for multiple comparisons are often commonplace and expected [@nichols2003controlling]. This guide only deals with some of the simpler but common cases. How big is the family? Whichever technique we employ we do need to determine how many tests we want to correct for having run; i.e. what size the ‘family’ of tests is. Unfortuntately, deciding which set of tests constitutes a ‘family’ is an unresolved problem, and reasonable people will come up with different solutions. As @feise2002multiple put it: It is unclear how wide the operative term “family” should be […] Does “family” include tests that were performed, but not published? Does it include a meta-analysis upon those tests? Should future papers on the same data set be accounted for in the first publication? Should each researcher have a career-wise adjusted p-value, or should there be a discipline-wise adjusted p-value? Should we publish an issue-wise adjusted p-value and a year-end-journal-wise adjusted p-value? Should our studies examine only one association at a time, thereby wasting valuable resources? No statistical theory provides answers for these practical issues, because it is impossible to formally account for an infinite number of potential inferences. The ‘right’ answer, then, will vary from case to case, and local conventions in particular disciplines will dictate what is normal and expected by supervisors and reviewers. However, there are a few common cases for which we can make some recommendations on what might constitue ‘good practice’. We do not claim that these are the ‘correct’ answers, but if you deviate from these recommendations it would be worth having a reasonable justification to hand! Multiple outcome variables In a clinical trial or other applied study you might want to measure more than one variable to detect changes across a variety of domains (e.g. both depression and anxiety). Although no firm consensus exists, it is common practice to: Analyse multiple outcomes without correction (and indicate this in the text) Designate one outcome from the study to be ‘primary’, and report this both in pre-trial protocols and reports of the results. See @feise2002multiple for more details. Multiple ‘timepoints’ for the outcome Another common case is where the same outcome variable is measured on multiple occasions to track progress over time. For example, an RCT might measure outcome at baseline, at the end of treatment, and again 12 months later. In the cases where there are only a few measurements of outcome made then you could choose between: Bonferroni correction of p values for the between-group difference at each timepoint or Designating one of the timepoints as the ‘primary’ outcome. In practice option 2 is more common, and probably makes more sense in applied settings where it is the longer-term prognosis of participants wich matters most. Pairwise comparisons in factorial designs Let’s say you have a complex complex factorial design and so multiple pairwise comparisons and other contrasts are possible. If you are only interested in a small number of the possible pairwise comparisons or specific contrasts then specify this up front. Either report the comparisons without correction (noting this in the text) or use FDR/Bonferroni correction for this small family of tests. If you are only interested in specific main effects or interactions from the full factorial model, then you should specify this in advance and report these F-tests or t-tests and associated p values without correction. It is perfectly reasonable to be interested only in specific pairwise comparisons contained within an Anova F-test. It is normal to report the F from the Anova, but it is not necessary for the F test to be significant before reporting the pairwise comparison of interest (with or without FDR/Bonferroni correction). If you have not specified the comparisons of interest in advance, then use FDR or Tukey correction for the complete set of pairwise tests. If you feel able to claim that some of these pairwise comparisons would never have been of interest then you could reduce the family size for this correction accordingly, but you should make this justification in your report. Practical examples Factorial Anova and pairwise comparisons In the Anova cookbook, we used a dataset from Howell’s textbook which recorded Recall among young v.s. older adults (Age) for each of 5 conditions: In an ideal world we would have published a trial protocol before collecting data, or at the least specified which comparisons were of interest to us. However for the purposes of this example I’ll assume you didn’t do this, and need to address the potential for mutliple comparisons accordingly. The design of the study suggests that we were interested in the effect of the experimental conditions on recall, and we might have predicted that these experimental manipulations would affect older and younder adults differently. Because we didn’t pre-specify our analyses, we should acknowledge that we would likly report any differences between any of the 5 experimental conditions (10 possible comparisons) and any differences between younger and older adults in any of those conditions (45 possible pairwise comparisons). Because replicating this experiment is relatively cheap (testing might only take 30 minutes per participant), and we are confident that other labs would want to replicate any significant findings we report, we are less concerned with the absolute rate of type 1 errors, but would like to limit our ‘false discovery rate’ (we have a repututation to maintain). We set our acceptable false discovery rate to 5%. We run an Anova model, including main effects for Age and Condition and their interaction: eysenck.model &lt;- lm(Recall ~ Age * Condition, data=eysenck) car::Anova(eysenck.model, type=3) %&gt;% pander() Anova Table (Type III tests) Sum Sq Df F value Pr(&gt;F) (Intercept) 490 1 61.05 9.85e-12 Age 1.25 1 0.1558 0.694 Condition 351.5 4 10.95 2.8e-07 Age:Condition 190.3 4 5.928 0.0002793 Residuals 722.3 90 NA NA We report that there is are significant main effects for Age, Condition, and a significant interaction for Age:Condition. At this point, if we hadn’t already plotted the raw data, we would certainly want to do that, or to make predictions for each cell in the design and plot them. Because the plot of the raw data (see above) suggested that the counting and rhyming conditions produced lower recall rates than the other conditions, we might want to report the relevant pairwise tests. We can use the the lsmeans() function from the lsmeans:: package to compute these: # use the lsmeans function which returns an lsm.list object. eysenck.lsm &lt;- lsmeans::lsmeans(eysenck.model, pairwise~Condition) ## NOTE: Results may be misleading due to involvement in interactions # the `contrasts` element in this list is what we want for now eysenck.lsm$contrasts ## contrast estimate SE df t.ratio p.value ## Counting - Rhyming -0.50 0.8958547 90 -0.558 0.9807 ## Counting - Adjective -6.15 0.8958547 90 -6.865 &lt;.0001 ## Counting - Imagery -8.75 0.8958547 90 -9.767 &lt;.0001 ## Counting - Intention -8.90 0.8958547 90 -9.935 &lt;.0001 ## Rhyming - Adjective -5.65 0.8958547 90 -6.307 &lt;.0001 ## Rhyming - Imagery -8.25 0.8958547 90 -9.209 &lt;.0001 ## Rhyming - Intention -8.40 0.8958547 90 -9.377 &lt;.0001 ## Adjective - Imagery -2.60 0.8958547 90 -2.902 0.0367 ## Adjective - Intention -2.75 0.8958547 90 -3.070 0.0231 ## Imagery - Intention -0.15 0.8958547 90 -0.167 0.9998 ## ## Results are averaged over the levels of: Age ## P value adjustment: tukey method for comparing a family of 5 estimates By default Tukey correction is applied for multiple comparisons, which is a reasonable choice. If you wanted to adjust for the false discovery rate instead, you can use the adjust argument: # not run, just shown as an example lsmeans::lsmeans(eysenck.model, pairwise~Condition, adjust=&quot;fdr&quot;) The plot also suggested that older and younger adults appeared to differ for the ‘adjective’, ‘imagery’, and ‘intention’ conditions. We can compute these detailed pairwise comparisons by including the interaction term, Age:Condition in the call to lsmeans(): eysenck.age.condition.lsm &lt;- lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition, adjust=&quot;fdr&quot;) eysenck.age.condition.lsm$contrasts %&gt;% broom::tidy() %&gt;% snip.middle(., 4) %&gt;% pander(missing=&quot;...&quot;, caption=&quot;8 of the 45 possible contrasts, FDR corrected&quot;, split.tables=Inf) 8 of the 45 possible contrasts, FDR corrected level1 level2 estimate std.error df statistic p.value Young,Counting Older,Counting 0.5 1.267 90 0.3947 0.7263 Young,Counting Young,Rhyming 0.1 1.267 90 0.07893 0.9373 Young,Counting Older,Rhyming -0.6 1.267 90 -0.4736 0.6824 Young,Counting Young,Adjective -4 1.267 90 -3.157 0.003251 … … … … … … … Young,Imagery Older,Intention -5.9 1.267 90 -4.657 2.612e-05 Older,Imagery Young,Intention 5.6 1.267 90 4.42 5.888e-05 Older,Imagery Older,Intention -1.7 1.267 90 -1.342 0.2288 Young,Intention Older,Intention -7.3 1.267 90 -5.762 3.972e-07 You should note that the FDR adjusted p values do not represent probabilities in the normal sense. Instead, the p value now indicates the false discovery rate at which the p value should be considered statistically significant. So, for example, if the adjusted p value = 0.09, then this indicates the contrast would be significant if the acceptable false discovery rate is 10%. Researchers often set their acceptable false discovery rate to be 5% out of habit or confusion with conventional levels for alpha, but it would be reasonable to set it to 10% or even higher in some circumstances. Whatever level you set, you need to be able to satisfy both yourself and a reviewer that the result is of interest: the particular values for alpha and FDR you agree on are arbitrary conventions, provided you both agree the finding is worthy of consideration! Extracting pairwise comparisons to display in your report In the code above we request FDR-adjusted p values, and then use the broom::tidy() function to convert the table into a dataframe, and then use pander to display the results as a table. For more information on extracting results from models and other R objects and displaying them properly see the section: models are data too. Adjusting for the FDR by hand If you have a set of tests for which you want to adjust for the FDR it’s easy to use R’s p.adjust() to do this for you. set.seed(5) p.examples &lt;- data_frame(p = runif(4, min=.001, max=.07)) %&gt;% mutate(p.fdr = p.adjust(p, method=&quot;fdr&quot;), p.sig = ifelse(p &lt; .05, &quot;*&quot;, &quot;&quot;), p.fdr.sig = ifelse(p.fdr &lt; .05, &quot;*&quot;, &quot;&quot;)) p.examples %&gt;% arrange(p) %&gt;% pander(caption=&quot;Randomly generated &#39;p values&#39;, with and without FDR correction applied.&quot;) Randomly generated ‘p values’, with and without FDR correction applied. p p.fdr p.sig p.fdr.sig 0.01481 0.04125 * * 0.02062 0.04125 * * 0.04828 0.06426 * 0.06426 0.06426 We can plot these values to see the effect the adjustment has: Figure 21.1: Example of the adjustments made to p values by FDR and Bonferroni methods. Unadjusted p values are shown in black. Note how conservative Bonferroni is with even this small number of comparisons to correct for. Other contrasts The lsmeans:: package provides tools to compute contrasts for Anova and other linear models: library(lsmeans) We can see a number of these different contrasts below. First, we run a model model including the factor Days: slope.model &lt;- lmer(Reaction ~ factor(Days) + (1|Subject), data=lme4::sleepstudy) And then we create the lsmeans object: slope.model.lsm &lt;- lsmeans::lsmeans(slope.model, ~Days) We can use this lsm object to calculate various contrasts we might want to see. First, consecutive contrasts where each level is compared with the next: lsmeans::contrast(slope.model.lsm, &quot;consec&quot;) ## contrast estimate SE df t.ratio p.value ## 1 - 0 7.8439500 10.47531 152.99 0.749 0.9887 ## 2 - 1 0.8661444 10.47531 152.99 0.083 1.0000 ## 3 - 2 17.6301111 10.47531 152.99 1.683 0.5278 ## 4 - 3 5.6574111 10.47531 152.99 0.540 0.9988 ## 5 - 4 19.8690333 10.47531 152.99 1.897 0.3777 ## 6 - 5 3.6598000 10.47531 152.99 0.349 1.0000 ## 7 - 6 6.5723278 10.47531 152.99 0.627 0.9965 ## 8 - 7 17.8789222 10.47531 152.99 1.707 0.5103 ## 9 - 8 14.2217167 10.47531 152.99 1.358 0.7627 ## ## P value adjustment: mvt method for 9 tests Polynomial contrasts allow us to test whether the increase in RT’s over the days is linear or curved: lsmeans::contrast(slope.model.lsm, &quot;poly&quot;, adjust=&quot;fdr&quot;, max.degree=3) ## contrast estimate SE df t.ratio p.value ## linear 1727.10218 134.5578 152.99 12.835 &lt;.0001 ## quadratic 88.97389 85.1018 152.99 1.045 0.4462 ## cubic -65.30289 686.1126 152.99 -0.095 0.9243 ## ## P value adjustment: fdr method for 3 tests If we don’t specify a contrasts argument, or a type of correction, we will get Tukey contrasts for every pairwise comparison: # 45 tests - results not shown because similar to those shown above lsmeans::contrast(slope.model.lsm, &quot;tukey&quot;) If we want to compare particular levels of Days with a specific reference level we can use the trt.vs.ctrl contrast type. In this example we are comparing each day with day 0, because day 0 is the first in the list of days and we specified ref=1 (try running the comparisons against day 5). lsmeans::contrast(slope.model.lsm, &quot;trt.vs.ctrl&quot;, ref=1, adjust=&quot;fdr&quot;) ## contrast estimate SE df t.ratio p.value ## 1 - 0 7.843950 10.47531 152.99 0.749 0.4551 ## 2 - 0 8.710094 10.47531 152.99 0.831 0.4551 ## 3 - 0 26.340206 10.47531 152.99 2.515 0.0167 ## 4 - 0 31.997617 10.47531 152.99 3.055 0.0040 ## 5 - 0 51.866650 10.47531 152.99 4.951 &lt;.0001 ## 6 - 0 55.526450 10.47531 152.99 5.301 &lt;.0001 ## 7 - 0 62.098778 10.47531 152.99 5.928 &lt;.0001 ## 8 - 0 79.977700 10.47531 152.99 7.635 &lt;.0001 ## 9 - 0 94.199417 10.47531 152.99 8.993 &lt;.0001 ## ## P value adjustment: fdr method for 9 tests And if we want to compare each level against the grand mean of the sample, the eff contrast type is short for effect. A way of thinking of this (in English) is that we are asking ‘what is the effect of being on a particular day, relative the other average of all days’. lsmeans::contrast(slope.model.lsm, &quot;eff&quot;, adjust=&quot;fdr&quot;) ## contrast estimate SE df t.ratio p.value ## 0 effect -41.856086 7.027049 152.99 -5.956 &lt;.0001 ## 1 effect -34.012136 7.027049 152.99 -4.840 &lt;.0001 ## 2 effect -33.145992 7.027049 152.99 -4.717 &lt;.0001 ## 3 effect -15.515881 7.027049 152.99 -2.208 0.0410 ## 4 effect -9.858469 7.027049 152.99 -1.403 0.1627 ## 5 effect 10.010564 7.027049 152.99 1.425 0.1627 ## 6 effect 13.670364 7.027049 152.99 1.945 0.0670 ## 7 effect 20.242692 7.027049 152.99 2.881 0.0076 ## 8 effect 38.121614 7.027049 152.99 5.425 &lt;.0001 ## 9 effect 52.343331 7.027049 152.99 7.449 &lt;.0001 ## ## P value adjustment: fdr method for 10 tests To list all the availble contrast types, or see the relevant help files, you can type: # help file describing all the contrasts families help(pairwise.lsmc) Contrasts for interactions Let’s recall our example of factorial Anova: car::Anova(eysenck.model, type=3) %&gt;% pander Anova Table (Type III tests) Sum Sq Df F value Pr(&gt;F) (Intercept) 490 1 61.05 9.85e-12 Age 1.25 1 0.1558 0.694 Condition 351.5 4 10.95 2.8e-07 Age:Condition 190.3 4 5.928 0.0002793 Residuals 722.3 90 NA NA We can compute pairwise comparisons within the interaction as follows: eysenck.lsm &lt;- lsmeans::lsmeans(eysenck.model, ~Age:Condition) And the various contrasts, here comparing each cell agains the grand mean: lsmeans::contrast(eysenck.lsm, &quot;eff&quot;, adjust=&quot;fdr&quot;) ## contrast estimate SE df t.ratio p.value ## Young,Counting effect -4.61 0.8498823 90 -5.424 &lt;.0001 ## Older,Counting effect -5.11 0.8498823 90 -6.013 &lt;.0001 ## Young,Rhyming effect -4.71 0.8498823 90 -5.542 &lt;.0001 ## Older,Rhyming effect -4.01 0.8498823 90 -4.718 &lt;.0001 ## Young,Adjective effect -0.61 0.8498823 90 -0.718 0.5275 ## Older,Adjective effect 3.19 0.8498823 90 3.753 0.0004 ## Young,Imagery effect 1.79 0.8498823 90 2.106 0.0475 ## Older,Imagery effect 5.99 0.8498823 90 7.048 &lt;.0001 ## Young,Intention effect 0.39 0.8498823 90 0.459 0.6474 ## Older,Intention effect 7.69 0.8498823 90 9.048 &lt;.0001 ## ## P value adjustment: fdr method for 10 tests Or all of the pairwise tests with Tukey adjustment: lsmeans::contrast(eysenck.lsm, &quot;tukey&quot;) %&gt;% broom::tidy() %&gt;% snip.middle(4) %&gt;% pander(missing=&quot;...&quot;, caption=&quot;Some of the Tukey corrected pairwise contrasts (table abbreviated)&quot;) Some of the Tukey corrected pairwise contrasts (table abbreviated) level1 level2 estimate std.error df statistic p.value Young,Counting Older,Counting 0.5 1.267 90 0.3947 1 Young,Counting Young,Rhyming 0.1 1.267 90 0.07893 1 Young,Counting Older,Rhyming -0.6 1.267 90 -0.4736 1 Young,Counting Young,Adjective -4 1.267 90 -3.157 0.06328 … … … … … … … Young,Imagery Older,Intention -5.9 1.267 90 -4.657 0.0004513 Older,Imagery Young,Intention 5.6 1.267 90 4.42 0.001095 Older,Imagery Older,Intention -1.7 1.267 90 -1.342 0.9409 Young,Intention Older,Intention -7.3 1.267 90 -5.762 5.006e-06 "],
["clustering.html", "22 Non-independence", " 22 Non-independence Psychological data often contains natural groupings. In intervention research, multiple patients may be treated by individual therapists, or children taught within classes, which are further nested within schools; in experimental research participants may respond on multiple occasions to a variety of stimuli. Although disparate in nature, these groupings share a common characteristic: they induce dependency between the observations we make. That is, our data points are not independently sampled from one another. What this means is that observations within a particular grouping will tend, all other things being equal, be more alike than those from a different group. Why does this matter? Think of the last quantitative experiment you read about. If you were the author of that study, and were offered 10 additional datapoints for ‘free’, which would you choose: 10 extra datapoints from existing participants. 10 data points from 10 new participants. In general you will gain more new information from data from a new participant. Intuitively we know this is correct because an extra observation from someone we have already studies is less likely to surprise us or be different from the data we already have than an observation from a new participant. Most traditional statistical models assume that data are sampled independently however. And the precision of the inferences we can draw from from statistical models is based on the amount of information we have available. This means that if we violate this assumption of independent sampling we will trick our model into thinking we have more information than we really do, and our inferences may be wrong. "],
["fixed-or-random.html", "23 Fixed and random effects", " 23 Fixed and random effects As noted by @gelman2005analysis (and summarised here), the terms ‘fixed’ and ‘random’ are used very loosely in both the methodological and applied literature. Gelman identifies 5 different senses in which the distinction between fixed and random effects can be drawn, and this inconsistency can lead to confusion. For practical purposes, if you think that you have some form of grouping in your data and that it makes sense to think of variation in outcomes between these groups then you should probably include it as a random intercept in your model. Likewise, if you include a predictor in your model and it is reasonable to think that the effect of this predictor would vary between groups in the data (e.g., between individuals) then you should include a random slope effect for this variable. Random intercepts Some example of groupings which should be included as random intercepts: Participants Classes and Schools Therapists or treatment providers (e.g. in cluster randomised trial) Stimuli or ‘items’ Groupings which are not clear cut in either direction: A smallish number of experimental conditions which could be thought of as ‘sampled’ from a larger population of possible groupings [@gelman2005analysis]. An example here would be groups which recieve different doses of a drug. Examples of groupings which are probably not best handled as random intercepts: Experimental conditions especially where the conditions are qualitatively different (although the interventions might warrant inclusion as a random slope, see below). Random slopes Where the effect of a variable might vary between individuals (or other grouping) should be considered for inclusion as a random slopes. Some examples might include: Time (or some function of time) An experimental intervention (e.g. in a factorial design) For a more in depth discussion of when to include a random slope this presentation and transcript from the Bristol CMM is excellent. "],
["link-functions.html", "24 Link functions", " 24 Link functions Standard regression models are linear combinations which allow predicted values to range between negative -∞ (infinity) and +∞. If you think about it, this makes sense, because there is no bound on either the parameters we fit (the regression coefficients) or the predictor values. Where outcome data are continuous and (in theory or if we make a reasonable assumption) this isn’t a problem. However for binary or count data this isn’t the case. For binary data we want to predict the probability of a positive response, and this can range between zero and 1. For count data, predicted outcomes must always be non-negative (i.e. zero or greater). Logistic and poisson regression extend regular linear regression to allow this to happen by using different ‘link functions’. These link functions transform the data, which are constrained to 0/1 or non-negative integers, to the range of the underlying linear model, which allows values between -∞ and +∞. So, in generalised linear regression our linear combination is predicting a transformed outcome. Logistic regression When we have binary data, we want to be able run something like regression, but where we predict a probability of the outcome. Because probabilities are limited to between 0 and 1, to link the data with the linear model we need to transform so they range from -∞ (infinity) to +∞. You can think of the solution as coming in two steps: Step 1 We can transform a probability on the 0—1 scale to a 0 ∞ scale by converting it to odds, which are expressed as a ratio: \\[\\textrm{odds} = \\dfrac{\\textrm{probability}}{1-\\textrm{probability}}\\] Probabilities and odds ratios are two equivalent ways of expressing the same idea. So a probability of .5 equates to an odds ratio of 1 (i.e. 1 to 1); p=.6 equates to odds of 1.5 (that is, 1.5 to 1, or 3 to 2), and p = .95 equates to an odds ratio of 19 (19 to 1). Odds convert or map probabilities from 0 to 1 onto the real numbers from 0 to ∞. Figure 9.1: Probabilities converted to the odds scale. As p approaches 1 Odds goes to infinity. We can reverse the transformation too (which is important later) because: \\[\\textrm{probability} = \\dfrac{\\textrm{odds}}{1+\\textrm{odds}}\\] If a bookie gives odds of 66:1 on a horse, what probability does he think it has of winning? Why do bookies use odds and not probabilities? Should researchers use odds or probabilities when discussing with members of the public? Step 2 When we convert a probability to odds, the odds will always be &gt; zero. This is still a problem for our linear model. We’d like our ‘regression’ coefficients to be able to vary between -∞ and ∞. To avoid this restriction, we can take the logarithm of the odds — sometimes called the logit. The figure below shows the transformation of probabilities between 0 and 1 to the log-odds scale. The logit has two nice properties: It converts odds of less than one to negative numbers, because the log of a number between 0 and 1 is always negative[^1]. It flattens the rather square curve for the odds in the figure above, and Figure 5.1: Probabilities converted to the logit (log-odds) scale. Notice how the slope implies that as probabilities approach 0 or 1 then the logit will get very large. Reversing the process to interpret the model As we’ve seen here, the logit or logistic link function transforms probabilities between 0/1 to the range from negative to positive infinity. This means logistic regression coefficients are in log-odds units, so we must interpret logistic regression coefficients differently from regular regression with continuous outcomes. In linear regression, the coefficient is the change the outcome for a unit change in the predictor. For logistic regression, the coefficient is the change in the log odds of the outcome being 1, for a unit change in the predictor. If we want to interpret logistic regression in terms of probabilities, we need to undo the transformation described in steps 1 and 2. To do this: We take the exponent of the logit to ‘undo’ the log transformation. This gives us the predicted odds. We convert the odds back to probability. A hypothetical example Imagine if we have a model to predict whether a person has any children. The outcome is binary, so equals 1 if the person has any children, and 0 otherwise. The model has an intercept and one predictor, \\(age\\) in years. We estimate two parameters: \\(\\beta_0 = 0.5\\) and \\(\\beta_{1} = 0.02\\). The outcome (\\(y\\)) of the linear model is the log-odds. The model prediction is: \\(\\hat{y} = \\beta_0 + \\beta_1\\textrm{age}\\) So, for someone aged 30: the predicted log-odds = \\(0.5 + 0.02 * 30 = 1.1\\) the predicted odds = \\(exp(1.1) = 3.004\\) the predicted probability = \\(3.004 / (1 + 3.004) = .75\\) For someone aged 40: the predicted log-odds = \\(0.5 + 0.02 * 40 = 1.3\\) the predicted odds = \\(exp(1.3) = 3.669\\) the predicted probability = \\(3.669 / (1 + 3.669) = .78\\) 24.0.0.1 Regression coefficients are odds ratios One final twist: In the section above we said that in logistic regression the coefficients are the change in the log odds of the outcome being 1, for a unit change in the predictor. Without going into too much detail, one nice fact about logs is that if you take the log of two numbers and subtract them to take the difference, then this is equal to dividing the same numbers and then taking the log of the result: A &lt;- log(1)-log(5) B &lt;- log(1/5) # we have to use rounding because of limitations in # the precision of R&#39;s arithmetic, but A and B are equal round(A, 10) == round(B, 10) ## [1] TRUE This means that change in the log-odds is the same as the ratio of the odds So, once we undo the log transformation by taking the exponent of the coefficient, we are left with the odds ratio. You can now jump back to running logistic regression. "],
["comparison-and-selection.html", "25 Building and choosing models", " 25 Building and choosing models Like maps, models are imperfect but useful Maps are always imperfect, and the decisons we make as we build them entail different visions of how the world is. For example, when making maps cartographers must decide at what size to print it, and how much detail to include. The smaller the map the more convenient it is to carry, but a smaller map can never represent reality as accurately. Similarly, we must leave out details to make the map practical to use; but what we omit depends on the planned use of the map. Stretching he analogy a little, we could evaluate maps according to their performance: did the map help us navigate to a desired location, for example? This is likely to be a function of two things: Veracity: that is, how faithfully does the map represent the ‘reality’ of the terrain it covers? Simplicity: does the map include details which are ephemeral, or irrelvant to the task in hand? At one extreme, we might take an aerial photo of a city, which would accurately represent the terrain, but would include many details which are ephemeral or transient and not necessary to the task of, for example, finding the city’s railway station. Indeed the photo is literally a snapshot of the city on a given day, and some elements might be unhelpful in navigating the city several months later; the photo might include temporary roadworks, for example, which could lead the reader to assume a street was blocked to traffic when this was not normally the case. At the other extreme, we might consider a highly stylised cartoon map of the same city, which includes onlu major roads and the station itself, but none of the secondary roads or other landmarks. Here the map is so lacking in detail that it may be hard to orient ourselves to the actual terrain. Statistical models are not exactly like maps, but we can evaluate them using similar criteria. A good model enables the user to make sense of the data they collect (i.e. navigate the city) and this involves making reliable predictions. Reliable predictions are possible when the model accurately represents the underlying reality of a phenomena, but also when the model is simple enough to ignore ephemeral or irrelevant information. Evaluating and selecting statistical models involves just this kind of tradeoff between veracity and simplicity. To stretch the analogy above, when we collect data, we are taking a photo of the city, making a snapshot of a given process at a particular place and time. This snapshot (hopefully) contains valuable information about how the process works in general, but it also includes noise and irrelevant information which could distract us, preventing us from using the data to accurately predict the future. Overfitting/underfitting To stealuse a different analogy, imagine you want to predict who will win the next US presidential election: Cartoon: xkcd.com In total there have only been 57 presidential elections and 44 presidents in the US, which isn’t a lot of data to learn from. The point of the cartoon is that if we try and include ephemeral predictors like beards and false teeth then the space of possible combinations becomes huge. This means that some of these combinations will start to fit the data because of chance variations, and not because they represent some fundamental aspect of the process generating the outcome (election wins). If you allow statistical models to expand in this way (that is, driven only by patterns in the data you have) then you will find that, inevitably, the model fits the data you have, but the you won’t be able to predict new data at all. This is one of the reasons why we prefer simpler models, all other things being equal. It also explains why practices like stepwise regression, where we add or remove variables depending on whether they are ‘statistically significant’, are almost always a bad idea. We should only accept a complex model (e.g. one including false teeth as a predictor of the presidency) if we have enough evidence to be confident this will predict new data. Choosing the ‘right variables’ As an aside, if you do find yourself in a situation with a large number of predictors and hope to build a simpler model — either with the goal of prediction or theory development — then it’s hopefully obvious from the discussion above that simply dropping or selecting predictors based on p values is unlikely to lead to good science. Not only will you tend to over-fit the data you have, but you may also discount valid predictor variables because of problems like multicollinearity (correlations between predictors). In other disciplines where inference from observational data are more common (e.g. ecology and biological anthropolgy) the concepts of model averaging or multimodal inference have gained currency, and may be be useful to applied psychologists. The core concept is that if we have several models, all of which are approximately as good as each other, then we should make predictions and inferences by averaging across the models, weighting each model by how well it fits the data: that is, we give the most weight to the best fitting model, but don’t completely discount other models which fit almost as well. See @mcelreath2016rethinking, chapter 6, for a good introduction. This video explains the concept well. The R package MuMln package implements most of the methods needed. See also this interesting post on model selection problems. "],
["references.html", "References", " References "]
]
