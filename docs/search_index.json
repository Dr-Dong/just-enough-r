[
["index.html", "‘Just enough’ R 1 Introduction A warning", " ‘Just enough’ R 1 Introduction R is software which makes it easy to work with and learn from data. It also happens to be a complete programmming language, but if you’re reading this guide then that might not be of interest to you. That’s OK — the goal here is not to teach you how to program in R1. The goal is to teach you just enough R to be confident to explore your data. We are going to use R the same way we use any other statistics software: To check and visualise data, run statistical analyses, and share our results with others. To do that it’s worth learning the absolute basics of the R language. The next few chapters walk you through those basics: what you need to know to be productive in R and RStudio. By analogy, imagine going on holiday and learning enough French to introduce yourself, count from one to ten, and order a baguette. You won’t be fluent - but you won’t starve, and your trip will be more fun. Part 1: ‘Must read’ Part 1 is meant to be read from beginning to end by those starting from scratch. Getting started Datasets and dataframes Manipulating dataframes (select and filter) Summarising data (split, apply, combine) Visualising data (layering graphics with ggplot2) Part 2: Nice to know Part 2 can be read sequentially, but the chapters are also designed to work as standalone guides for specific techniques. Working with real data Correlations t-tests Linear models Anova Understanding interactions (visualising interactions in raw data) Predictions and marginal effects Mediation Linear mixed models (multilevel models) Meta analysis Confirmatory factor analysis Structural Equation Modelling Power analysis … 2 Part 3: Loose ends Part 3 should be used interactively to answer questions that arise along the way. Some of the content here is not specific to R, but may be useful in interpreting the output of some of the techniques taught in sections 1 and 2. Installing RStudio Installing packages Handling missing values in R Using RMarkdown effectively (e.g. using pander and knitr) Confidence intervals vs. prediction intervals A warning This guide is extremely opinionated. There are many ways to get things done with R, but trying to learn them all at once causes unhappiness. In particular, lots of the base R functions are old, quirky, inconstently named, and hard to remember. This guide recommends that you use several new ‘packages’ that replicate and extend some of R’s basic functionality. Using this new set of packages, which are very thoughtfull designed and work together nicely, will help you form a more consistent mental model of how to work in R. You can learn the crufty old bits (which do still have their uses) later on. I also assume you are using the RStudio editor and working in an RMarkdown document (see the next section). This is important because this guide itself is written in RMarkdown, and editing it will be an important part of the learning process. If you don’t have access to RStudio yet, see the installation guide. This is actually a lie, but I’m hoping you won’t notice until it’s too late.↩ It would also be lovely to have chapters on multiple imputation, power analysis, simulation, Bayesian modelling and much else. These aren’t planned imminently, but contributions are welcome.↩ "],
["start_here.html", "2 Getting started with R and RStudio 2.1 Using RMarkdown to record and share work 2.2 Writing and ‘knitting’ RMarkdown 2.3 RStudio 2.4 Your first R commands 2.5 Naming things: variable assignment 3 Vectors and lists 3.1 Vectors 3.2 Lists 3.3 Questions on vectors and lists", " 2 Getting started with R and RStudio 2.1 Using RMarkdown to record and share work This it might seem an odd place to start: we haven’t got anything to share yet! But the RStudio editor (see below) includes important features which help us record and organise our work, and share it with colleagues. For many people this ability to keep a detailed record of your work, and revisit and review it later, turns out to be the major advantages of R over traditional statistics packages. You are currently reading the output of an ‘RMarkdown’ document. ‘R’ is a computer language for working with data. Markdown is a simple text format which allows you to combine writing, images and code (see http://commonmark.org/help/). An RMarkdown document mixes R code with markdown. This means you can combine your analysis with text explaining and interpreting the results. RMarkdown is easily converted to other formats like HTML, Word, or PDF to share with other people. When you click the Knit button (in the Rstudio interface), a document will be generated that combines your text with the results of your R code. 2.2 Writing and ‘knitting’ RMarkdown To include R code within a document we write 3 backticks (```), followed by {r}. We the include our R code, and close the block with 3 more backticks. ```{r} 2 + 2 ``` When a document including this chunk is run or ‘knitted’, the final result will include the the line 2+2 followed by the number 4 on the next line. This means we can use RMarkdown to ‘show our workings’: our analysis can be interleaved with narrative text to explain or interpret the calculations. You can see how this works in practice in the next section. 2.3 RStudio RStudio is a text editor which has been customised to make working with R easy. It can be installed on your own computer, or you can login to a shared RStudio server3 from a web browser. Either way the interface is largely the same. The figure above shows the main RStudio interface, comprising: The main R-script or RMarkdown editor window The R console, into which you can type R commands, and see output from commands run in the script editor. The ‘environment’ panel, which lists all the variables you have defined and currently available to use. The files and help panel. Within this the files tab enables you to open files stored on the server, or in the current project on your disk. You can see a short video demonstrating the RStudio interface here: The video4: Shows you how to type commands into the Console and view the results. Run a plotting function, and see the result. Create RMarkdown file, and ‘Knit’ it to produce a document containing the results of your code and explanatory text. Once you have watched the video: Try creating a new RMarkdown document in RStudio. Edit some of the text, and press the Knit button to see the results. If you feel brave, edit one of the R blocks and see what happens! 2.4 Your first R commands You can type R commands directly into the console and see the result there too, but you should make a habit of working in an RMarkdown file. This keeps a record of everything you try, and makes it easy to edit/amend commands which don’t work as you expect. Now would be a good time to open and RMarkdown document to see how it works. A good place to start would be to open the source to this document. The best way to do this is to download the source code for this project, and then open the file start_here.Rmd. The source is available here: https://github.com/benwhalley/just-enough-r/archive/master.zip 5 To run code in the RStudio interface put your cursor on a line within an R Block (or select the code you want to run), and press Ctrl-Enter. The result will appear below the code block. The command in the R block below prints (shows on screen) the first few rows of the mtcars dataset, which is built in to R as an example. Place your cursor somewhere in the line the command is on and run it by typing Ctrl-Enter: head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 If you are reading this from within RStudio, running head(mtcars) will have included an interactive table in the document, which you can use this to view the mtcars dataset. If you are still reading the compiled html or pdf document you will see a table containing the same data, included within the body of the document. Hopefully at this point it’s obvious that RStudio and RMarkdown give you: A nice place to work with R and explore your data A nice format to share your workings (e.g. with other researchers or your tutor) A mechanism to save reports of your analysis, to share with other people who don’t use RStudio 2.5 Naming things: variable assignment One of the nice things about R is that we can assign labels to parts of our analysis to keep track of them. Using good, descriptive names is good practice and makes your code easier to read (e.g. when you or others need to revisit in future. To assign labels to particular values we use the &lt;- symbol. That is, we have a value and use the &lt;- symbol to point to the variable we want to assign it to. For example: meaning.of.life &lt;- 42 This assigns the value 42 to the variable meaning.of.life. This block wouldn’t display anything because assigning a variable doesn’t create any output. To both assign a variable and display it we would type: meaning.of.life &lt;- 42 meaning.of.life ## [1] 42 Or if we want to be explicit: print(meaning.of.life) ## [1] 42 Helpfully, we can also do simple calculations as we assign variables: one.score &lt;- 20 four.score.years.and.ten &lt;- one.score * 4 + 10 print(four.score.years.and.ten) ## [1] 90 As you will see below, we can give anything a label by assigning it to a variable. It doesn’t have to be simple numbers: we can also assign words, graphics and plots, the results of a statistical model, or lists of any of these things. 3 Vectors and lists When working with data, we often have lists of ‘things’: for example a list of measurements we have made. When all the things are of the same ‘type’, then in R this is called a vector. When the list contains a mix of different things, then R calls it a list In general we should avoid mixing up different types of ‘thing’, and so use vectors wherever possible. 3.1 Vectors We can create a vector of numbers and display it like this: # this creates a vector of heights, in cm heights &lt;- c(203, 148, 156, 158, 167, 162, 172, 164, 172, 187, 134, 182, 175) The c() command is shorthand for combine, so the example above combines the individual elements (numbers) into a new vector. We can create a vector of alphanumeric names just eas easily: names &lt;- c(&quot;Ben&quot;, &quot;Joe&quot;, &quot;Sue&quot;, &quot;Rosa&quot;) And we can check the values stored in these variables by printing them, e.g.: heights ## [1] 203 148 156 158 167 162 172 164 172 187 134 182 175 Try creating your own vector of numbers in a new code block below6 using the c(...) command. Then change the name of the variable you assign it to. 3.1.1 Accessing elements within vectors Once we have created a vector, we often want to access the individual elements again. We do this based on their position. Let’s say we have created a vector: my.vector &lt;- c(10, 20, 30, 40) We can display the whole vector by just typing it’s name, as we saw above. But if we want to show only the first element of this vector, we type: my.vector[1] ## [1] 10 Here, the square brackets specify a subset of the vector we want - in this case, just the first element. 3.1.2 Selecting more than one element in a vector A neat feature of subsetting is that we can grab more than one element at a time. To do this, we need to tell R the positions of the elements we want, and so we provide a vector of the positions of the elements we want. It might seem obvious, but the first element has position 1, the second has position 2, and so on. So, if we wanted to extract the 4th and 5th elements from the vector of heights we saw above we would type: elements.to.grab &lt;- c(4, 5) heights[elements.to.grab] ## [1] 158 167 We can also make a subset of the original vector and assign it to a new variable: first.two.elements &lt;- heights[c(1, 2)] first.two.elements ## [1] 203 148 --> 3.1.3 Processing vectors Many of R’s most useful functions process vectors of numbers in some way. For example, if we want to calculate the average of our vector of heights we just type: mean(heights) ## [1] 167.6923 R contains lots of built in functions which we can use to summarise a vector of numbers. For example: median(heights) ## [1] 167 sd(heights) ## [1] 17.59443 min(heights) ## [1] 134 max(heights) ## [1] 203 range(heights) ## [1] 134 203 IQR(heights) ## [1] 17 length(heights) ## [1] 13 All of these functions accept a vector as input, do some proccesing, and then return a single number which gets displayed by RStudio. But not all functions return a single number in the way that mean did above. Some return a new vector, or some other type of object instead. For example, the quantile function returns the values at the 0, 25th, 50th, 75th and 100th percentiles (by default). height.quantiles &lt;- quantile(heights) height.quantiles ## 0% 25% 50% 75% 100% ## 134 158 167 175 203 If a function returns a vector, we can use it just like any other vector: height.quantiles &lt;- quantile(heights) # grab the third element, which is the median height.quantiles[3] ## 50% ## 167 # assign the first element to a variable min.height &lt;- height.quantiles[1] min.height ## 0% ## 134 But other functions process a vector without returning any numbers. For example, the hist function returns a histogram: hist(heights) We’ll cover lots more plotting and visualisation later on. 3.1.4 Processing vectors to make new vectors So far we’ve seen R functions which process a vector of numbers and produce a single number, a new vector of a different length (like quantile or fivenum), or some other object (like hist which makes a plot). However many other functions accept a single input, do something to it, and return a single processed value. For example, the square root function, sqrt, accepts a single value and returns a single value: running sqrt(10) will return 3.1623. In R, if a function accepts a single value as input and returns a single value as output (like sqrt(10)), then you can usually give a vector as input too. Some people find this surprising7, but R assumes that if you’re processing a vector of numbers, you want the function applied to each of them in the same way. This turns out to be very useful. For example, let’s say we want the square root of each of the elements of our height data: # these are the raw values heights ## [1] 203 148 156 158 167 162 172 164 172 187 134 182 175 # takes the sqrt of each value and returns a vector of all the square roots sqrt(heights) ## [1] 14.24781 12.16553 12.49000 12.56981 12.92285 12.72792 13.11488 ## [8] 12.80625 13.11488 13.67479 11.57584 13.49074 13.22876 This also works with simple arithmetic So, if we wanted to convert all the heights from cm to meters we could just type: heights / 100 ## [1] 2.03 1.48 1.56 1.58 1.67 1.62 1.72 1.64 1.72 1.87 1.34 1.82 1.75 This trick also works with other functions like paste, which combines the inputs you send it to produce an alphanumeric string: paste(&quot;Once&quot;, &quot;upon&quot;, &quot;a&quot;, &quot;time&quot;) ## [1] &quot;Once upon a time&quot; If we send a vector to paste it assumes we want a vector of results, with each element in the vector pasted next to each other: bottles &lt;- c(100, 99, 98, &quot;...&quot;) paste(bottles, &quot;green bottles hanging on the wall&quot;) ## [1] &quot;100 green bottles hanging on the wall&quot; ## [2] &quot;99 green bottles hanging on the wall&quot; ## [3] &quot;98 green bottles hanging on the wall&quot; ## [4] &quot;... green bottles hanging on the wall&quot; In other programming languages we might have had to write a ‘loop’ to create each line of the song, but R lets us write short statements to summarise what needs to be done; we don’t need to worry worrying about how it gets done. 3.1.5 Making up data (new vectors) Sometimes you’ll need to create vectors containing regular sequences or randomly selected numbers. To create regular sequences a convenient shortcut is the ‘colon’ operator. For example, if we type 1:10 then we get a vector of numbers from 1 to 10: 1:10 ## [1] 1 2 3 4 5 6 7 8 9 10 The seq function allows you to create more specific sequences: # make a sequence, specifying the interval between them seq(from=0.1, to=2, by=.1) ## [1] 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.1 1.2 1.3 1.4 1.5 1.6 1.7 ## [18] 1.8 1.9 2.0 We can also use random number-generating functions built into R to create vectors: # 10 uniformly distributed random numbers between 0 and 1 runif(10) ## [1] 0.02315374 0.03862412 0.68340757 0.59057499 0.21861662 0.98611333 ## [7] 0.60367464 0.30426355 0.88933960 0.35285989 # 1,000 uniformly distributed random numbers between 1 and 100 my.numbers &lt;- runif(1000, 1, 10) # 10 random-normal numbers with mean 10 and SD=1 rnorm(10, mean=10) ## [1] 11.555879 10.937296 11.274370 9.582144 9.619570 10.047089 8.644861 ## [8] 9.400595 12.748728 10.296794 # 10 random-normal numbers with mean 10 and SD=5 rnorm(10, 10, 5) ## [1] 11.750853 7.761015 17.518838 8.666787 16.711495 15.643151 4.219275 ## [8] 8.140572 8.395004 10.046175 We can then use these numbers in our code, for example plotting them: random.numbers &lt;- rnorm(10000) hist(random.numbers) 3.1.6 Useful functions to learn now There are thousands of functions built into R. Below are a few examples which are likely to be useful as you work with your data: # repeat something N times rep(&quot;Apple pie&quot;, 10) ## [1] &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; ## [6] &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; &quot;Apple pie&quot; # repeat a short vector, combining into a single longer vector rep(c(&quot;Custard&quot;, &quot;Gravy&quot;), 5) ## [1] &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; &quot;Custard&quot; ## [8] &quot;Gravy&quot; &quot;Custard&quot; &quot;Gravy&quot; # make a sequence and then sort it countdown &lt;- 100:1 sort(countdown) ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 ## [18] 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 ## [35] 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ## [52] 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 ## [69] 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [86] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 # generate some random data (here, ages in years) ages &lt;- round(rnorm(10, mean=40, sd=10)) # get the rank order of elements (i.e. what their positions would be if the vector was sorted) ages ## [1] 51 57 45 32 42 43 40 43 36 51 rank(ages, ties.method=&quot;first&quot;) ## [1] 8 10 7 1 4 5 3 6 2 9 # you could even label the vector of ages if you wanted labels &lt;- paste(&quot;Position:&quot;, rank(ages, ties.method=&quot;first&quot;)) names(ages) &lt;- labels ages ## Position: 8 Position: 10 Position: 7 Position: 1 Position: 4 ## 51 57 45 32 42 ## Position: 5 Position: 3 Position: 6 Position: 2 Position: 9 ## 43 40 43 36 51 # return the unique values in a vector unique( rep(1:10, 100) ) ## [1] 1 2 3 4 5 6 7 8 9 10 Try and experiment with each of these functions. Check the output against what you expected to happen, and make sure you understand what they do. 3.2 Lists Try running the code below: confusing.vector &lt;- c(1, 2, 3, &quot;Wibble&quot;) first.element &lt;- confusing.vector[1] sqrt(first.element) ## Error in sqrt(first.element): non-numeric argument to mathematical function Take a minute to try and make a guess at what went wrong. Why does R complain that ‘1’ is non-numeric? When we built the vector we used c to combine the elements 1, 2, 3 and &quot;Wibble&quot;. Although our first and second elements are numbers, &quot;Wibble&quot; is not - it’s made up of letters (this is called a character string). Remember that vectors can only contain one type of thing. And so R automatically converts all the elements to the same type, if it can. Because R can’t reliably convert &quot;Wibble&quot; to a number, everything in the vector was converted to the character type instead. We get an error because R can’t mutiply words together. If you’re not sure what type of thing your vector contains, you can use the typeof command: typeof(1:10) ## [1] &quot;integer&quot; typeof(runif(10)) ## [1] &quot;double&quot; typeof(c(1, 2, &quot;Wibble&quot;)) ## [1] &quot;character&quot; Here the meaning of integer should be self explanatory. The vector runif(10) has type double, because it contains ‘double-precision’ floating point numbers. For our purposes you can just think of double as meaning any number with decimal places. The last vector has the type character because it includes the character string Wibble, and all the other numbers in it were coerced to become character strings too. If we want to (safely) mix up different types of object without them being converted we need a proper list, rather than a vector. In R we would write: my.list &lt;- list(2, 2, &quot;Wibble&quot;) We can still access elements from lists as we do for vectors, although now we need to use double square brackets, for example: my.list[[1]] ## [1] 2 But now our numbers haven’t been converted to character strings, and we can still multiply them. my.list[[1]] * my.list[[2]] ## [1] 4 Square brackets are ugly and can be confusing though, so we often give names to the elements of our list when we create it: my.party &lt;- list(number.guests=8, when=&quot;Friday&quot;, drinks = c(&quot;Juice&quot;, &quot;Beer&quot;, &quot;Whisky&quot;)) Which means we can then access the elements by name later on. To do this, you write the name of the vector, then a $ sign, and then the name of the element you want to access: my.party$when ## [1] &quot;Friday&quot; You might have spotted that we included a vector inside the party list. This is not a problem, and we can still access individual elements of this vector too: my.party$drinks[1] ## [1] &quot;Juice&quot; 3.3 Questions on vectors and lists Create a vector containing 3 numbers then: Access just the last number Create a new list containing the first and last number Create a list containing, your address and your age in years. Then: Multiply your age in years by your flat or house number (by accessing the relevant elements in the list) Run the following R code and explain what has happened: sqrt(1:10) * 10 ## [1] 10.00000 14.14214 17.32051 20.00000 22.36068 24.49490 26.45751 ## [8] 28.28427 30.00000 31.62278 3.3.1 Extended questions: What is the average of the 9 times table, up to and including 9 x 12? Use the paste and c(...) functions to create a vector which contains the sequence “1 elephant”, “2 elephants”, …, “1000 elephants”. e.g. one run by your university.↩ Note: this isn’t the final version.. it will be more polished!↩ If you wanted, you could view and download the source for just this document here: https://github.com/benwhalley/just-enough-r/blob/master/start_here.Rmd but it will save time to download the whole project now.↩ i.e. edit the RMarkdown document↩ Mostly people who already know other programming languages like C. It’s not that surprising if you read the R code as you would English.↩ "],
["datasets.html", "4 Datasets and dataframes 4.1 Using ‘built in’ data 4.2 Importing and exporting data 4.3 Importing data over the web 4.4 Importing from SPSS and other packages", " 4 Datasets and dataframes A dataframe is an object which can store data as you might encounter it in SPSS, Stata or other statistics packages. It’s much like a spreadsheet, but with some constraints applied ‘Constraints’ sounds bad, but are helpful here: they make dataframes more structured and predictable to work with: Each column is a vector, and so can only store one type of data 8 Every column has to be the same length (although missing values are allowed). Each column should have a name. Put another way, a dataframe behaves like a list of vectors, which means we can use a lot of the same rules to access elements within them (more on this below). 4.1 Using ‘built in’ data The quickest way to see a dataframe in action is to use one that is built in9 to R. For example: head(airquality) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 Or we can use dplyr::glimpse() function to take a different look at the first few rows of the mtcars data. This flips the dataframe so the variables are listed in the first column of the output: glimpse(mtcars) ## Observations: 32 ## Variables: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.... ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, ... ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 1... ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, ... ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.9... ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3... ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 2... ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, ... ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, ... ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, ... In both these examples the datasets (airquality and mtcars) are already loaded and available to be used in the head() or glimpse() functions. Other useful functions for looking at datasets include: summary(airquality) ## Ozone Solar.R Wind Temp ## Min. : 1.00 Min. : 7.0 Min. : 1.700 Min. :56.00 ## 1st Qu.: 18.00 1st Qu.:115.8 1st Qu.: 7.400 1st Qu.:72.00 ## Median : 31.50 Median :205.0 Median : 9.700 Median :79.00 ## Mean : 42.13 Mean :185.9 Mean : 9.958 Mean :77.88 ## 3rd Qu.: 63.25 3rd Qu.:258.8 3rd Qu.:11.500 3rd Qu.:85.00 ## Max. :168.00 Max. :334.0 Max. :20.700 Max. :97.00 ## NA's :37 NA's :7 ## Month Day ## Min. :5.000 Min. : 1.0 ## 1st Qu.:6.000 1st Qu.: 8.0 ## Median :7.000 Median :16.0 ## Mean :6.993 Mean :15.8 ## 3rd Qu.:8.000 3rd Qu.:23.0 ## Max. :9.000 Max. :31.0 ## Or the more compact and useful output from describe() which is in the pysch package: psych::describe(airquality) ## vars n mean sd median trimmed mad min max range skew ## Ozone 1 116 42.13 32.99 31.5 37.80 25.95 1.0 168.0 167 1.21 ## Solar.R 2 146 185.93 90.06 205.0 190.34 98.59 7.0 334.0 327 -0.42 ## Wind 3 153 9.96 3.52 9.7 9.87 3.41 1.7 20.7 19 0.34 ## Temp 4 153 77.88 9.47 79.0 78.28 8.90 56.0 97.0 41 -0.37 ## Month 5 153 6.99 1.42 7.0 6.99 1.48 5.0 9.0 4 0.00 ## Day 6 153 15.80 8.86 16.0 15.80 11.86 1.0 31.0 30 0.00 ## kurtosis se ## Ozone 1.11 3.06 ## Solar.R -1.00 7.45 ## Wind 0.03 0.28 ## Temp -0.46 0.77 ## Month -1.32 0.11 ## Day -1.22 0.72 There are also some helpful plotting functions which accept a dataframe: ⊕These plots might not be worth including in a final write-up, but are very useful when exploring your data. boxplot(airquality) psych::cor.plot(airquality) 4.2 Importing and exporting data If you have data outside of R, the simplest way to import it is to first save it as a comma or tab-separated text file, normally with the file extension .csv or .txt10. Let’s say we have file called angry_moods.csv in the same directory as our .Rmd file. We can read this data using the read_csv() function from the readr package11: angry.moods &lt;- readr::read_csv('angry_moods.csv') ## Parsed with column specification: ## cols( ## Gender = col_integer(), ## Sports = col_integer(), ## Anger.Out = col_integer(), ## Anger.In = col_integer(), ## Control.Out = col_integer(), ## Control.In = col_integer(), ## Anger.Expression = col_integer() ## ) head(angry.moods) ## # A tibble: 6 × 7 ## Gender Sports Anger.Out Anger.In Control.Out Control.In Anger.Expression ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1 18 13 23 20 36 ## 2 2 1 14 17 25 24 30 ## 3 2 1 13 14 28 28 19 ## 4 2 1 17 24 23 23 43 ## 5 1 1 16 17 26 28 27 ## 6 1 1 16 22 25 23 38 As you can see, when loading the .csv file the read_csv() makes some assumptions about the type of data the file contains. In this case, all the columns contain integer values. It’s worth checking this message to make sure that stray cells in the file you are importing don’t cause problems when importing. Excel won’t complain about this sort of thing, but R is more strict and won’t mix text and numbers in the same column. A common error is for stray notes or text values in a spreadsheet to cause a column which should be numeric to be converted to the character type. Once it’s loaded, you can use this new dataset like any other: pairs(angry.moods) 4.3 Importing data over the web One neat feature of the readr package is that you can import data from the web, using a URL rather than a filename on your local computer. This can be really helpful when sharing data and code with colleagues. For example, we can load the angry_moods.csv file from a URL: angry.moods.from.url &lt;- readr::read_csv( &quot;https://raw.githubusercontent.com/benwhalley/just-enough-r/master/angry_moods.csv&quot;) head(angry.moods.from.url) 4.4 Importing from SPSS and other packages This is often more trouble than it’s worth (just use a csv file!) but if you really must see https://www.datacamp.com/community/tutorials/r-data-import-tutorial. Remember that in a previous chapter we created vectors, which are sequences that contain only one type of thing, and lists (which can contain a mix of different things.↩ To find a list of all the built in datasets you can type help(datasets)↩ This is easy to achieve in Excel and most other stats packages using the Save As... menu item↩ There are also standard functions built into R, such as read.csv() or read.table() for importing data. These are fine if you can’t install the readr package for some reason, but they are quite old and the default behaviour is sometimes counterintuitive. I recommend using the readr equivalents: readr::read_csv() or readr::read_tsv().↩ "],
["working-with-dataframes.html", "4.5 Working with dataframes 4.6 Introducing the tidyverse 4.7 Selecting columns from a dataframe 4.8 Selecting rows of data 4.9 Combining column selections and filters with dplyr 4.10 Modifying and creating new columns", " 4.5 Working with dataframes 4.6 Introducing the tidyverse R includes hundreds of built-in ways to select individual elements, rows or columns from a dataframe. This guide isn’t going to teach you many of them. The truth is that R can be overwhelming to new users, especially those new to programming. R is sometimes too powerful and flexible: there are too many different to accomplish the same end, and this can lead to confusion. Recently, a suite of packages has been developed for R which tries to provide a simple, consistent set of tools for working with data and graphics. This suite of packages is called the tidyverse, and you can load all of these pacakges by calling: library(tidyverse) In this guide we make much use of two components from the tidyverse: dplyr: to select, filter and summarise data ggplot2: to make plots It’s strongly recommended that you use these in your own code. 4.7 Selecting columns from a dataframe Selecting a single column: Because dataframes act like lists of vectors, we can access columns from them using the $ symbol. For example, here we select the Ozone column, which returns a vector of the observations made: airquality$Ozone ## [1] 41 36 12 18 NA 28 23 19 8 NA 7 16 11 14 18 14 34 ## [18] 6 30 11 1 11 4 32 NA NA NA 23 45 115 37 NA NA NA ## [35] NA NA NA 29 NA 71 39 NA NA 23 NA NA 21 37 20 12 13 ## [52] NA NA NA NA NA NA NA NA NA NA 135 49 32 NA 64 40 77 ## [69] 97 97 85 NA 10 27 NA 7 48 35 61 79 63 16 NA NA 80 ## [86] 108 20 52 82 50 64 59 39 9 16 78 35 66 122 89 110 NA ## [103] NA 44 28 65 NA 22 59 23 31 44 21 9 NA 45 168 73 NA ## [120] 76 118 84 85 96 78 73 91 47 32 20 23 21 24 44 21 28 ## [137] 9 13 46 18 13 24 16 13 23 36 7 14 30 NA 14 18 20 And we can pass this vector to functions, for example summary(): summary(airquality$Ozone) ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA's ## 1.00 18.00 31.50 42.13 63.25 168.00 37 Selecting more than one column: To select multiple columns the select() function from dplyr is the simplest solution. You give select() a dataframe and the names of the columns you want, and it returns a new dataframe with just those columns, in the order you specified: head( select(mtcars, cyl, hp) ) ## cyl hp ## Mazda RX4 6 110 ## Mazda RX4 Wag 6 110 ## Datsun 710 4 93 ## Hornet 4 Drive 6 110 ## Hornet Sportabout 8 175 ## Valiant 6 105 Because all the main dplyr functions tend to return a new dataframe, we can assign the results to a variable, and use that as normal: cylandweight &lt;- select(mtcars, cyl, wt) summary(cylandweight) ## cyl wt ## Min. :4.000 Min. :1.513 ## 1st Qu.:4.000 1st Qu.:2.581 ## Median :6.000 Median :3.325 ## Mean :6.188 Mean :3.217 ## 3rd Qu.:8.000 3rd Qu.:3.610 ## Max. :8.000 Max. :5.424 You can also put a minus (-) sign in front of the column name to indicate which columns you don’t want: head( select(airquality, -Ozone, -Solar.R, -Wind) ) ## Temp Month Day ## 1 67 5 1 ## 2 72 5 2 ## 3 74 5 3 ## 4 62 5 4 ## 5 56 5 5 ## 6 66 5 6 You can use a patterns to match a subset of the columns you want. For example, here we select all the columns where the name contains the letter d: head( select(mtcars, contains(&quot;d&quot;)) ) ## disp drat ## Mazda RX4 160 3.90 ## Mazda RX4 Wag 160 3.90 ## Datsun 710 108 3.85 ## Hornet 4 Drive 258 3.08 ## Hornet Sportabout 360 3.15 ## Valiant 225 2.76 And you can combine these techniques to make more complex selections: head( select(mtcars, contains(&quot;d&quot;), -drat) ) ## disp ## Mazda RX4 160 ## Mazda RX4 Wag 160 ## Datsun 710 108 ## Hornet 4 Drive 258 ## Hornet Sportabout 360 ## Valiant 225 As a quick reference, you can use the following ‘verbs’ to select columns in different ways: starts_with() ends_with() contains() everything() There are other commands too, but these are probably the most useful to begin with. See the help files for more information. 4.8 Selecting rows of data To select particular rows from a dataframe, dplyr provides the very useful select() function. Let’s say we just want the 6-cylindered cars from the mtcars dataframe: filter(mtcars, cyl==6) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160.0 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160.0 110 3.90 2.875 17.02 0 1 4 4 ## 3 21.4 6 258.0 110 3.08 3.215 19.44 1 0 3 1 ## 4 18.1 6 225.0 105 2.76 3.460 20.22 1 0 3 1 ## 5 19.2 6 167.6 123 3.92 3.440 18.30 1 0 4 4 ## 6 17.8 6 167.6 123 3.92 3.440 18.90 1 0 4 4 ## 7 19.7 6 145.0 175 3.62 2.770 15.50 0 1 5 6 Here we used the filter function to select rows matching a particular criteria: in this case, that cyl==6. We can match two criteria at once if needed12: filter(mtcars, cyl==6 &amp; gear==3) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## 2 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 4.9 Combining column selections and filters with dplyr As you might have noticed above, we can ‘nest’ function calls in R. For example, we might want to both select some columns and filter rows. Taking the mtcars data, we might want to select the weights of only those cars with low mpg: gas.guzzlers &lt;- select(filter(mtcars, mpg &lt; 15), wt) summary(gas.guzzlers) ## wt ## Min. :3.570 ## 1st Qu.:3.840 ## Median :5.250 ## Mean :4.686 ## 3rd Qu.:5.345 ## Max. :5.424 This is OK, but can get quite confusing to read, and the more deeply functions are nested the easier it is to make a mistake. dplyr provides an alternative to nested function calls, called the pipe. Imagine your dataframe as a big bucket containing data. From this bucket, you can ‘pour’ your data down through a series of tubes and filters, until at the bottom of your screen you have a smaller bucket containing just the data you want. Think of your data ‘flowing’ down the screen. To make data flow from one bucket to another, we use the ‘pipe’ operator: %&gt;% big.bucket.of.data &lt;- mtcars big.bucket.of.data %&gt;% filter(mpg &lt;15) %&gt;% select(wt) %&gt;% summary ## wt ## Min. :3.570 ## 1st Qu.:3.840 ## Median :5.250 ## Mean :4.686 ## 3rd Qu.:5.345 ## Max. :5.424 So we have achieved the same outcome, but the code reads as a series of operations which the data flows through, connected by our pipes (the %&gt;%). At the end of the last pipe, our data gets dumped into the summary() function13 We could just has well have saved this smaller ‘bucket’ of data so we can use it later on: smaller.bucket &lt;- big.bucket.of.data %&gt;% filter(mpg &lt;15) %&gt;% select(wt) This turns out to be an incredibly useful pattern when processing and working with data. We can pour data through a series of filters and other operations, saving intermediate states where necessary. 4.10 Modifying and creating new columns Often when working with data we want to compute new values from columns we already have. Let’s say we have some data on the PHQ-9, which measures depression: phq9.df &lt;- readr::read_csv(&quot;phq.csv&quot;) ## Parsed with column specification: ## cols( ## patient = col_integer(), ## phq9_01 = col_integer(), ## phq9_02 = col_integer(), ## phq9_03 = col_integer(), ## phq9_04 = col_integer(), ## phq9_05 = col_integer(), ## phq9_06 = col_integer(), ## phq9_07 = col_integer(), ## phq9_08 = col_integer(), ## phq9_09 = col_integer(), ## month = col_integer(), ## group = col_integer() ## ) glimpse(phq9.df) ## Observations: 2,429 ## Variables: 12 ## $ patient &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ... ## $ phq9_01 &lt;int&gt; 3, 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 3, ... ## $ phq9_02 &lt;int&gt; 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_03 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, ... ## $ phq9_04 &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, ... ## $ phq9_05 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, ... ## $ phq9_06 &lt;int&gt; 3, 2, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_07 &lt;int&gt; 3, 3, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 1, 1, ... ## $ phq9_08 &lt;int&gt; 0, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, ... ## $ phq9_09 &lt;int&gt; 2, 2, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, ... ## $ month &lt;int&gt; 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 0, 1,... ## $ group &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ... We want to calculate the PHQ-9 score for each patient, at each month. This is easy with dplyr::mutate(): phq9.scored.df &lt;- phq9.df %&gt;% mutate(phq9 = phq9_01 + phq9_02 + phq9_03 + phq9_04 + phq9_05 + phq9_06 + phq9_07 + phq9_08 + phq9_09) phq9.scored.df %&gt;% select(patient, group, month, phq9) %&gt;% head ## # A tibble: 6 × 4 ## patient group month phq9 ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1 1 0 23 ## 2 2 0 0 21 ## 3 2 0 1 17 ## 4 2 0 2 18 ## 5 2 0 3 22 ## 6 2 0 4 21 Notice that we first stored the computed scores in phq9.scored.df and then used select() to get rid of the raw data columns to display only what we needed. Some notes on == and &amp;: You might have noted above that I wrote == rather than just = to define the criteria. This is because most programming languages, including R, use two = symbols to distinguish: comparison from assignment. Here we are doing comparison, so we use ==. In R normall use &lt;- to assign variables, which avoids any ambiguity. The &amp; symbol does what you probably expect — it simply means ‘AND’.↩ You might notice that when we write the select function we don’t explicitly name the dataframe to be used. This is because R implicitly passes the output of the pipe to the first argument of the function. So here, the output of filter(mpg&lt;15) is used as the dataframe in the select function.↩ "],
["summarising-data.html", "5 Summarising data 5.1 Summaries of dataframes 5.2 Split, apply, combine 5.3 Split: breaking the data into groups 5.4 Apply and combine 5.5 A ‘real’ example 5.6 Sorting data", " 5 Summarising data Before you begin this section, make sure you have fully understood the section on datasets and dataframes, and in particular that you are happy using the %&gt;% symbol to describe a flow of data. 5.1 Summaries of dataframes So far you have seen a number of functions which provide summaries of a dataframe. For example: summary(angry.moods) ## Gender Sports Anger.Out Anger.In ## Min. :1.000 Min. :1.000 Min. : 9.00 Min. :10.00 ## 1st Qu.:1.000 1st Qu.:1.000 1st Qu.:13.00 1st Qu.:15.00 ## Median :2.000 Median :2.000 Median :16.00 Median :18.50 ## Mean :1.615 Mean :1.679 Mean :16.08 Mean :18.58 ## 3rd Qu.:2.000 3rd Qu.:2.000 3rd Qu.:18.00 3rd Qu.:22.00 ## Max. :2.000 Max. :2.000 Max. :27.00 Max. :31.00 ## Control.Out Control.In Anger.Expression ## Min. :14.00 Min. :11.00 Min. : 7.00 ## 1st Qu.:21.00 1st Qu.:18.25 1st Qu.:27.00 ## Median :24.00 Median :22.00 Median :36.00 ## Mean :23.69 Mean :21.96 Mean :37.00 ## 3rd Qu.:27.00 3rd Qu.:24.75 3rd Qu.:44.75 ## Max. :32.00 Max. :32.00 Max. :68.00 Or psych::describe(angry.moods, skew=FALSE) ## vars n mean sd min max range se ## Gender 1 78 1.62 0.49 1 2 1 0.06 ## Sports 2 78 1.68 0.47 1 2 1 0.05 ## Anger.Out 3 78 16.08 4.22 9 27 18 0.48 ## Anger.In 4 78 18.58 4.70 10 31 21 0.53 ## Control.Out 5 78 23.69 4.69 14 32 18 0.53 ## Control.In 6 78 21.96 4.95 11 32 21 0.56 ## Anger.Expression 7 78 37.00 12.94 7 68 61 1.47 However, these functions operate on the dataset as a whole. What if we want to get summaries grouped by one of our variables, for example Gender? Or perhaps we want to use our summary data in a further analysis: for example, we might want to compute average reaction times in each block of an experiment to run an Anova or regression model. What we really want is a summary function which gives us back a dataframe. The dplyr::summarise() does just that: angry.moods %&gt;% summarise( mean.anger.out=mean(Anger.Out), sd.anger.out=sd(Anger.Out) ) ## # A tibble: 1 × 2 ## mean.anger.out sd.anger.out ## &lt;dbl&gt; &lt;dbl&gt; ## 1 16.07692 4.21737 This has returned a dataframe, which we could store and use as before, although in this instance the dataframe only has one row. What if we want the numbers for men and women separately? 5.2 Split, apply, combine Let’s think more about the case where we want to compute statistics on men and women separately. Although many packages would have functions with options to do this (for example, perhaps you would specify grouping variables in a summary function), there’s a more general pattern at work. We want to: Split our data (into men and women) Apply some function to them (e.g. calculate the mean) and then Combine it into a single table again (for more processing or analysis) It’s helpful to think of this split → apply → combine pattern whenever we are processing data because it makes explicit what it is that we want to do. 5.3 Split: breaking the data into groups The first task is to organise our dataframe into the relevant groups. To do this we use group_by(): angry.moods %&gt;% group_by(Gender) %&gt;% head ## Source: local data frame [6 x 7] ## Groups: Gender [2] ## ## Gender Sports Anger.Out Anger.In Control.Out Control.In Anger.Expression ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2 1 18 13 23 20 36 ## 2 2 1 14 17 25 24 30 ## 3 2 1 13 14 28 28 19 ## 4 2 1 17 24 23 23 43 ## 5 1 1 16 17 26 28 27 ## 6 1 1 16 22 25 23 38 Weirdly, this doesn’t seem to have done anything. The data aren’t sorted by Gender, and there is no visible sign of the grouping, but stick with it… 5.4 Apply and combine Continuing the example above, once we have grouped our data we can then apply a function to it — for exmaple, summarise: angry.moods %&gt;% group_by(Gender) %&gt;% summarise( mean.anger.out=mean(Anger.Out) ) ## # A tibble: 2 × 2 ## Gender mean.anger.out ## &lt;int&gt; &lt;dbl&gt; ## 1 1 16.56667 ## 2 2 15.77083 And R and dplyr have done as we asked: split the data by Gender, using group_by() apply the summarise() function combine the results into a new data frame 5.5 A ‘real’ example In the previous section on datasets, we saw some found some raw data from a study which had measured depression with the PHQ-9. Patients were measured on numerous occasions (month is recorded) and were split into treatment and control groups: phq9.df &lt;- readr::read_csv(&quot;phq.csv&quot;) ## Parsed with column specification: ## cols( ## patient = col_integer(), ## phq9_01 = col_integer(), ## phq9_02 = col_integer(), ## phq9_03 = col_integer(), ## phq9_04 = col_integer(), ## phq9_05 = col_integer(), ## phq9_06 = col_integer(), ## phq9_07 = col_integer(), ## phq9_08 = col_integer(), ## phq9_09 = col_integer(), ## month = col_integer(), ## group = col_integer() ## ) glimpse(phq9.df) ## Observations: 2,429 ## Variables: 12 ## $ patient &lt;int&gt; 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, ... ## $ phq9_01 &lt;int&gt; 3, 1, 1, 2, 2, 2, 3, 2, 3, 3, 1, 3, 2, 1, 2, 3, 3, 3, ... ## $ phq9_02 &lt;int&gt; 3, 2, 2, 2, 3, 3, 3, 2, 3, 3, 1, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_03 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 3, 2, 3, 3, ... ## $ phq9_04 &lt;int&gt; 3, 3, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 3, 3, 3, ... ## $ phq9_05 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 3, 2, 1, 2, ... ## $ phq9_06 &lt;int&gt; 3, 2, 2, 2, 3, 3, 2, 1, 2, 3, 3, 3, 2, 1, 3, 3, 3, 3, ... ## $ phq9_07 &lt;int&gt; 3, 3, 1, 1, 2, 1, 2, 2, 1, 3, 2, 2, 2, 2, 3, 2, 1, 1, ... ## $ phq9_08 &lt;int&gt; 0, 2, 2, 1, 1, 1, 1, 2, 1, 3, 1, 2, 1, 0, 2, 1, 0, 1, ... ## $ phq9_09 &lt;int&gt; 2, 2, 1, 1, 2, 2, 2, 1, 1, 3, 1, 2, 1, 1, 3, 3, 3, 3, ... ## $ month &lt;int&gt; 0, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 18, 0, 1,... ## $ group &lt;int&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, ... If this were our data we might want to: Calculate the sum of the PHQ-9 variables (the PHQ-9 score) Calculate the average PHQ-9 score at each month, and in each group Show these means by group for months 0, 7 and 12 Using only the commands above14 we can write: phq9.summary.df &lt;- phq9.df %&gt;% mutate(phq9 = phq9_01 + phq9_02 + phq9_03 + phq9_04 + phq9_05 + phq9_06 + phq9_07 + phq9_08 + phq9_09 ) %&gt;% select(patient, group, month, phq9) %&gt;% # remove rows with missing values filter(!is.na(phq9)) %&gt;% # split group_by(month, group) %&gt;% # apply and combine summarise(phq.mean = mean(phq9)) phq9.summary.df %&gt;% filter(month %in% c(0, 7, 12)) %&gt;% pander::pander() month group phq.mean 0 0 19.75904 0 1 18.97368 7 0 16.61818 7 1 13.42056 12 0 16.15385 12 1 12.54082 5.6 Sorting data Sorting data is easy with dplyr::arrange(): airquality %&gt;% arrange(Ozone) %&gt;% head ## Ozone Solar.R Wind Temp Month Day ## 1 1 8 9.7 59 5 21 ## 2 4 25 9.7 61 5 23 ## 3 6 78 18.4 57 5 18 ## 4 7 NA 6.9 74 5 11 ## 5 7 48 14.3 80 7 15 ## 6 7 49 10.3 69 9 24 By default sorting is ascending, but you can use a minus sign to reverse this: airquality %&gt;% arrange(-Ozone) %&gt;% head ## Ozone Solar.R Wind Temp Month Day ## 1 168 238 3.4 81 8 25 ## 2 135 269 4.1 84 7 1 ## 3 122 255 4.0 89 8 7 ## 4 118 225 2.3 94 8 29 ## 5 115 223 5.7 79 5 30 ## 6 110 207 8.0 90 8 9 You can sort on multiple columns too: airquality %&gt;% select(Month, Ozone) %&gt;% arrange(Month, -Ozone) %&gt;% head ## Month Ozone ## 1 5 115 ## 2 5 45 ## 3 5 41 ## 4 5 37 ## 5 5 36 ## 6 5 34 You might have noticed I sneaked something new in here: the call to pander(). This is a weirdly named but useful function when writing RMarkdown documents. It converts any R object into more readable output: here it makes a nice table for us in the compiled document. We cover more tips and tricks for formatting RMarkdown documents here. You might also want to check this page on missing values to explain the filter which uses !is.na(), but you could leave it for later.↩ "],
["graphics.html", "6 Graphics 6.1 Benefits of visualising data 6.2 Choosing a plot type 6.3 Layering graphics", " 6 Graphics THIS SECTION IS INCOMPLETE - YOU MIGHT WANT TO SKIP TO THE NEXT ONE Sections to cover: 6.1 Benefits of visualising data Psychology and human factors of graphics + Tufte. Importance of graphs to communicate. Motivating examples from RCTs. 6.2 Choosing a plot type Describe 2 strategies when plotting in R: Quick an dirty (helper functions like pairs, Hmisc::hist.data.frame or ggplot2::qplot) Doing it right (ggplot or careful use of base graphics) When exploring a dataset, often useful to use built in functions or helpers from other libraries. These help you quickly visualise relationships, but aren’t always exactly what you need and can be hard to customise. The other approach is to build your plot from scratch using the layers approach in ggplot (you can also do this with base graphics, but it requires a detailed knowledge of R and can be fiddly). This enables you to construct a plot which exactly matches the aims of your communication, and can be tweaked to make it publication-ready. 6.3 Layering graphics ggplot - e.g. geom_point + geom_smooth "],
["linear-models-simple.html", "7 Simple linear models 7.1 Describing our models (formulas) 7.2 Running a linear model 7.3 More on formulas 7.4 What next", " 7 Simple linear models This section assumes most readers will have done an introductory statistics course and had practce running multiple regression and or Anova in SPSS or a similar package. 7.1 Describing our models (formulas) R requires that you are explicit about the statistical model you want to run, but provides a neat, concise way of describing a model, called a formula. For multiple regression and simple Anova, the formulas we write map closely onto the underlying linear model. The formula syntax provides shortcuts to quickly describe all the models you are likely to need. Formulas have two parts: the left hand side and the right hand side, which are separated by the tilde symbol: ~. Here, the tilde just means ‘is predicted by’. For example, for formula height ~ age + gender specifies a regression model where height is the outcome, and age and gender are the predictor variables.15 There are lots more useful tricks to learn when writing formulas, which are covered below. In the interests of instant gratification, let’s work through a simple example first: 7.2 Running a linear model Linear models (including Anova and multiple regression) are run using the lm function, short for ‘linear model’. We will use the mtcars dataset, which is built into R, for our first example. First, we have a quick look at the data. The pairs plot suggests that mpg might be related to a number of the other variables including disp (engine size) and wt (car weight): mtcars %&gt;% select(mpg, disp, wt) %&gt;% pairs Before running any model, we should ask outselves what question we are trying to answer? In this instance, we can see that both weight and engine size are related to mpg, but they are also correlated with one another. We might want to know, “are weight and engine size independent predictors of mpg?” That is, if we know a car’s weight, do we gain additional information about it’s mpg by measuring engine size? To answer this, we could use multiple regression, including both wt and disp as predictors of mpg. The formula for this would be mpg ~ wt + disp. The command below runs the model: lm(mpg ~ wt + disp, data=mtcars) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Coefficients: ## (Intercept) wt disp ## 34.96055 -3.35083 -0.01772 For readers used to wading through SPSS output, R might seem concise to the point of rudeness. By default, the lm commands displays very little, only repeating the formula and listing the coefficients for each predictor in the model. So what next? Unlike SPSS, we must be explicit and tell R exactly what we want. The most convenient way to do this is to first store the results of the lm() function: m.1 &lt;- lm(mpg ~ wt + disp, data=mtcars) This stores the results of lm in a variable named m.116. We can then use other functions to get more information about the model. For example: summary(m.1) ## ## Call: ## lm(formula = mpg ~ wt + disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## wt -3.35082 1.16413 -2.878 0.00743 ** ## disp -0.01773 0.00919 -1.929 0.06362 . ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Although still compact, the summary function provides familiar output, including the estimate, SE, and p value for each parameter. Take a moment to find the following statistics in the output above: The coefficients and p values for each predictor The R2 for the overall model. What % of variance in mpg is explained? Answer the original question: ‘accounting for weight, does disp tell us anything extra about a car’s mpg?’ 7.3 More on formulas Above we briefly introduced R’s formula syntax. Formulas for linear models have the following structure: left_hand_side ~ right_hand_side For linear models the left side is our outcome, which is must be a continous variable (i.e. not categorical or binary)17. The right hand side lists our predictors. In the example above we used the + symbol to separate the predictors wt and disp. This told R to simply add each predictor to the model. However, many times we want to specify relationships between our predictors. For example, we might want to run an Anova with 2 categorical predictors, each with 2 levels (e.g. a 2x2 between-subjects design). Below, we define and run a linear model with both vs and am as predictors, along with the interaction of vs:am. We save this model, and use the anova command to print the standard Anova table for the model. m.2 &lt;- lm(mpg ~ vs + am + vs:am, data=mtcars) anova(m.2) ## Analysis of Variance Table ## ## Response: mpg ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## vs 1 496.53 496.53 41.1963 5.981e-07 *** ## am 1 276.03 276.03 22.9021 4.984e-05 *** ## vs:am 1 16.01 16.01 1.3283 0.2589 ## Residuals 28 337.48 12.05 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 This might seem odd to some readers (running the linear model before the Anova), but it’s important to understand that printing an Anova table is just one of the things you can do with a linear model — allbeit an important one for experimental psychologists. 7.3.1 Other formula shortcuts In addition to the + symbol, we can use other shortcuts to create linear models. As seen above, the colon (:) operator indicates the interaction between two terms. So a:b is equivalent to creating a new variable in the data frame where a is multiplied by b. The * symbol indicates the expansion of other terms in the model. So, a*b is the equivalent of a + b + a:b. Finally, it’s good to know that other functions can be used within R formulas to save work. For example, if you wanted to transform your dependent variable then log(y) ~ x will do what you might expect, and saves creating temporary variables in your dataset. The formula syntax is very powerful, and the above only shows the basics, but you can read the formulae help pages in RStudio for more details. As an exercise, run the following models using the mtcars dataset: With mpg as the outcome, and with cyl and hp as predictors As above, but adding the interaction of cyl and hp. Repeat the model above, but write the formula a different way (make the formula either more or less explicit, but retaining the same predictors in the model). 7.4 What next It is strongly recommended that you read the section on Anova before doing anything else. R has a number of important differences in it’s default settings, as compared to packages like Stata or SPSS, and these can make important differences to the way you interpret the output of Anova models. I avoid the terms dependent/independent variables because they are confusing to many students, and because they can be misleading when discussing non-experimental data. ‘Outcome’ and ‘predictors’ are preferred instead.↩ This is actually a pretty terrible variable name. Try to give descriptive names to variables you create in your code; this prevents errors and makes code easier to read.↩ Other types of model (e.g. generalised linear models using the glm() function) can accept binary or categorical outcomes.↩ "],
["real-data.html", "8 Working with ‘real’ data 8.1 How to ‘tidy’ data 8.2 Deal with multiple files 8.3 Error checking 8.4 Missing values", " 8 Working with ‘real’ data Note: If you already have nicely formatted data ready for use in R then you could skip this section and revisit it later. Most tutorials and textbooks use neatly formatted example datasets to illustrate particular techniques. However in the real-world our data can be: In the wrong format Spread across multiple files Badly coded, or with errors Incomplete, with values missing for many different reasons This chapter shows you how to address each of these problems. 8.1 How to ‘tidy’ data Melt, spread … 8.2 Deal with multiple files File handling and import Writing a function for do() which returns a dataframe Joins and merges 8.3 Error checking 999, 666 and *: the marks of the beast! 8.4 Missing values is.na() and is.finite expand() "],
["correlations.html", "9 Correlations 9.1 Obtaining a correlation matrix 9.2 Making correlation tables for publication 9.3 Other types of correlation", " 9 Correlations The base R cor() function provides a simple way to get Pearson correlations, but to get a correlation matrix as you might expect from SPSS or Stata it’s best to use the corr.test() function in the psych package. Before you start though, plotting the correlations might be the best way of getting to grips with the patterns of relationship in your data. A pairs plot is a nice way of doing this: airquality %&gt;% select(-Month, -Day) %&gt;% pairs If we were satisfied the relationships were (reasonably) linear, we could also visualise correlations themselves with a ‘corrgram’, using the corrgram library: library(&quot;corrgram&quot;) airquality %&gt;% select(-Month, -Day) %&gt;% corrgram(lower.panel=corrgram::panel.ellipse, upper.panel=panel.cor, diag.panel=panel.density) Figure 9.1: A corrgram, showing pearson correlations (above the diagonal), variable distributions (on the diagonal) and ellipses and smoothed lines of best fit (below the diagnonal). Long, narrow ellipses denote large correlations; circular ellipses indicate small correlations. The ggpairs function from the GGally package is also a nice way of plotting relationships between a combination of categorical and continuous data - it packs a lot of information into a limited space: mtcars %&gt;% mutate(cyl = factor(cyl)) %&gt;% select(mpg, wt, drat, cyl) %&gt;% GGally::ggpairs() 9.1 Obtaining a correlation matrix The psych::corr.test() function is a quick way to obtain a pairwise correlation matrix for an entire dataset, along with p values and confidence intervals which the base R cor() function will not provide: mycorrelations &lt;- psych::corr.test(airquality) mycorrelations ## Call:psych::corr.test(x = airquality) ## Correlation matrix ## Ozone Solar.R Wind Temp Month Day ## Ozone 1.00 0.35 -0.60 0.70 0.16 -0.01 ## Solar.R 0.35 1.00 -0.06 0.28 -0.08 -0.15 ## Wind -0.60 -0.06 1.00 -0.46 -0.18 0.03 ## Temp 0.70 0.28 -0.46 1.00 0.42 -0.13 ## Month 0.16 -0.08 -0.18 0.42 1.00 -0.01 ## Day -0.01 -0.15 0.03 -0.13 -0.01 1.00 ## Sample Size ## Ozone Solar.R Wind Temp Month Day ## Ozone 116 111 116 116 116 116 ## Solar.R 111 146 146 146 146 146 ## Wind 116 146 153 153 153 153 ## Temp 116 146 153 153 153 153 ## Month 116 146 153 153 153 153 ## Day 116 146 153 153 153 153 ## Probability values (Entries above the diagonal are adjusted for multiple tests.) ## Ozone Solar.R Wind Temp Month Day ## Ozone 0.00 0.00 0.00 0.00 0.56 1.00 ## Solar.R 0.00 0.00 1.00 0.01 1.00 0.56 ## Wind 0.00 0.50 0.00 0.00 0.25 1.00 ## Temp 0.00 0.00 0.00 0.00 0.00 0.65 ## Month 0.08 0.37 0.03 0.00 0.00 1.00 ## Day 0.89 0.07 0.74 0.11 0.92 0.00 ## ## To see confidence intervals of the correlations, print with the short=FALSE option One thing to be aware of is that by default corr.test() produces p values that are adjusted for multiple comparisons in the top right hand triangle (i.e. above the diagonal). If you want the uncorrected values use the values below the diagonal (or pass adjust=FALSE when calling the function). 9.2 Making correlation tables for publication 9.2.1 apaTables If you want to produce output tables for publication the apaTables package might be useful. This block saves an APA formatted correlation table to an external Word document. library(apaTables) apa.cor.table(airquality, filename=&quot;Table1_APA.doc&quot;, show.conf.interval=F) ## ## ## Means, standard deviations, and correlations ## ## ## Variable M SD 1 2 3 4 5 ## 1. Ozone 42.13 32.99 ## ## 2. Solar.R 185.93 90.06 .35** ## ## 3. Wind 9.96 3.52 -.60** -.06 ## ## 4. Temp 77.88 9.47 .70** .28** -.46** ## ## 5. Month 6.99 1.42 .16 -.08 -.18* .42** ## ## 6. Day 15.80 8.86 -.01 -.15 .03 -.13 -.01 ## ## ## Note. * indicates p &lt; .05; ** indicates p &lt; .01. ## M and SD are used to represent mean and standard deviation, respectively. ## 9.2.2 By hand If you’re not bothered about strict APA foramt, you might still want to extract the r and p values as dataframes which can then be saved to a csv and opened in excel, or converted to a table by some other means. You can do this by storing the corr.test output in a variable, and the accessing the $r and $p values within it: mycorrelations &lt;- psych::corr.test(airquality) write.csv(mycorrelations$p, file=&quot;airquality-r-values.csv&quot;) mycorrelations$p ## Ozone Solar.R Wind Temp Month ## Ozone 0.000000e+00 0.0019724194 1.298162e-11 0.000000e+00 5.617870e-01 ## Solar.R 1.793109e-04 0.0000000000 1.000000e+00 7.517729e-03 1.000000e+00 ## Wind 9.272583e-13 0.4959552068 0.000000e+00 3.434076e-08 2.471060e-01 ## Temp 0.000000e+00 0.0007517729 2.641597e-09 0.000000e+00 7.231443e-07 ## Month 7.760010e-02 0.3663533509 2.745622e-02 6.026202e-08 0.000000e+00 ## Day 8.879425e-01 0.0702233769 7.387466e-01 1.076164e-01 9.221900e-01 ## Day ## Ozone 1.0000000 ## Solar.R 0.5617870 ## Wind 1.0000000 ## Temp 0.6456986 ## Month 1.0000000 ## Day 0.0000000 mycorrelations$r ## Ozone Solar.R Wind Temp Month ## Ozone 1.00000000 0.34834169 -0.60154653 0.6983603 0.164519314 ## Solar.R 0.34834169 1.00000000 -0.05679167 0.2758403 -0.075300764 ## Wind -0.60154653 -0.05679167 1.00000000 -0.4579879 -0.178292579 ## Temp 0.69836034 0.27584027 -0.45798788 1.0000000 0.420947252 ## Month 0.16451931 -0.07530076 -0.17829258 0.4209473 1.000000000 ## Day -0.01322565 -0.15027498 0.02718090 -0.1305932 -0.007961763 ## Day ## Ozone -0.013225647 ## Solar.R -0.150274979 ## Wind 0.027180903 ## Temp -0.130593175 ## Month -0.007961763 ## Day 1.000000000 You can also access the CI for each pariwise correlation as a table: mycorrelations$ci %&gt;% head() %&gt;% pander() lower r upper p Ozone-Slr.R 0.17 0.35 0.5 0 Ozone-Wind -0.71 -0.6 -0.47 0 Ozone-Temp 0.59 0.7 0.78 0 Ozone-Month -0.018 0.17 0.34 0.078 Ozone-Day -0.2 -0.013 0.17 0.89 Slr.R-Wind -0.22 -0.057 0.11 0.5 9.3 Other types of correlation By default corr.test produces Pearson correlations, but You can pass the method argument psych::corr.test(): psych::corr.test(airquality, method=&quot;spearman&quot;) psych::corr.test(airquality, method=&quot;kendall&quot;) "],
["t-tests.html", "10 t-tests 10.1 Running t-tests", " 10 t-tests Before you run any tests it’s worth plotting your data. Assuming you have a continuous outcome and categorical (binary) predictor (here we use a subset of the built in chickwts data), a boxplot can work well: chicks.eating.beans &lt;- chickwts %&gt;% filter(feed %in% c(&quot;horsebean&quot;, &quot;soybean&quot;)) chicks.eating.beans %&gt;% ggplot(aes(feed, weight)) + geom_boxplot() Figure 10.1: The box in a boxplot indictes the IQR; the whisker indicates the min/max values or 1.5 * the IQR, whichever is the smaller. If there are outliers beyond 1.5 * the IQR then they are shown as points. Or a violin or bottle plot, which shows the distributions within each group: chicks.eating.beans %&gt;% ggplot(aes(feed, weight)) + geom_violin() Layering boxes and bottles can work well too because it combines information about the distribution with key statistics like the median and IQR, and also because it scales reasonably well to multiple categories: chickwts %&gt;% ggplot(aes(feed, weight)) + geom_violin() + geom_boxplot(width=.1) + xlab(&quot;&quot;) Bottleplots are just density plots, turned 90 degrees. Density plots might be more familiar to some, but it’s hard to show more than 2 or 3 categories: chicks.eating.beans %&gt;% ggplot(aes(weight, fill=feed)) + geom_density(alpha=.5) And density plots are just smoothed histograms (which you might prefer if you’re a fan of 80’s computer games): chicks.eating.beans %&gt;% ggplot(aes(weight)) + geom_histogram(bins=7) + facet_grid(feed ~ .) 10.1 Running t-tests Assuming you really do still want to run a null hypothesis test, the t.test() function performs most common variants: 10.1.1 2 independent groups: with(chicks.eating.beans, t.test(weight ~ feed)) ## ## Welch Two Sample t-test ## ## data: weight by feed ## t = -4.5543, df = 21.995, p-value = 0.0001559 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -125.49476 -46.96238 ## sample estimates: ## mean in group horsebean mean in group soybean ## 160.2000 246.4286 Or equivalently, if your data are untidy and each group has it’s own column: untidy.chicks &lt;- chicks.eating.beans %&gt;% mutate(chick = row_number()) %&gt;% reshape2::dcast(chick~feed, value.var = 'weight') with(untidy.chicks, t.test(horsebean, soybean)) 10.1.2 Unequal variances By default R assumes your groups have unequal variances and applies an appropriate correction. If you don’t want this you can add var.equal = TRUE and get a vanilla t-test: with(untidy.chicks, t.test(horsebean, soybean, var.equal=TRUE)) 10.1.3 Paired samples a &lt;- rnorm(50, 2.5, 1) b = a + rnorm(50, .5, 1) t.test(a, b, paired=TRUE) 10.1.4 One-sample test i.e. comparing sample mean with a specific value: # test if mean of `outcome` variable is different from 2 somedata &lt;- rnorm(50, 2.5, 1) t.test(somedata, mu=2) "],
["anova.html", "11 Anova 11.1 Anova for between subjects designs 11.2 Repeated measures", " 11 Anova This intention of this section is to be a kind of cookbook for common types of Anova, and the chapter mostly consists of examples. However, there are some key principles when working with Anova in R that everyone should know: R can run any Anova model you wan but. R’s default settings for Anova are different to those SPSS or Stata. You probably do want to change the default settings, or use a package that does this for you. # options(contrasts = c(&quot;contr.helmert&quot;, &quot;contr.poly&quot;)) # m &lt;- lm(mpg~factor(cyl)*wt, data=mtcars) # car::Anova(m, type=3) # # # options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # m &lt;- lm(mpg~factor(cyl)*wt, data=mtcars) # car::Anova(m, type=3) # # # options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # m &lt;- lm(mpg~factor(cyl)*wt, data=mtcars) # car::Anova(m, type=3) # # contrasts(factor(mtcars$cyl)) # # library(lmerTest) # # options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # anova(lmer(hamd~factor(grp)*factor(month)+(1|patient), data=rf2main)) # # # options(contrasts = c(&quot;contr.helmert&quot;, &quot;contr.poly&quot;)) # m &lt;- lmer(hamd~grp*month+(1|patient), data=rf2main) # anova(lmer(hamd~factor(grp)*factor(month)+(1|patient), data=rf2main), type=2) # anova(lmer(hamd~factor(grp)*factor(month)+(1|patient), data=rf2main), type=3) # # # options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # m &lt;- lmer(hamd~grp*month+(1|patient), data=rf2main) # anova(m) # lsmeansLT(m) # drop1(m, test=&quot;Chisq&quot;) # options(contrasts = c(&quot;contr.sum&quot;, &quot;contr.poly&quot;)) # rf2main$grp # # options(contrasts = c(&quot;contr.treatment&quot;, &quot;contr.poly&quot;)) # m &lt;- lmer(hamd~hamd.b+grp*month+(1|patient), data=rf2main %&gt;% filter(month!=&quot;0&quot;)) # # lsmeansLT(m, &quot;grp:month&quot;) # library(multcomp) # glht(m, linfct = c(&quot;month12 = 1&quot;, &quot;grpTAU=0&quot;)) # leveneTest # mcp(grp=&quot;Tukey&quot;)) # summary(m) # anova(m, type=2) # anova(m, type=3) # lsmeansLT(m) 11.1 Anova for between subjects designs TODO 11.2 Repeated measures TODO "],
["understanding-interactions.html", "12 Understanding interactions (part 1) 12.1 What is an interaction? 12.2 Visualising interactions in raw data 12.3 Continuous predictors 12.4 What next?", " 12 Understanding interactions (part 1) Objectives of this section: Clarify/recap what an interaction is Appreciate the importance of visualising interactions Compare different methods of plotting interactions in raw data Deal with cases where predictors are both categorical and continuous (or a mix) 12.1 What is an interaction? For an interaction to occur we must measure: an outcome (severity of injury in a car crash, for example) at least 2 predictors of that outcome (e.g. age and gender) Let’s think of a scenario where we’ve measured severity of injury after road accidents, along with the age and gender of the drivers involved. Let’s assume18: Women are likely to be more seriously injured than men in a crash (a +10 point increase in severity) Drivers over 60 are more likely to injured than younger drivers (+10 point severity vs &lt;60 years) For an interaction to occur we have to show that, for example: If you ware old and also female then you are more severely injured than we would expect simply by adding the effects for being female (+10 points) and for being over 60 (+10 points). That is, if an interaction occurs the risk of being older and female is &gt; a 20 point increase in severity. Interactions capture the idea that the effect of one predictor changes across the range of another. 12.1.1 Example interaction visualised We can see this illustrated in the figure below: Figure 9.1: Bar plot of injury severity by age and gender. And this plot might be better re-drawn as a point and line plot: Figure 12.1: Point and line plot of injury severity by age and gender. The reason this plot improves on the bar graph is because: Readers tend to misinterpret bar plots by assuming that values ‘above’ the bar are less likely than values contained ‘within’ the bar, when this is not the case [@newman2012bar]. The main effects are easy to distinguish in the line plot: just ask yourself if the lines are horizontal or not, and whether they are separated vertically. In contrast, reading the interction from the bar graph requires that we average pairs of bars (sometimes not adjacent to one another) and compare them - a much more difficult mental operation. The interaction is easy to spot: Ask yourself if the lines are parallel. If they are parallel then the difference between men and women is constant for individuals of different ages. 12.2 Visualising interactions in raw data Before setting out to test for an interaction using some kind of statistical model, it’s a good idea to first visualise the relationships between outcomes and predictors. A student dissertation project investigated the analgesic quality of music during an experimental pain stimulus. Music was selected to be either liked (or disliked) by participants and was either familiar or unfamiliar to them. Pain was rated without music (no.music) and with music (with.music) using a 10cm visual analog scale anchored with the labels “no pain” and “worst pain ever”. Before modelling the outcome, it would be helpful to see if the data are congruent with the study prediction that liked and familiar music would be more effective than disliked or unfamiliar music We can do this in many different ways. The most common would be a simple bar plot, which we can create using the stat_summary() function from ggplot2. painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = familiar, y=change.in.pain)) + facet_wrap(~liked) + stat_summary(geom=&quot;bar&quot;) + xlab(&quot;&quot;) This gives a pretty clear indication that something is going on, but we have no idea about the distriburion of the underlying data, and so how much confidence to place in the finding. If we want to preserve more information about the underlying distribution we can use density plots, boxplots, or pointrange plots, among others. Here we use a grouped19 density plot: painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = change.in.pain, color = interaction(familiar:liked))) + geom_density() + scale_color_discrete(name=&quot;&quot;) And here we use a boxplot to achieve similar ends: painmusic %&gt;% mutate(change.in.pain = with.music - no.music) %&gt;% ggplot(aes(x = interaction(familiar:liked), y = change.in.pain)) + geom_boxplot() + geom_hline(yintercept = 0, linetype=&quot;dotted&quot;) + xlab(&quot;&quot;) The advantage of both these plots is that they preserve quite a bit of infrmation about the variable of interest. However, they don’t make it easy to read the main effects and interaction as we saw for the point-line plot above. We can combine some benefits of both plots by adding an error bar to the point-line plot: painmusic %&gt;% ggplot(aes(liked, with.music - no.music, group=familiar, color=familiar)) + stat_summary(geom=&quot;pointrange&quot;, fun.data=mean_se) + stat_summary(geom=&quot;line&quot;, fun.data=mean_se) + ylab(&quot;Pain (VAS) with.music - no.music&quot;) + scale_color_discrete(name=&quot;&quot;) + xlab(&quot;&quot;) This plot doesn’t include all of the information about the distribution of effects that the density or boxplots do (for example, we can’t see any asymmetry in the distributions any more), but we still get some information about the variability of the effect of the experimental conditions on pain by plotting the SE of the mean over the top of each point20 At this point, especially if your current data include only categorical predictors, you might want to move on to the section on making predictions from models and visualising these. 12.3 Continuous predictors … 12.4 What next? You might like to move on to making predictions from statistical models, and plotting these. This example is loosely based on figures reported by @kockelman2002driver↩ we have used the interaction() function to automatically create groups within the data for both liked and familiar.↩ We could equally well plot the CI for the mean, or the quartiles)↩ "],
["predictions-and-margins.html", "13 Making predictions 13.1 Predictions vs margins 13.2 Predicted means 13.3 Effects (margins) 13.4 Continuous predictors 13.5 Predicted means and margins using lm() 13.6 Making prdictions for margins (effects of predictors) 13.7 Marginal effects 13.8 With continuous covariates 14 Visualising interactions from linear models", " 13 Making predictions Objectives of this section: Distingish predicted means (predictions) from predicted effects (‘margins’) Calculate both predictions and marginal effects for a lm() Plot predictions and margins Think about how to plot effects in meaningful ways 13.1 Predictions vs margins Before we start, let’s consider what we’re trying to achieve in making predictions from our models. We need to make a distinction between: Predicted means Predicted effects or marginal effects Consider the example used in a previous section where we measured injury.severity after road accidents, plus two predictor variables: gender and age. 13.2 Predicted means ‘Predicted means’ (or predictions) refers to our best estimate for each category of person we’re interested in. For example, if age were categorical (i.e. young vs. older people) then might have 4 predictions to calculate from our model: Age Gender mean Young Male ? Old Male ? Young Female ? Old Female ? And as before, we might plot these data: Figure 9.1: Point and line plot of injury severity by age and gender. This plot uses the raw data, but these points could equally have been estimated from a statistical model which adjusted for other predictors. 13.3 Effects (margins) Terms like: predicted effects, margins or marginal effects refer, instead, to the effect of one predictor. There may be more than one marginal effect because the effect of one predictor can change across the range of another predictor. Extending the example above, if we take the difference between men and women for each category of age, we can plot these differences. The steps we need to go through are: Reshape the data to be wide, including a separate column for injury scores for men and women Subtract the score for men from that of women, to calculate the effect of being female Plot this difference score margins.plot &lt;- inter.df %&gt;% # reshape the data to a wider format reshape2::dcast(older~female) %&gt;% # calculate the difference between men and women for each age mutate(effect.of.female = Female - Male) %&gt;% # plot the difference ggplot(aes(older, effect.of.female, group=1)) + geom_point() + geom_line() + ylab(&quot;Effect of being female&quot;) + xlab(&quot;&quot;) + geom_hline(yintercept = 0) ## Using severity.of.injury as value column: use value.var to override. margins.plot As before, these differences use the raw data, but could have been calculated from a statistical model. In the section below we do this, making predictions for means and marginal effects from a lm(). 13.4 Continuous predictors In the examples above, our data were all categorical, which mean that it was straightforward to identify categories of people for whom we might want to make a prediction (i.e. young men, young women, older men, older women). However, age is typically measured as a continuous variable, and we would want to use a grouped scatter plot to see this: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(size=1) + scale_color_discrete(name=&quot;&quot;) But to make predictions from this continuous data we need to fit a line through the points (i.e. run a model). We can do this graphically by calling geom_smooth() which attempts to fit a smooth line through the data we observe: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(alpha=.2, size=1) + geom_smooth(se=F)+ scale_color_discrete(name=&quot;&quot;) Figure 13.1: Scatter plot overlaid with smooth best-fit lines And if we are confident that the relationships between predictor and outcome are sufficiently linear, then we can ask ggplot to fit a straight line using linear regression: injuries %&gt;% ggplot(aes(age, severity.of.injury, group=gender, color=gender)) + geom_point(alpha = .1, size = 1) + geom_smooth(se = F, linetype=&quot;dashed&quot;) + geom_smooth(method = &quot;lm&quot;, se = F) + scale_color_discrete(name=&quot;&quot;) Figure 13.2: Scatter plot overlaid with smoothed lines (dotted) and linear predictions (coloured) What these plots illustrate is the steps a researcher might take before fitting a regression model. The straight lines in the final plot represent our best guess for a person of a given age and gender, assuming a linear regression. We can read from these lines to make a point prediction for men and women of a specific age, and use the information about our uncertainty in the prediction, captured by the model, to estimate the likely error. To make our findings simpler to communicate, we might want to make estimates at specific ages and plot these. These ages could be: Values with biological or cultural meaning: for example 18 (new driver) v.s. 65 (retirement age) Statistical convention (e.g. median, 25th, and 75th centile, or mean +/- 1 SD) We’ll see examples of both below. 13.5 Predicted means and margins using lm() The section above details two types of predictions: predictions for means, and predictions for margins (effects). We can use the figure below as a way of visualising the difference: gridExtra::grid.arrange(means.plot+ggtitle(&quot;Means&quot;), margins.plot+ggtitle(&quot;Margins&quot;), ncol=2) Figure 13.3: Example of predicted means vs. margins. Note, the margin plotted in the second panel is the difference between the coloured lines in the first. A horizontal line is added at zero in panel 2 by convention. 13.5.1 Running the model Lets say we want to run a linear model predicts injury severity from gender and a categorical measurement of age (young v.s. old). Our model formula would be: severity.of.injury ~ age.category * gender. Here we fit it an request the Anova table which enables us to test the main effects and interaction21: injurymodel &lt;- lm(severity.of.injury ~ age.category * gender, data=injuries) anova(injurymodel) ## Analysis of Variance Table ## ## Response: severity.of.injury ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## age.category 1 4173.3 4173.3 154.573 &lt; 2.2e-16 *** ## gender 1 8488.5 8488.5 314.404 &lt; 2.2e-16 *** ## age.category:gender 1 1141.5 1141.5 42.279 1.25e-10 *** ## Residuals 996 26890.8 27.0 ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 Having saved the regression model in the variable injurymodel we can use this to make predictions for means and estimate marginal effects: 13.5.2 Making predictions for means When making predictions, they key question to bear in mind is ‘predictions for what?’ That is, what values of the predictor variables are we going to use to estimate the outcome? It goes like this: Create a new dataframe which contains the values of the predictors we want to make predictions at Make the predictions using the predict() function. Convert the output of predict() to a dataframe and plot the numbers. 13.5.3 Step 1: Make a new dataframe prediction.data &lt;- data_frame( age.category = c(&quot;young&quot;, &quot;older&quot;, &quot;young&quot;, &quot;older&quot;), gender = c(&quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Female&quot;) ) prediction.data ## # A tibble: 4 × 2 ## age.category gender ## &lt;chr&gt; &lt;chr&gt; ## 1 young Male ## 2 older Male ## 3 young Female ## 4 older Female 13.5.4 Step 2: Make the predictions The R predict() function has two useful arguments: newdata, which we set to our new data frame containing the predictor values of interest interval which we here set to confidence22 injury.predictions &lt;- predict(injurymodel, newdata=prediction.data, interval=&quot;confidence&quot;) injury.predictions ## fit lwr upr ## 1 57.14239 56.49360 57.79117 ## 2 59.19682 58.56688 59.82676 ## 3 60.79554 60.14278 61.44830 ## 4 67.12521 66.47642 67.77399 13.6 Making prdictions for margins (effects of predictors) library('tidyverse') m &lt;- lm(mpg~vs+wt, data=mtcars) m.predictions &lt;- predict(m, interval='confidence') mtcars.plus.predictions &lt;- bind_cols( mtcars, m.predictions %&gt;% as.data.frame() ) prediction.frame &lt;- expand.grid(vs=0:1, wt=2) %&gt;% as.data.frame() prediction.frame.plus.predictions &lt;- bind_cols( prediction.frame, predict(m, newdata=prediction.frame, interval='confidence') %&gt;% as.data.frame() ) mtcars.plus.predictions %&gt;% ggplot(aes(vs, fit, ymin=lwr, ymax=upr)) + stat_summary(geom=&quot;pointrange&quot;) ## No summary function supplied, defaulting to `mean_se() prediction.frame.plus.predictions %&gt;% ggplot(aes(vs, fit, ymin=lwr, ymax=upr)) + geom_pointrange() prediction.frame.plus.predictions ## vs wt fit lwr upr ## 1 0 2 24.11860 21.61207 26.62514 ## 2 1 2 27.27297 25.57096 28.97499 mtcars.plus.predictions %&gt;% group_by(vs) %&gt;% summarise_each(funs(mean), fit, lwr, upr) ## # A tibble: 2 × 4 ## vs fit lwr upr ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 16.61667 14.93766 18.29568 ## 2 1 24.55714 22.81586 26.29843 13.7 Marginal effects What is the effect of being black or female on the chance of you getting diabetes? Two ways of computing, depending on which of these two you hate least: Calculate the effect of being black for someone who is 50% female (marginal effect at the means, MEM) Calculate the effect first pretending someone is black, then pretending they are white, and taking the difference between these estimate (average marginal effect, AME) library(margins) margins(m, at = list(wt = 1:2)) ## Warning in check_values(data, at): A 'at' value for 'wt' is outside ## observed data range (1.513,5.424)! ## Average marginal effects at specified values ## lm(formula = mpg ~ vs + wt, data = mtcars) ## at(wt) vs wt ## 1 3.154 -4.443 ## 2 3.154 -4.443 m2 &lt;- lm(mpg~vs*wt, data=mtcars) summary(m2) ## ## Call: ## lm(formula = mpg ~ vs * wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9950 -1.7881 -0.3423 1.2935 5.2061 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.5314 2.6221 11.263 6.55e-12 *** ## vs 11.7667 3.7638 3.126 0.0041 ** ## wt -3.5013 0.6915 -5.063 2.33e-05 *** ## vs:wt -2.9097 1.2157 -2.393 0.0236 * ## --- ## Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1 ## ## Residual standard error: 2.578 on 28 degrees of freedom ## Multiple R-squared: 0.8348, Adjusted R-squared: 0.8171 ## F-statistic: 47.16 on 3 and 28 DF, p-value: 4.497e-11 m2.margins &lt;- margins(m2, at = list(wt = 1.5:4.5)) ## Warning in check_values(data, at): A 'at' value for 'wt' is outside ## observed data range (1.513,5.424)! summary(m2.margins) ## factor wt AME SE z p lower upper ## vs 1.5 7.4021 2.0902 3.5413 0.0004 3.3054 11.4989 ## vs 2.5 4.4924 1.2376 3.6299 0.0003 2.0667 6.9181 ## vs 3.5 1.5827 1.2848 1.2319 0.2180 -0.9353 4.1008 ## vs 4.5 -1.3270 2.1737 -0.6105 0.5416 -5.5874 2.9334 ## wt 1.5 -4.7743 0.5854 -8.1560 0.0000 -5.9216 -3.6270 ## wt 2.5 -4.7743 0.5854 -8.1556 0.0000 -5.9217 -3.6269 ## wt 3.5 -4.7743 0.5854 -8.1562 0.0000 -5.9216 -3.6270 ## wt 4.5 -4.7743 0.5854 -8.1561 0.0000 -5.9216 -3.6270 summary(m2.margins) %&gt;% as.data.frame() %&gt;% filter(factor==&quot;vs&quot;) %&gt;% ggplot(aes(wt, AME)) + geom_point() + geom_line() 13.8 With continuous covariates Run 2 x Continuous Anova Predict at different levels of 14 Visualising interactions from linear models check this: https://strengejacke.wordpress.com/2013/10/31/visual-interpretation-of-interaction-terms-in-linear-models-with-ggplot-rstats/ Steps this page will work through: Running the the model (first will be a 2x2 between Anova) Using predict(). Creating predictions at specific values Binding predictions and the original data together. Using GGplot to layer points, lines and error bars. Because this is simulated data, the main effects and interactions all have tiny p values.↩ This gives us the confidence interval for the prediction, which is the range within which we would expect the true value to fall, 95% of the time, if we replicated the study. We could ask instead for the prediction interval, which would be the range within which 95% of new observations with the same predictor values would fall. For more on this see the section on confidence v.s. prediction intervals↩ "],
["mediation.html", "15 Mediation", " 15 Mediation This chapter will assume: You know what mediation analyses are and why you want to run them You a basic understanding of the Baron and Kenny approach, but an inkling that there are more up to date ways of testing mediation hypotheses. "],
["multilevel-models.html", "16 Multilevel models", " 16 Multilevel models This chapter assumes: You know what a multilevel model is … "],
["cfa.html", "17 Confirmatory factor analysis 17.1 Defining the model 17.2 Model fit 17.3 Modification indices 17.4 Missing data", " 17 Confirmatory factor analysis This section adapted from a guide originally produced by Jon May First make sure the lavaan package is installed23; it stands for Latent Variable Analysis and is the most popular package for CFA in R. install.packages(lavaan) library(lavaan) Open your data and check that all looks well: hz &lt;- lavaan::HolzingerSwineford1939 hz %&gt;% glimpse() ## Observations: 301 ## Variables: 15 ## $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, ... ## $ sex &lt;int&gt; 1, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 2, 2, 1, 2... ## $ ageyr &lt;int&gt; 13, 13, 13, 13, 12, 14, 12, 12, 13, 12, 12, 12, 12, 12,... ## $ agemo &lt;int&gt; 1, 7, 1, 2, 2, 1, 1, 2, 0, 5, 2, 11, 7, 8, 6, 1, 11, 5,... ## $ school &lt;fctr&gt; Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, Pasteur, ... ## $ grade &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7... ## $ x1 &lt;dbl&gt; 3.333333, 5.333333, 4.500000, 5.333333, 4.833333, 5.333... ## $ x2 &lt;dbl&gt; 7.75, 5.25, 5.25, 7.75, 4.75, 5.00, 6.00, 6.25, 5.75, 5... ## $ x3 &lt;dbl&gt; 0.375, 2.125, 1.875, 3.000, 0.875, 2.250, 1.000, 1.875,... ## $ x4 &lt;dbl&gt; 2.333333, 1.666667, 1.000000, 2.666667, 2.666667, 1.000... ## $ x5 &lt;dbl&gt; 5.75, 3.00, 1.75, 4.50, 4.00, 3.00, 6.00, 4.25, 5.75, 5... ## $ x6 &lt;dbl&gt; 1.2857143, 1.2857143, 0.4285714, 2.4285714, 2.5714286, ... ## $ x7 &lt;dbl&gt; 3.391304, 3.782609, 3.260870, 3.000000, 3.695652, 4.347... ## $ x8 &lt;dbl&gt; 5.75, 6.25, 3.90, 5.30, 6.30, 6.65, 6.20, 5.15, 4.65, 4... ## $ x9 &lt;dbl&gt; 6.361111, 7.916667, 4.416667, 4.861111, 5.916667, 7.500... 17.1 Defining the model Define the CFA model you want to run using ‘lavaan model syntax’24. For example: hz.model &lt;- ' visual =~ x1 + x2 + x3 writing =~ x4 + x5 + x6 maths =~ x7 + x8 + x9 ' The model is defined in text, and can be broken over lines for clarity. Here was save it in a variable named hz.model. The model is defined by naming latent variables, and then specifying which observed variables measure them. The latent variable names are followed by =~ which means ‘is manifested by’. To run the analysis: hz.fit &lt;- cfa(hz.model, data=hz) To display the results: hz.fit.summary &lt;- summary(hz.fit, standardized=TRUE) ## lavaan (0.5-23.1097) converged normally after 35 iterations ## ## Number of observations 301 ## ## Estimator ML ## Minimum Function Test Statistic 85.306 ## Degrees of freedom 24 ## P-value (Chi-square) 0.000 ## ## Parameter Estimates: ## ## Information Expected ## Standard Errors Standard ## ## Latent Variables: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual =~ ## x1 1.000 0.900 0.772 ## x2 0.554 0.100 5.554 0.000 0.498 0.424 ## x3 0.729 0.109 6.685 0.000 0.656 0.581 ## writing =~ ## x4 1.000 0.990 0.852 ## x5 1.113 0.065 17.014 0.000 1.102 0.855 ## x6 0.926 0.055 16.703 0.000 0.917 0.838 ## maths =~ ## x7 1.000 0.619 0.570 ## x8 1.180 0.165 7.152 0.000 0.731 0.723 ## x9 1.082 0.151 7.155 0.000 0.670 0.665 ## ## Covariances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## visual ~~ ## writing 0.408 0.074 5.552 0.000 0.459 0.459 ## maths 0.262 0.056 4.660 0.000 0.471 0.471 ## writing ~~ ## maths 0.173 0.049 3.518 0.000 0.283 0.283 ## ## Variances: ## Estimate Std.Err z-value P(&gt;|z|) Std.lv Std.all ## .x1 0.549 0.114 4.833 0.000 0.549 0.404 ## .x2 1.134 0.102 11.146 0.000 1.134 0.821 ## .x3 0.844 0.091 9.317 0.000 0.844 0.662 ## .x4 0.371 0.048 7.779 0.000 0.371 0.275 ## .x5 0.446 0.058 7.642 0.000 0.446 0.269 ## .x6 0.356 0.043 8.277 0.000 0.356 0.298 ## .x7 0.799 0.081 9.823 0.000 0.799 0.676 ## .x8 0.488 0.074 6.573 0.000 0.488 0.477 ## .x9 0.566 0.071 8.003 0.000 0.566 0.558 ## visual 0.809 0.145 5.564 0.000 1.000 1.000 ## writing 0.979 0.112 8.737 0.000 1.000 1.000 ## maths 0.384 0.086 4.451 0.000 1.000 1.000 hz.fit.summary ## NULL The output has three parts: Parameter estimates. The values in the first column are the standardised weights from the observed variables to the latent factors. Factor covariances. The values in the first column are the covariances between the latent factors. Error variances. The values in the first column are the estimates of each observed variable’s error variance. 17.2 Model fit To examine the model fit: fitmeasures(hz.fit, c('cfi', 'rmsea', 'rmsea.ci.upper', 'bic')) ## cfi rmsea rmsea.ci.upper bic ## 0.931 0.092 0.114 7595.339 This looks OK, but could be improved. 17.3 Modification indices To examine the modification indices (here sorted to see the largest first) we type: modificationindices(hz.fit) %&gt;% as.data.frame() %&gt;% arrange(-mi) %&gt;% filter(mi &gt; 10) %&gt;% select(lhs, op, rhs, mi, epc) %&gt;% pander::pander() lhs op rhs mi epc visual =~ x9 36.41 0.577 x7 ~~ x8 34.15 0.5364 visual =~ x7 18.63 -0.4219 x8 ~~ x9 14.95 -0.4231 Latent factor to variable links have =~ in the ‘op’ column. Error covariances for observed variables have ~~ as the op. These symbols match the symbols used to describe a path in the lavaan model syntax. If we add the largest MI path to our model it will look like this: hz.model.2 &lt;- &quot; visual =~ x1 + x2 + x3 writing =~ x4 + x5 + x6 + x9 maths =~ x7 + x8 + x9 &quot; hz.fit.2 &lt;- cfa(hz.model.2, data=hz) fitmeasures(hz.fit.2, c('cfi', 'rmsea', 'rmsea.ci.upper', 'bic')) ## cfi rmsea rmsea.ci.upper bic ## 0.936 0.091 0.113 7595.660 RMSEA has improved marginally, but we’d probably want to investigate this model further, and make additional improvements to it. 17.4 Missing data If you have missing data, add the argument estimator='MLM' to the cfa() function to use a robust method. There are no missing data in this dataset, but it would look like this: hz.fit.2.mlm &lt;- cfa(hz.model.2, data=hz, estimator=&quot;MLM&quot;) hz.fit.2.mlm ## lavaan (0.5-23.1097) converged normally after 35 iterations ## ## Number of observations 301 ## ## Estimator ML Robust ## Minimum Function Test Statistic 79.919 75.703 ## Degrees of freedom 23 23 ## P-value (Chi-square) 0.000 0.000 ## Scaling correction factor 1.056 ## for the Satorra-Bentler correction see: installing packages↩ A full guide is here: http://lavaan.ugent.be/tutorial/syntax1.html↩ "],
["installation.html", "18 Installing R and RStudio", " 18 Installing R and RStudio XXX To do: For the moment just use: http://r.thesignalbox.net (ask Ben Whalley for a user account). "],
["packages.html", "19 Loading packages", " 19 Loading packages R has been around for a very long time, but has remained popular because it is easy for people to add new functions to it. You can run almost any statistical model and produce a wide variety of graphics in R because people have contributed new functions and these extend the base language. These new features are distributed in bundles known as ‘packages’. For now we’ll assume someone has helped you install all the packages you need25. To access the features in packages, you normally load the package with the library() function. Running library(&lt;packagename&gt;) loads all the new functions within it, and it is then possible to call them from your code. For example, typing: library(ggplot2) Will load the ggplot2 package. You can then call the qplot function it provides: qplot(mtcars$mpg, bins=7) You don’t strictly need to load packages to use the features within them though. If a package is installed on your system you can also call a function it provides directly. In the example below we call the hist.data.frame from the Hmisc package, and obtain histograms of all the variables in the mtcars dataset: Hmisc::hist.data.frame(mtcars) The rule is to type package::function(parameters), where :: separates the package and function names. Parameters are just the inputs to the function. There are two reasons not to load a package before using it: Laziness: it can save typing if you just want to use one function from a package, and only once. Explicitness: It’s an unfortunate fact that some function names are repeated across a number of packages. This can be confusing if they work differently, and if you don’t know which package the version you are using comes from. Using package_name:function_name can help make things explicit. See the installation guide if this isn’t the case↩ "],
["help.html", "20 Getting help", " 20 Getting help If you don’t know or can’t remember what a function does, R provides help files which explain how they work. To access a help file for a function, just type ?command in the console, or run ?command command within an R block. For example, running ?mean would bring up the documentation for the mean function. You can also type CRTL-Shift-H while your cursor is over any R function in the RStudio interface. It’s fair to say R documentation isn’t always written for beginners. However the ‘examples’ sections are usually quite informative: you can normally see this by scrolling right to the end of the help file. "],
["missing-values.html", "21 Missing values", " 21 Missing values TODO Explain about NA and is.na() and is.finite() here "],
["rmarkdown-tricks.html", "22 Tips and tricks with RMarkdown", " 22 Tips and tricks with RMarkdown TODO, cover Using pander to format tables nicely Rounding of values with sprintf Accessing coefficients() Using broom to tidy model results Using bibtex and citing in text Calculate VPC/ICC from an lmer models using model %&gt;% summary %&gt;% as.data.frame()$varcor Hint at things hidden in R objects (e.g. formula in the lm object). Using @ and $ to autocomplete and find things "],
["confidence-vs-prediction-intervals.html", "23 Confidence, credible and prediction intervals 23.1 The problem with confidence intervals 23.2 Forgetting that the CI depends on sample size. 23.3 What does this mean for my work on [insert speciality here]?", "library(tidyverse) ## Loading tidyverse: ggplot2 ## Loading tidyverse: tibble ## Loading tidyverse: tidyr ## Loading tidyverse: readr ## Loading tidyverse: purrr ## Loading tidyverse: dplyr ## Conflicts with tidy packages ---------------------------------------------- ## filter(): dplyr, stats ## lag(): dplyr, stats 23 Confidence, credible and prediction intervals TODO: EXPAND ON THESE DEFINITIONS AND USE GRAPHICS AND PLOTS TO ILLUSTRATE Confidence interval: The range within which we would expect the true value to fall, 95% of the time, if we replicated the study. Prediction interval: the range within which we expect 95% of new observations to fall. If we’re considering the prediction interval for a specific point prediction (i.e. where we set predictors to specific values), then this interval woud be for new observations with the same predictor values. 23.1 The problem with confidence intervals Confidence intervals are helpful when we want to think about how precise our estimate is. For example, in an RCT we will want to estimate the difference between treatment groups, and it’s conceivable be reasonable to want to know, for example, the range within which the true effect would fall 95% of the time if we replicated our study many times. If we run a study with small N, intuitively we know that we have less information about the difference between our RCT treatments, and so we’d like the CI to expand accordingly. So — all things being equal — the confidence interval reduces as our sample size increases. The problem with confidence intervals come because many researchers and clinicians read them incorrectly. Typically, they either: Forget that the CI represents the precision of the estimate Misinterpret the CI as the range in which we are 95% sure the true value lies. 23.2 Forgetting that the CI depends on sample size. By forgetting that the CI contracts as the sample size increases, researchers can become overconfident about their ability to predict new observations. Imagine that we sample data from two populations with the same mean, but different variability: set.seed(1234) df &lt;- expand.grid(v=c(1,3,3,3), i=1:1000) %&gt;% as.data.frame %&gt;% mutate(y = rnorm(length(.$i), 100, v)) %&gt;% mutate(samp = factor(v, labels=c(&quot;Low variability&quot;, &quot;High variability&quot;))) df %&gt;% ggplot(aes(y)) + geom_histogram() + facet_grid(~samp) + scale_color_discrete(&quot;&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. If we sample 100 individuals from each population the confidence interval around the sample mean would be wider in the high variability group. If we increase our sample size we would become more confident about the location of the mean, and this confidence interval would shrink. But imagine taking a single new sample from either population. These samples would be new grey squares, which we place on the histograms above. It does not matter how much extra data we have collected in group B or how sure what the mean of the group is: We would always be less certain making predictions for new observations in the high variability group. The important insight here is that if our data are noisy and highly variable we can never make firm predictions for new individuals, even if we collect so much data that we are very certain about the location of the mean. 23.3 What does this mean for my work on [insert speciality here]? "]
]
