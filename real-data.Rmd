---
title: 'Real data'
output:
  bookdown::tufte_html2
---

```{r, include=FALSE}
# ignore all this for the moment
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=F, message=F)
library(tufte)
library(tidyverse)
library(broom)
library(pander)
```


# Working with 'real' data {#real-data}

*Note: If you already lucky enough to have nicely formatted data, ready for use in R, then you could skip this section and revisit it later,* save for the section on [factors and other variable types](#factors-and-numerics).

Most tutorials and textbooks use neatly formatted example datasets to illustrate particular techniques. However in the real-world our data can be:

- In the wrong format
- Spread across multiple files
- Badly coded, or with errors
- Incomplete, with values missing for many different reasons


This chapter shows you how to address each of these problems.



## Storing your data {- #storing-data}


### CSV files are your friend {- #use-csv}

Comma-sepatated-values files are a plain text format which are idea for storing your data. Some advantages include:

- Understood by almost every piece of software on the planet
- Will remain readbale in future
- Easy to store 2D data (like data frames)
- Human readable (just open in Notepad)


Commercial formats like Excel, SPSS (.sav) and Stata (.dta) don't have these properties.

Although CSV has some disadvantages, they are all easily overcome if you [save the steps of your data processing and analysis in your R code](#save-intermediate-steps), see below.





### Save processes, not outcomes {- #save-intermediate-steps}

Many students (and academics) make errors in their analyses because they process data by hand (e.g. editing files in Excel) or use GUI tools to run analyses.

In both cases these errors are hard to identify or rectify because only the outputs of the analysis can be saved, and *no record has been made of how these outputs were produced*. 

In contrast, if you do your data processing and analysis in R/RMarkdown you benefit from a concrete, repeatable series of steps  which can be checked/verified by others. This can also save lots of time if you need to processing additional data later on (e.g. if you run more participants).


Some principles to follow when working:

- Save your raw data in the simplest possible format, in CSV

- Always include column names in the file

- Use descriptive names, but with a regular strucuture.

- Never include spaces or special characters in the column names. Use underscores (`_`) if you want to make things more readable.

- Make names <20 characters in length if possible








### RDS files can be useful to preserve R objects {- #rds-files}

If you have R objects which you'd like to save, for example because they took a long time to compute, the the RDS format is the best way of preserving them.

To save something:

```{r, eval=F}
# create a huge df of random numbers... 
massive.df <- data_frame(nums = rnorm(1:1e8))
saveRDS(massive.df, file="massive.RDS")
```

Then later on you can load it like this:

```{r, eval=F}
restored.massive.df <-  readRDS('massive.RDS')
```

[If you do this in RMarkdown, by default the RDS files will be saved in the same directory as your .Rmd file.]{.tip}




## Types of variable {- #factors-and-numerics}

When working with data in Excel or other packages like SPSS you've probably become aware that different types of data get treated differently. For example, in Excel you can't set up a formula like `=SUM(...)` on cells which include letters (rather than just numbers). It does't make sense. However, Excel and many other programmes will sometimes make guesses about what to do if you combine different types of data. For example, if you add `28` to `1 Feb 2017` the result is `1 March 2017`. This is sometimes what you want, but can often lead to unexpected results and errors in data analyses.

R is much more strict about not mixing types of data. Vectors (and columns in dataframes) can only contain one type of thing. In general, there are probably 4 types of data you will encounter in data analysis problems:

- Numeric variables
- Character variables
- Factors
- Dates


```{r, echo=F, include=F}
# process the lakers data from lubridate to illustrate some points
lakers <- lubridate::lakers %>% 
  select(date, opponent, team, points) %>% 
  mutate(date=lubridate::ymd(date), team=factor(team)) 
saveRDS(lakers, file="lakers.RDS")
```


The file `lakers.RDS` contains a dataset adapted from the `lubridate::lakers` dataset.

It contains four variables to illustrate the common variable types. From the original dataset which provides scores and other information from each Los Angeles Lakers basketball game in the 2008-2009 season we have the `date`, `opponent`, `team`, and  `points` variables.

```{r}
lakers <- readRDS("lakers.RDS")
lakers %>% 
  glimpse
```


One thing to note here is that the `glimpse()` command tells us the *type* of each variable. So we have 

- `points`: type `int`, short for integer (i.e. whole numbers). 
- `date`: type `date`
- `opponent`: type `chr`, short for 'character', or alaphanumeric data
- `team`: type `fctr`, short for factor and


[Other numeric variables might sometimes have type = `dbl`, which stands for 'double precision' floating point number (that is, a decimal fraction). But for most data analysis purposes we can treat all numeric variable types the same.]{.admonition}




### Numeric variables {-}

We've already seem numeric variables in the section on [vectors and lists](#vectors). These behave pretty much as you'd expect, and we won't expand on them here. 

#### {- .explainer}

One small aside is that you should be aware that there are limits to the precision with which R (and computers in general) can store decimal values. This only tends to matter when working with very large or very small numbers â€” but this can crop up when estimating regression coefficients that are very small for example, and is one reason why [scaling inputs to regression models can improve performance and accuracy of results](#scaling-regression-inputs).



### Characters, factors (and booleans) {- #character-and-factor}

In many cases variables will be used to identify cases which have *qualitative differences*: for example, where different groups or measurement occasions in an experimental study, or different genders.

It can sometimes cause confusion that these categorical variables can be stored in as numeric variables, or as 'character' types (strings of letters and numbers), or as a third type called factors.

For example, you will often come across data where groupings are stored in either one of these formats:

```{r}
data_frame(month = 1:12,
  month.name = format(ISOdatetime(2000,1:12,1,0,0,0),"%b"),
  group = c("Waiting", "Treatment", "Control", rep(NA, 9)),
  group.number = c(1:3, rep(NA, 9)))
```


One problem with storing categories as numeric variables is that we can end up with [confusing results when running regression models](#factors-vs-linear-inputs). 


For this reason, it's often best to store your categorical variables as strings of letters and numbers (e.g. "Group 1", "Group 2") and avoid simple numbers (e.g. 1, 2, 3).

*Factors* are R's answer to this problem of storing categorical data.
Factors assign one number for each unique value in a variable, and optionally allow you to attach a label to it.

For example:

```{r}
1:10

group.factor <- factor(1:10)
group.factor

group.labelled <- factor(1:10, labels = paste("Group", 1:10))
group.labelled
```

We can see this 'underlying' number which represents each category by using `as.numeric`:

```{r}
# note, there is no guarantee that "Group 1" == 1 (although it is here)
as.numeric(group.labelled)
```



These days, for simple analyses it's best to store everything as the `character` type (letters and numbers), but factors can still be useful for making tables or graphs where the list of categories is known and needs to be in a particular order. For more about factors, and lots of useful functions for working with them, see the `forcats::` package: <https://github.com/tidyverse/forcats>



### Dates {-}

Like  most computers, R stores dates as the number of days since January 1, 1970. This means that we can work with dates just like other numbers, and it makes sense to have the `min()`, or `max()` of a series of dates:


```{r}
# the first few dates in the sequence
head(lakers$date)

# first and last dates
min(lakers$date)
max(lakers$date)

```

Because dates are numbers we can also do arithmetic with them, and R will give us a difference (in this case, in days):

```{r}
max(lakers$date) - min(lakers$date)
```

However, R does treat dates slightly differently from other numbers, and will format plot axes appropriately, which is helpful (see more on this in the [graphics section](#graphics)):

```{r}
hist(lakers$date, breaks=7)
```


### Missing values {-}

Missing values aren't a data type as such, but are an important concept in R; the way different functions handle missing values can be both helpful and frustrating in equal measure. [The next section covers the handling of missing data in more detail](#missing).


## Missing values {- #missing}

Missing values aren't a data type as such, but are an important concept in R; the way different functions handle missing values can be both helpful and frustrating in equal measure.

Missing values in a vector are denoted by the letters `NA`, but notice that these letters are unquoted. That is to say `NA` is not the same as `"NA"`!

To check for missing values in a vector (or dataframe column) we use the `is.na()` function:

```{r}
nums.with.missing <- c(1, 2, NA)
nums.with.missing

is.na(nums.with.missing)
```


Here the `is.na()` function has tested whether each item in our vector called `nums.with.missing` is missing. It returns a new vector with the results of each test: either `TRUE` or `FALSE`.

We can also use the negation operator, the `!` symbol to reverse the meaning of `is.na`. So we can read `!is.na(nums)` as "test whether the values in `nums` are NOT missing":

```{r}
# test if missing
is.na(nums.with.missing)

# test if NOT missing
!is.na(nums.with.missing)
```


We can use the `is.na()` function as part of dplyr filters:

```{r}
airquality %>% 
  filter(is.na(Solar.R)) %>% 
  head(3) %>% 
  pander
```


Or to select only cases without missing values for a particular variable:

```{r}
airquality %>% 
  filter(!is.na(Solar.R)) %>% 
  head(3) %>% 
  pander
```

#### Complete cases {- #complete-cases}


Sometimes we want to select only rows which have no missing values --- i.e. *complete cases*. 

The `complete.cases` function accepts a dataframe (or matrix) and tests whether each *row* is complete. It returns a vector with a `TRUE/FALSE` result for each row:

```{r}
complete.cases(airquality) %>% 
  head
```

This can also be useful in dplyr filters. Here we show all the rows which are *not* complete (note the exclamation mark):

```{r}
airquality %>% 
  filter(!complete.cases(airquality))
```



#### {- .tip}

Sometimes it's convenient to use the `.` (period) to represent the output from the previous pipe command. For example, we could rewrite the previous example as:

```{r}
airquality %>% 
  filter(!complete.cases(.))  # note the . (period) here in place of `airmiles`
```


This is nice because we can apply the `complete.cases` function to the output of the previous pipe. For example, if we wanted to select complete cases for a subset of the variables we could write:

```{r}
airquality %>% 
  select(Ozone, Solar.R) %>% 
  filter(!complete.cases(.))
```

Or alternatively:

```{r}
rows.to.keep <- !complete.cases(select(airquality, Ozone, Solar.R))
airquality %>% 
  filter(rows.to.keep) %>% 
  head(3) %>% 
  pander
```



#### Missing data and R functions {- #na.rm}

It's normally good practice to pre-process your data and select the rows you want to analyse *before* passing dataframes to R functions.

The reason for this is that different functions behave differently with missing data. 

For example:

```{r}
mean(airquality$Solar.R)
```

Here the default for `mean()` is to return NA if any of the values are missing. We can explicitly tell R to ignore missing values by setting `na.rm=TRUE`

```{r}
mean(airquality$Solar.R, na.rm=TRUE)
```


In contrast some other functions, for example the `lm()` which runs a linear regression will ignore missing values by default. If we run `summary` on the call to `lm` then we can see the line near the bottom of the output which reads: "(7 observations deleted due to missingness)"

```{r}
lm(Solar.R ~ Temp, data=airquality) %>% 
  summary
```


[Normally R will do the 'sensible thing' when there are missing values, but it's always worth checking whether you do have any missing data, and addressing this explicitly in your code]{.tip}




#### Patterns of missingness {-}

The `mice` package has some nice functions to describe patterns of missingness in the data. These can be useful both at the exploratory stage, when you are checking and validating your data, but can also be used to create tables of missingness for publication:

```{r}
mice::md.pattern(airquality) 
```


In this table, `md.pattern` list the number of cases with particular patterns of missing data.
- Each row describes a misisng data 'pattern'
- The first column indicates the number of cases
- The central columns indicate whether a particular variable is missing for the pattern (0=missing)
- The last column counts the number of values missing for the pattern
- The final row counts the number of missing values for each variable.



##### Visualising missingness

Graphics can also be useful to explore patterns in missingness.  

`rct.data` contains data from an RCT of functional imagery training (FIT) for weight loss, which measured outcome (weight in kg) at baseline and two followups (`kg1`, `kg2`, `kg3`). The trial also measured global quality of life (`gqol`).

As is common, there were some missing data at the follouwp:

```{r}
fit.data <- readRDS("data/fit-weight.RDS") %>% 
  select(kg1, kg2, kg3, age, gqol1)

mice::md.pattern(fit.data)
```


We might be interested to explore patterns in which observations were missing. Here we use colour to identify missing observations as a function of the data recorded at baseline:

```{r}
fit.data %>% 
  mutate(missing.followup = is.na(kg2)) %>% 
  ggplot(aes(kg1, age, color=missing.followup)) +
  geom_point()
```

There's a clear trend here for lighter patients (at baseline) to have more missing data at followup. There's also a suggestion that younger patients are more likely to have been lost to followup.

If needed, we could perform formal statistical tests for these differences:

```{r}
t.test(kg1 ~ is.na(kg2), data=fit.data)
t.test(age ~ is.na(kg2), data=fit.data)
```


However, given the small number of missing values and the post-hoc nature of these analyses these tests are rather underpowered and we might prefer to report and comment on the plot and alone.


For some nice missing data visualisation techniques, including those for repeated measures data, see @zhang2015missing.





## Tidying data {-}


'Tidying' data means converting it into the format that is most useful for data analyses. 


This part of the guide is currently incomplete, but excellent tuturials exist here: 

- http://tidyr.tidyverse.org and 
- http://r4ds.had.co.nz/tidy-data.html





## Dealing with multiple files {-}

Oftentimes you will have multiple data files files (hopefully .csv format) to import and process - for example, those produced by experimental software like [PsychoPy](http://www.psychopy.org). This is one of the few times when you might have to do something resembling 'real programming', but it's still fairly straightforward.

In the [repeated measures Anova example later on in this guide](#trad-rm-anova) we encounter some data from an experiment where reaction times were recorded in 25 trials (`Trial`) before and after (`Time`) one of 4 experimental manipulations (`Condition` = {1,2,3,4}). There were 48  participants in total:

```{r, include=F}
expt.data <- readRDS("expt.data.csv")
```

```{r, include=F, eval=F}
# export the expt.data to multiple files as an example
filterandsave <- function(df){
  person = first(df$person)
  filename <- paste0("data/mutliple-file-example/person", person, ".csv" )
  write.csv(df, file=filename, row.names=F)
  data_frame(person=person, filename=filename)
}

expt.data %>% 
  group_by(person) %>% 
  do(filterandsave(.))
```

Let's say all the files are in a single directory, and numbered sequentially. Using the `list.files()` function we can list the contents of a directory on the hard drive:

```{r}
list.files('data/multiple-file-example/')
```


Helpfully, `list.files()` creates a [vector](#vectors) of the filenames in the directory. 

At this point, there are many, many ways of importing the contents of these files, but below we use a technique which is concise, reliable, and less error-prone than many others. It also continues to use the `dplyr` library.

This approach has 3 steps:

1. Put all the names of the .csv files into a dataframe.
2. For each row in the dataframe, run a function which imports the file as a dataframe.
3. Combine all these dataframes together.



###### Putting the filenames into a dataframe {-}

Because `list.files` produces a vector, we can make them a column in a new dataframe:

```{r}
raw.files <- data_frame(filename = list.files('data/multiple-file-example/'))
```


And we can make a new column with the complete path (i.e. including the directory holding the files), using the [`paste0`](#paste) which combines strings of text. We wouldn't have to do this if the raw files were in the same directory as our RMarkdown file, but that would get messy.

```{r}
raw.file.paths <- raw.files  %>% 
  mutate(filepath = paste0("data/multiple-file-example/", filename))

raw.file.paths %>% 
  head(3) %>% 
  pander()
```



###### Using `do()` {- #dplyr-do}

We can then use the `do()` function in `dplyr::` to import the data for each file and combine the results in a single dataframe.

The `do()` function allows us to run any R function for each group or row in a dataframe. 

The means that our original dataframe is broken up into chunks (either groups of rows, if we use `group_by()`, or individual rows if we use `rowwise()`) and each chunk is fed to the function we specify. This function must do it's work and return a new dataframe, and these are then combined into a single larger dataframe.

So in this example, we break our dataframe of filenames up into individual rows using `rowwise` and then specify the `read_csv` function which takes the name of a csv file, and returns the content as a dataframe ([see the importing data section](#importing-data)).

For example:

```{r, message=F}
raw.data <- raw.file.paths %>%
  # 'do' the function for each row in turn
  rowwise() %>% 
  do(., read_csv(file=.$filepath))
```


We can check these data look OK by sampling 10 rows at random:

```{r}
raw.data %>% 
  sample_n(10) %>% 
  pander()
```



##### Using custom functions with `do()` {-}

In this example, each of the raw data files included the participant number (the `person` variable). However, this isn't always the case.

This isn't a problem though, if we create our own [helper function](#helper-functions) to import the data. Writing small functions in R is very easy, and the example below wraps the `read.csv()` function and adds a new colum, `filename` to the imported data frame which would enable us to keep track of where each row in the final combined dataset came from.

This is the helper function: 

```{r}
read.csv.and.add.filename <- function(filepath){
  read_csv(filepath) %>% 
    mutate(filepath=filepath)
}
```

And we can use the helper function with `do()` like so:

```{r}
raw.data.with.paths <- raw.file.paths %>%
  rowwise() %>% 
  do(., read.csv.and.add.filename(.$filepath))

raw.data.with.paths %>% 
  head %>% 
  pander
```


At this point you might want to [use the `extract()` or `separate()` functions](#extract-to-split-column-names) to post-process the filename and re-create the `person` variable from this (although here that's already been done for us).





<!-- TODO1 -->
<!-- - File handling and import -->
<!-- - Writing a function for `do()` which returns a dataframe -->
<!-- - Joins and merges -->



<!-- ##  Error checking {-} -->

<!-- - `999`, `666` and `*`: the marks of the beast! -->




