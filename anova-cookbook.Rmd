---
title: 'Anova cookbook'
output:
  bookdown::tufte_html2
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, collapse=TRUE, cache=TRUE)
library(tidyverse)
library(pander)

library(lmerTest)
```



## Anova 'Cookbook' {- #anova-cookbook}

This section is intended as a shortcut to running Anova for a variety of common types of model. If you want to understand more about what you are doing, read the section on [principles of Anova in R](#anova).



### Between-subjects Anova {-}


#### Factorial anova, 2x2 {-}

TODO



#### Factorial anova, where one factor has > 2 levels {-}

We are using a dataset from Howell (REF), chapter 13 which recorded `Recall` among young v.s. older adults (`Age`) for each of 5 conditions.


```{r include=F}
eysenck <- read.table('howell-data/Tab13-2.dat', header=T) %>% 
  mutate(
    Condition=factor(Condition, 
                     labels=c("Counting", "Rhyming", "Adjective", "Imagery", "Intention")),
    Age = factor(Age, labels=c("Young", "Older")))
saveRDS(eysenck, file="data/eysenck.Rdata")

```


This data would commonly be plotted something like this:


```{r}
eysenck <- readRDS("data/eysenck.Rdata")
eysenck %>% 
  ggplot(aes(Condition, Recall, group=Age, color=Age)) + 
  stat_summary(geom="pointrange", fun.data = mean_cl_boot) +
  ylab("Recall (95% CI)") + xlab("")
```


[Visual inspection of the data (see Figure X) suggested that older adults recalled more words than younger adults, and that this difference was greatest for the intention, imagery, and adjective conditions. Recall peformance was worst in the counting and rhyming conditions.]{.apa-example}



Or alternatively if we wanted to provde a better summary of the distribution of the raw data we could use a boxplot:

```{r, fig.cap="Boxplot for recall in older and young adults, by condition."}
eysenck %>% 
  ggplot(aes(Age, Recall)) + 
  geom_boxplot(width=.33) + facet_grid(~Condition) +
  ylab("Recall (95% CI)") + xlab("")
```


We can run a linear model including the effect of `Age` and `Condition` and the interaction of these variables, and calculate the Anova:

```{r}
eysenck.model <- lm(Recall~Age*Condition, data=eysenck)
car::Anova(eysenck.model, type=3)
```








### Repeated measures or 'split plot' designs {-}

It might be controversial to say so, but the tools to run traditional repeat measures Anova in R are a pain to use. It's not easy to run repeated measures Anova models using base packages alone and, although there are numerous packages which do simplify this a little, their syntax can be obtuse or confusing, and the output sometimes cryptic. To make matters worse, various textbooks, online guides and the R help files themselves show many ways to achieve the same ends, and it can be difficult to follow the differences between the underlying models that are run.

At this point, given the [many other advantages of linear mixed models over traditional repeated measures Anova](http://jamanetwork.com/journals/jamapsychiatry/article-abstract/481967), and given that many researchers abuse traditional Anova in practice (e.g. using it for unbalanced data, or where some data are missing), the recommendation here is to simply give up and learn how to run linear mixed models. These can (very closely) replicate traditional Anova approaches, but also:

- Handle missing data or unbalanced designs gracefully and efficiently.

- Be expanded to include multiple levels of nesting. For example, allowing pupils to be nested within classes, within schools. Alternatively multiple measurements of individual patients might be clustered by hospital or therapist.

- Allow time to be treated as a continuous variable. For example, time can be modelled as a slope or some kind of curve, rather than a fixed set of observation-points. This can be more parsimonious, and more flexible when dealing with real-world data (e.g. from clinical trials).

It would be best at this point to [jump straight to the main section multilevel or mixed-effects models](#multilevel-models), but to give one brief example of mixed models in use:



The `sleepstudy` dataset in the `lme4` package provides reaction time data recorded from participants over a period of 10 days, during which time they were deprived of sleep.

```{r}
lme4::sleepstudy %>% head(12) %>% pandoc.table
```

We can plot these data to show the increase in RT as sleep deprivation continues:

```{r}
lme4::sleepstudy %>% 
  ggplot(aes(factor(Days), Reaction)) + 
  geom_boxplot() + 
  xlab("Days") + ylab("RT (ms)") + 
  geom_label(aes(y=400, x=2, label="you start to\nfeel bad here"), color="red") + 
  geom_label(aes(y=450, x=9, label="imagine how bad\nyou feel by this point"), color="red") 
```


If we want to test whether there are significant differences in RTs between `Days`, we could fit something very similar to a traditional repeat measures Anova using the `lme4::lmer()` function, and obtain an Anova table for the model using the special `lmerTest::anova()` function:

```{r}
sleep.model <- lmer(Reaction ~ factor(Days) + (1 | Subject), data=lme4::sleepstudy)
lmerTest::anova(sleep.model)
```


If you had really wanted to fit the traditional repeated measures Anova, the closest equivalent would be:

```{r}
afex::aov_car(Reaction ~ Days + Error(Subject/(Days)), data=lme4::sleepstudy)
```

This gives almost-identical results. You may find that in other cases the `lmer` and traditional anova models diverge slightly, but this is likely to be caused by factors including imbalances in the data, partially missing data (only complete cases can be analyses by traditional anova) or other violations of the assumptions of one or both of the models. There is no clear steer in the literature as to which model is 'best' in the general sense, and it is likely that the linear model will be a better fit for a greater range of datasets.

See the [multilevel models section](#multilevel-models) for details of more interesting models using this dataset which:

- Fit a simple slope for `Days`
- Fit curves or other functions for `Days`
- Allow the effect of sleep deprivation to vary for different participants








## Checking assumptions {-}



If we want to check assumptions of the Anova are met, these tables and plots would be a reasonable place to start. First running Levene's test:

```{r}
car::leveneTest(eysenck.model) %>% 
  pandoc.table()
```


Then a QQ-plot of the model residuals to assess normality:


```{r, fig.cap="QQ plot to assess normality of model residuals"}
car::qqPlot(eysenck.model)
```


And finally a residual-vs-fitted plot:

```{r, fig.cap="Residual vs fitted (spread vs. level) plot to check homogeneity of variance."}

data_frame(
  fitted = predict(eysenck.model), 
  residual = residuals(eysenck.model)) %>% 
  # and then plot points and a smoothed line
  ggplot(aes(fitted, residual)) + 
    geom_point() + 
    geom_smooth(se=F)

```




## Post hoc tests {-}

If we want to look at post-hoc pairwise tests we can use the the `lsmeans()` function from the `lsmeans::` package:

```{r}
lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition)
```

By default Tukey correction is applied for multiple comparisons which is a reasonable default. If you want to use other methods (e.g. to use false discovery rate adjustment, see the section on [multiple comparisons](#multiple-comparisons)) you can use the `adjust` argument. 

In the code below we request FDR-adjusted p values, and then use the `broom::tidy()` function to convert the table into a dataframe, and then show only the first 6 rows as a table in RMarkdown: 

```{r}
# calculate pairwise contrasts
eysenck.fdr <- lsmeans::lsmeans(eysenck.model, pairwise~Age:Condition, adjust="fdr")

# show the first 6 rows from this long table
eysenck.fdr$contrasts %>% 
  broom::tidy() %>% 
  head(6) %>% 
  pandoc.table(caption="First 6 rows of the pairwise contrasts with FDR-adjusted p values")
```










